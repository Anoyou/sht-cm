#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ¬è™«æ ¸å¿ƒè°ƒåº¦é€»è¾‘æ¨¡å— - è´Ÿè´£æ‰§è¡Œçˆ¬å–ä»»åŠ¡çš„æ ¸å¿ƒé€»è¾‘
"""

import time
import logging
import os
import json
import random
import asyncio
import concurrent.futures
import datetime as _dt
from datetime import datetime, timezone, timedelta
from crawler import SHT, AsyncSHTCrawler
from utils.async_bridge import run_async
from sqlalchemy import func
from models import db, Resource, FailedTID, Category
from configuration import config_manager
from utils import get_flask_app, get_flask_app_context
from constants import SECTION_MAP, SECTION_NAME_TO_FID
from .state import sync_crawl_state
from .notifier import _send_telegram_message, _send_crawl_report, render_message_template
from .utils import stop_event, pause_event, sleep_interruptible

logger = logging.getLogger(__name__)


def update_crawl_state(updates):
    """
    ç»Ÿä¸€çŠ¶æ€æ›´æ–°å…¥å£

    æ›´æ–°çˆ¬è™«çŠ¶æ€åˆ°ç»Ÿä¸€çŠ¶æ€ç®¡ç†å™¨ï¼ŒåŒæ—¶æ›´æ–°å…¨å±€å˜é‡ä»¥ä¿æŒå‘åå…¼å®¹
    """
    from utils.state_manager import update_unified_state

    try:
        # 1. æ›´æ–°ç»Ÿä¸€çŠ¶æ€ç®¡ç†å™¨
        if updates:
            update_unified_state(updates, source='scheduler_core')

        # 2. åŒæ—¶æ›´æ–°ä¼ ç»ŸçŠ¶æ€ï¼ˆå‘åå…¼å®¹ï¼‰
        try:
            from flask import current_app
            crawl_status = current_app.config.get('CRAWL_STATUS', {})
            crawl_progress = current_app.config.get('CRAWL_PROGRESS', {})
            crawl_control = current_app.config.get('CRAWL_CONTROL', {})
        except Exception:
            from cache_manager import cache_manager, CacheKeys
            crawl_status = cache_manager.shared_get(CacheKeys.CRAWL_STATUS) or {}
            crawl_progress = cache_manager.shared_get(CacheKeys.CRAWL_PROGRESS) or {}
            crawl_control = cache_manager.shared_get(CacheKeys.CRAWL_CONTROL) or {}

        for key, value in updates.items():
            # ç›´æ¥æ›´æ–°å­—æ®µï¼Œä¸è¿›è¡Œå­˜åœ¨æ€§æ£€æŸ¥
            # è¿™æ ·æ–°å¢çš„å­—æ®µä¹Ÿèƒ½æ­£ç¡®ä¿å­˜åˆ°çŠ¶æ€ä¸­
            if key in ['is_crawling', 'is_paused', 'message', 'should_stop']:
                crawl_status[key] = value
            elif key in ['sections_total', 'sections_done', 'current_section',
                       'current_page', 'max_pages', 'total_saved', 'total_skipped',
                       'current_section_pages', 'current_section_processed',
                       'processed_pages', 'estimated_total_pages', 'progress_percent',
                       'total_failed', 'current_section_saved', 'current_section_skipped',
                       'start_time',
                       # é¡µç æ¦‚å¿µåŒºåˆ†å­—æ®µ
                       'current_page_actual', 'max_pages_actual',
                       'current_page_task', 'max_pages_task']:
                crawl_progress[key] = value
            elif key in ['stop', 'paused']:
                crawl_control[key] = value

        # 3. åŒæ­¥åˆ°å…±äº«ç¼“å­˜ï¼Œä¾¿äºè·¨è¿›ç¨‹è¯»å–
        try:
            from cache_manager import cache_manager, CacheKeys
            cache_manager.shared_set(CacheKeys.CRAWL_STATUS, crawl_status)
            cache_manager.shared_set(CacheKeys.CRAWL_PROGRESS, crawl_progress)
            cache_manager.shared_set(CacheKeys.CRAWL_CONTROL, crawl_control)
        except Exception:
            pass

    except Exception as e:
        logger.debug(f"æ›´æ–°çŠ¶æ€å¤±è´¥: {e}")


def run_crawling_task():
    """æ‰§è¡Œçˆ¬å–ä»»åŠ¡"""
    logger.info("å¼€å§‹æ‰§è¡Œçˆ¬å–ä»»åŠ¡...")
    
    sht = SHT()
    
    for fid, section_name in SECTION_MAP.items():
        logger.info(f"æ­£åœ¨çˆ¬å–åˆ†ç±»: {section_name}")
        
        # çˆ¬å–å‰å‡ é¡µçš„æ•°æ®
        for page in range(1, 4):  # çˆ¬å–å‰3é¡µ
            url = f"https://sehuatang.org/forum.php?mod=forumdisplay&fid={fid}&mobile=2&page={page}"
            
            try:
                tid_list = sht.crawler_tid_list(url)
                
                if not tid_list:
                    logger.warning(f"ç¬¬{page}é¡µçˆ¬å–å¤±è´¥ï¼Œè·³è¿‡")
                    continue
                
                for tid in tid_list:
                    detail_url = (
                        f"https://sehuatang.org/forum.php?mod=viewthread&tid={tid}"
                    )
                    
                    try:
                        data = sht.crawler_detail(detail_url)
                        if data:
                            # ä¿å­˜åˆ°æ•°æ®åº“
                            app = get_flask_app_context()
                            with app.app_context():
                                saved = sht.save_to_db(data, section_name, tid, detail_url)
                                if saved:
                                    logger.info(f"æˆåŠŸä¿å­˜èµ„æº: {data.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                                else:
                                    logger.info(f"èµ„æºå·²å­˜åœ¨ï¼Œè·³è¿‡: {data.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                    except Exception as e:
                        logger.error(f"TID {tid} çˆ¬å–å¤±è´¥: {e}")
                        
            except Exception as e:
                logger.error(f"çˆ¬å–åˆ†ç±» {section_name} ç¬¬{page}é¡µå¤±è´¥: {e}")
    
    logger.info("çˆ¬å–ä»»åŠ¡å®Œæˆ")


def run_crawling_with_options(section_fids=None, date_mode=None, date_value=None,
                              dateline=None, max_pages=3, crawl_options=None,
                              page_mode='fixed', page_range=None, task_type='manual'):
    """
    è¿è¡Œçˆ¬å–ä»»åŠ¡
    :param page_range: é¡µç èŒƒå›´ [start, end] list (å¯é€‰)
    """
    # ç”¨äºè·Ÿè¸ªé€šçŸ¥å‘é€çŠ¶æ€
    notification_sent = False

    try:
        global stop_event, pause_event
        stop_event.clear()
        pause_event.set() # ç¡®ä¿å¼€å§‹æ—¶éæš‚åœ

        # è¯»å–çŠ¶æ€å®¹å™¨ï¼ˆä¼˜å…ˆ Flask app.configï¼Œå…¶æ¬¡å…±äº«ç¼“å­˜ï¼‰
        try:
            from flask import current_app
            crawl_status = current_app.config.get('CRAWL_STATUS', {})
            crawl_control = current_app.config.get('CRAWL_CONTROL', {})
            crawl_progress = current_app.config.get('CRAWL_PROGRESS', {})
        except Exception:
            from cache_manager import cache_manager, CacheKeys
            crawl_status = cache_manager.shared_get(CacheKeys.CRAWL_STATUS) or {}
            crawl_control = cache_manager.shared_get(CacheKeys.CRAWL_CONTROL) or {}
            crawl_progress = cache_manager.shared_get(CacheKeys.CRAWL_PROGRESS) or {}

        # å¼ºåˆ¶é‡ç½®å¹¶è®¾ç½®çŠ¶æ€æœº
        try:
            from crawler_control.cc_control_bridge import get_crawler_control_bridge
            bridge = get_crawler_control_bridge()

            # 1. æ¸…é™¤æ‰€æœ‰æ—§ä¿¡å·
            bridge.queue_manager.clear_signals()
            logger.info("ğŸ§¹ å·²æ¸…é™¤æ‰€æœ‰æ—§ä¿¡å·")

            # 2. å¼ºåˆ¶é‡ç½®çŠ¶æ€åˆ°idleï¼ˆæ¸…é™¤æŒä¹…åŒ–çš„æ—§çŠ¶æ€ï¼‰
            bridge.coordinator.reset_state()
            logger.info("ğŸ”„ å·²é‡ç½®çŠ¶æ€æœºåˆ°idle")

            # 3. ç«‹å³è½¬æ¢åˆ°running
            bridge.coordinator.transition_state('running', {'started_at': time.time()})

            # 4. éªŒè¯çŠ¶æ€
            current_state = bridge.coordinator.get_current_state()
            logger.info(f"ğŸ“Š å½“å‰çŠ¶æ€æœºçŠ¶æ€: {current_state.current_state}")

            if current_state.current_state != 'running':
                # 5. å¦‚æœè¿˜ä¸æ˜¯runningï¼Œå¼ºåˆ¶è®¾ç½®
                logger.error(f"âŒ çŠ¶æ€å¼‚å¸¸ï¼Œå¼ºåˆ¶è®¾ç½®ä¸ºrunning")
                bridge.coordinator._current_state.current_state = 'running'
                bridge.coordinator._current_state.is_crawling = True
                bridge.coordinator._current_state.is_paused = False
                bridge.coordinator.force_persist()  # å¼ºåˆ¶æŒä¹…åŒ–
                logger.warning("âš ï¸ å·²å¼ºåˆ¶è®¾ç½®çŠ¶æ€ä¸ºrunning")
            else:
                logger.info("âœ… å·²é€šçŸ¥çŠ¶æ€æœºï¼šçˆ¬è™«è¿›å…¥runningçŠ¶æ€")

        except Exception as e:
            logger.error(f"âŒ çŠ¶æ€æœºåˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            # ä¸è¦ç»§ç»­ï¼Œå› ä¸ºæ§åˆ¶ç³»ç»Ÿä¸å·¥ä½œ
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–çˆ¬è™«æ§åˆ¶ç³»ç»Ÿ: {e}")
    
        # åº”ç”¨æœ€æ–°çš„æ—¥å¿—ç­‰çº§é…ç½®
        try:
            config_manager.apply_log_level()
        except Exception as e:
            logger.warning(f"åº”ç”¨æ—¥å¿—ç­‰çº§å¤±è´¥: {e}")
    
        logger.info(f"ğŸš€ å¼€å§‹ç­›é€‰çˆ¬å–: fids={section_fids}, date_mode={date_mode}, date_value={date_value}, dateline={dateline}, pages={max_pages}")
        logger.info(f"ğŸ“‹ çˆ¬å–é…ç½® - åˆ†ç±»æ•°: {len(section_fids) if section_fids else 'å…¨éƒ¨'}, æœ€å¤§é¡µæ•°: {max_pages}")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        update_crawl_state({'start_time': start_time})
        last_notification_time = start_time  # ç”¨äº5åˆ†é’Ÿå®šæ—¶é€šçŸ¥
        
        # æ˜¾ç¤ºæ—¥æœŸè¿‡æ»¤è®¾ç½®å’Œæ™ºèƒ½å»ºè®®
        if date_mode == 'all' or not date_mode:
            logger.info(f"ğŸ“… æ—¥æœŸè¿‡æ»¤: çˆ¬å–æ‰€æœ‰æ—¥æœŸçš„èµ„æº")
        elif date_mode == 'day' and date_value:
            logger.info(f"ğŸ“… æ—¥æœŸè¿‡æ»¤: ä»…çˆ¬å– {date_value} å‘å¸ƒçš„èµ„æº")
            # æ£€æŸ¥æ—¥æœŸæ˜¯å¦åˆç†
            # ç§»é™¤å†—ä½™å±€éƒ¨å¯¼å…¥ï¼Œæ”¹ç”¨å…¨å±€å¯¼å…¥
            try:
                target_date = datetime.strptime(date_value, '%Y-%m-%d').date()
                today = datetime.now().date()
                if target_date > today:
                    logger.warning(f"âš ï¸ è®¾ç½®çš„æ—¥æœŸ {date_value} æ˜¯æœªæ¥æ—¥æœŸï¼Œå¯èƒ½ä¸ä¼šæ‰¾åˆ°èµ„æº")
                elif (today - target_date).days > 30:
                    logger.info(f"ğŸ’¡ è®¾ç½®çš„æ—¥æœŸ {date_value} è¾ƒæ—©ï¼Œå»ºè®®ä½¿ç”¨æœˆä»½æ¨¡å¼æˆ–æ—¶é—´èŒƒå›´è¿‡æ»¤")
            except ValueError:
                logger.error(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯: {date_value}ï¼Œåº”ä¸º YYYY-MM-DD æ ¼å¼")
        elif date_mode == 'month' and date_value:
            logger.info(f"ğŸ“… æ—¥æœŸè¿‡æ»¤: ä»…çˆ¬å– {date_value} æœˆä»½å‘å¸ƒçš„èµ„æº")
            # æ£€æŸ¥æœˆä»½æ ¼å¼
            try:
                datetime.strptime(date_value, '%Y-%m')
            except ValueError:
                logger.error(f"âŒ æœˆä»½æ ¼å¼é”™è¯¯: {date_value}ï¼Œåº”ä¸º YYYY-MM æ ¼å¼")
        else:
            logger.info(f"ğŸ“… æ—¥æœŸè¿‡æ»¤: æ¨¡å¼={date_mode}, å€¼={date_value}")
        
        if dateline:
            # å°†ç§’æ•°è½¬æ¢ä¸ºå¯è¯»çš„æ—¶é—´æè¿°
            seconds = int(dateline)
            if seconds == 86400:
                time_desc = "è¿‘1å¤©"
            elif seconds == 604800:
                time_desc = "è¿‘1å‘¨"
            elif seconds == 2592000:
                time_desc = "è¿‘1æœˆ"
            elif seconds == 31536000:
                time_desc = "è¿‘1å¹´"
            else:
                days = seconds // 86400
                time_desc = f"è¿‘{days}å¤©"
            logger.info(f"â° æ—¶é—´èŒƒå›´è¿‡æ»¤: {time_desc} ({dateline} ç§’å†…çš„èµ„æº)")
        
        logger.debug(f"ğŸ” ä¼ å…¥çš„section_fidsç±»å‹: {type(section_fids)}, å†…å®¹: {section_fids}")
        
        from constants import SECTION_MAP, SECTION_NAME_TO_FID
        
        sht = SHT()
        
        chosen_items = []
        
        if not section_fids:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šåˆ†ç±»ï¼Œçˆ¬å–æ‰€æœ‰åˆ†ç±»
            chosen_items = list(SECTION_MAP.items())
            logger.debug("ğŸ”„ æœªæŒ‡å®šåˆ†ç±»ï¼Œå°†çˆ¬å–æ‰€æœ‰åˆ†ç±»")
        else:
            # ç»Ÿä¸€è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œå¢å¼ºé²æ£’æ€§
            section_fids = [str(fid) for fid in section_fids]
            logger.debug(f"ğŸ” ä¼ å…¥çš„section_fids: {section_fids}")
            logger.debug(f"ğŸ” SECTION_MAPçš„é”®: {list(SECTION_MAP.keys())}")
            logger.debug(f"ğŸ” SECTION_MAPçš„å€¼: {list(SECTION_MAP.values())}")
            
            # æ£€æŸ¥ä¼ å…¥çš„æ˜¯fidè¿˜æ˜¯åˆ†ç±»åç§°
            if all(fid in SECTION_MAP for fid in section_fids):
                # ä¼ å…¥çš„æ˜¯fid
                chosen_items = [(fid, SECTION_MAP[fid]) for fid in section_fids]
                logger.debug("ğŸ”„ æ£€æµ‹åˆ°ä¼ å…¥çš„æ˜¯fid")
            else:
                # ä¼ å…¥çš„æ˜¯åˆ†ç±»åç§°ï¼Œéœ€è¦è½¬æ¢ä¸ºfid
                logger.debug(f"ğŸ” SECTION_NAME_TO_FIDæ˜ å°„: {SECTION_NAME_TO_FID}")
                
                chosen_items = []
                for name in section_fids:
                    if name in SECTION_NAME_TO_FID:
                        fid = SECTION_NAME_TO_FID[name]
                        chosen_items.append((fid, name))
                        logger.debug(f"âœ… æˆåŠŸæ˜ å°„: '{name}' -> fid '{fid}'")
                    else:
                        logger.error(f"âŒ æ— æ³•æ‰¾åˆ°åˆ†ç±»åç§° '{name}' å¯¹åº”çš„fid")
                
                logger.debug("ğŸ”„ æ£€æµ‹åˆ°ä¼ å…¥çš„æ˜¯åˆ†ç±»åç§°ï¼Œå·²è½¬æ¢ä¸ºfid")
        
        if not chosen_items:
            logger.error(f"âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„åˆ†ç±»ï¼Œsection_fids={section_fids}")
            logger.error(f"âŒ å¯ç”¨çš„åˆ†ç±»åç§°: {list(SECTION_MAP.values())}")
            logger.error(f"âŒ å¯ç”¨çš„fid: {list(SECTION_MAP.keys())}")
            return
        
        logger.info(f"ğŸ“‹ å®é™…å¤„ç†çš„åˆ†ç±»: {[(fid, name) for fid, name in chosen_items]}")
    
        # ======== ç”Ÿæˆå¹¶å‘é€çˆ¬å–ä»»åŠ¡å¼€å§‹æŠ¥å‘Š ========
        try:
            task_type_text = "æ‰‹åŠ¨çˆ¬å–ä»»åŠ¡" if task_type == 'manual' else "è‡ªåŠ¨å®šæ—¶çˆ¬å–ä»»åŠ¡"
    
            # æ¨¡å¼æè¿°æ˜ å°„
            crawler_mode = config_manager.get('CRAWLER_MODE', 'async').lower()
            mode_desc_map = {
                'async': 'å¼‚æ­¥å¹¶å‘',
                'thread': 'å¤šçº¿ç¨‹',
                'sync': 'åŒæ­¥å•çº¿ç¨‹'
            }
            mode_desc = mode_desc_map.get(crawler_mode, crawler_mode)
    
            # æ„å»ºæ¿å—åˆ—è¡¨æ–‡æœ¬ï¼ˆå®Œæ•´åˆ—è¡¨ï¼‰
            if not section_fids:
                all_board_names = "å…¨éƒ¨æ¿å—"
            else:
                all_board_names = "ã€".join([name for _, name in chosen_items])
    
            # æ„å»ºæ—¶é—´èŒƒå›´æè¿° (ä¼˜åŒ–å)
            if dateline:
                seconds = int(dateline)
                day_map = {86400: "ä¸€å¤©å†…", 172800: "ä¸¤å¤©å†…", 259200: "ä¸‰å¤©å†…", 604800: "ä¸€å‘¨å†…", 2592000: "ä¸€ä¸ªæœˆå†…", 7776000: "ä¸‰ä¸ªæœˆå†…", 15552000: "åŠå¹´å†…", 31536000: "ä¸€å¹´å†…"}
                time_range = day_map.get(seconds, f"è¿‘{seconds // 86400}å¤©")
            elif date_mode:
                mode_map = {
                    'all': 'å…¨éƒ¨æ—¶é—´',
                    'day': 'ä¸€å¤©å†…', '1day': 'ä¸€å¤©å†…',
                    '2day': 'ä¸¤å¤©å†…',
                    '3day': 'ä¸‰å¤©å†…',
                    'week': 'ä¸€å‘¨å†…', '1week': 'ä¸€å‘¨å†…',
                    'month': 'ä¸€ä¸ªæœˆå†…', '1month': 'ä¸€ä¸ªæœˆå†…',
                    '3month': 'ä¸‰ä¸ªæœˆå†…',
                    '6month': 'åŠå¹´å†…',
                    'year': 'ä¸€å¹´å†…', '1year': 'ä¸€å¹´å†…'
                }
                time_range = mode_map.get(str(date_mode).lower(), date_mode)
                if date_value and date_mode in ['day', 'month']:
                    time_range = f"{date_value} ({time_range})"
            else:
                time_range = "å…¨éƒ¨æ—¶é—´"
    
            # é¡µæ•°æ¨¡å¼æè¿°
            if page_mode == 'fixed':
                page_mode_desc = "å›ºå®šæ¨¡å¼"
                page_desc = f"{max_pages}é¡µ"
            elif page_mode == 'full':
                page_mode_desc = "å…¨éƒ¨é¡µé¢"
                page_desc = f"{max_pages}é¡µ"
            elif page_mode == 'range' and page_range and len(page_range) == 2:
                # æŒ‡å®šèŒƒå›´æ—¶æ˜¾ç¤ºå…·ä½“é¡µç 
                start_page, end_page = page_range
                page_mode_desc = "æŒ‡å®šèŒƒå›´"
                page_desc = f"ç¬¬{start_page}-{end_page}é¡µ"
            else:
                page_mode_desc = "æŒ‡å®šèŒƒå›´"
                page_desc = f"{max_pages}é¡µ"
    
            # ç”Ÿæˆåˆå§‹é€šçŸ¥ - å»¶è¿Ÿåˆ°è·å–ç¬¬ä¸€ä¸ªæ¿å—ä¿¡æ¯åå†å‘é€
            initial_report_template = {
                'task_type': task_type_text,
                'all_boards': all_board_names,
                'time_range': time_range,
                'page_mode': page_mode_desc,
                'page_desc': page_desc,
                'mode': mode_desc
            }
    
        except Exception as report_err:
            logger.warning(f"ç”Ÿæˆçˆ¬å–æŠ¥å‘Šæ—¶å‡ºé”™ï¼ˆä¸å½±å“çˆ¬å–ä»»åŠ¡ï¼‰: {report_err}")
        # ======== æŠ¥å‘Šç”Ÿæˆç»“æŸ ========
        total_saved = 0
        total_skipped = 0
        total_failed = 0
        per_section = {name: {'saved': 0, 'skipped': 0, 'failed': 0} for name in [n for _, n in chosen_items]}
    
        # ä½¿ç”¨ç»Ÿä¸€çŠ¶æ€æ›´æ–°
        update_crawl_state({
            'sections_total': len(list(chosen_items)),
            'sections_done': 0,
            'max_pages': max_pages,
            'processed_pages': 0,
            'total_saved': 0,
            'total_skipped': 0,
            'start_time': start_time,
            'is_crawling': True,
            'message': 'æ­£åœ¨åˆå§‹åŒ–...'
        })
        
        # é¡µé¢çº§åˆ«ç»Ÿè®¡
        page_stats = {
            'successful_pages': [],  # æˆåŠŸçš„é¡µé¢
            'failed_pages': [],      # å¤±è´¥çš„é¡µé¢
            'total_pages_attempted': 0,
            'total_pages_successful': 0,
            'total_pages_failed': 0
        }
        
        # è®¡ç®—é¢„ä¼°æ€»é¡µæ•°
        estimated_total = 0
        for fid, section_name in chosen_items:
            if section_name:
                estimated_total += max_pages
    
        # ä½¿ç”¨ç»Ÿä¸€çŠ¶æ€æ›´æ–°
        update_crawl_state({'estimated_total_pages': estimated_total})
        
        # æ‰¹é‡è·å–æ¿å—ä¿¡æ¯ï¼Œé¿å…é‡å¤è¯·æ±‚ - ä¼˜åŒ–ç‰ˆæœ¬
        logger.info("ğŸ“Š æ‰¹é‡è·å–æ¿å—ä¿¡æ¯...")
        # è·å–æ•°æ®åº“ä¸­çš„æ‰€æœ‰åˆ†ç±»
        categories = Category.get_all_categories()
        all_forums_info = {c.fid: c.to_dict() for c in categories}
    
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ¿å—ä¿¡æ¯
        needs_refresh = False
        
        # é€»è¾‘ï¼š1. æ•°æ®åº“æ— ä¿¡æ¯ 2. æ¿å—ç»Ÿè®¡å…¨ä¸º0 3. è·ç¦»ä¸Šæ¬¡æ›´æ–°è¶…è¿‡24å°æ—¶
        if not all_forums_info:
            logger.info("ğŸ“‹ æ•°æ®åº“æ— æ¿å—ä¿¡æ¯ï¼Œéœ€è¦åˆå§‹åŒ–è·å–")
            needs_refresh = True
        else:
            # æ›´æ™ºèƒ½çš„é™ˆæ—§æ£€æµ‹é€»è¾‘
            # åªæœ‰å½“ï¼š1. å…³é”®æ•°æ®ç¼ºå¤±(total_topics=0) ä¸” 2. æ—¶é—´ç¡®å®å¾ˆæ—§ï¼Œæ‰è§¦å‘åˆ·æ–°
            stale_threshold = datetime.now(timezone.utc) - timedelta(hours=24)
            
            # æ£€æŸ¥å½“å‰é€‰ä¸­çš„æ¿å—æ˜¯å¦çœŸçš„éœ€è¦åŒæ­¥
            target_fids = [str(fid) for fid, _ in chosen_items]
            need_sync_fids = []
            for fid in target_fids:
                info = all_forums_info.get(fid)
                if not info:
                    need_sync_fids.append(fid)
                    continue
                
                # æ£€æŸ¥æ›´æ–°æ—¶é—´
                last_upd = info.get('last_updated')
                if isinstance(last_upd, str):
                    try:
                        last_upd = datetime.fromisoformat(last_upd)
                    except (ValueError, TypeError):
                        last_upd = None
                
                if last_upd and last_upd.tzinfo is None:
                    last_upd = last_upd.replace(tzinfo=timezone.utc)
                
                # åˆ¤æ®ï¼šæ•°æ®ç¼ºå¤± æˆ– æ—¶é—´è¶…è¿‡24å°æ—¶
                if (not info.get('total_topics')) or (not last_upd) or (last_upd < stale_threshold):
                    need_sync_fids.append(fid)

            if not need_sync_fids:
                logger.info(f"ğŸ“‹ æ•°æ®åº“ä¿¡æ¯å°šåœ¨æœ‰æ•ˆæœŸå†…ï¼Œå…± {len(all_forums_info)} ä¸ªæ¿å—")
                needs_refresh = False
            else:
                logger.info(f"ğŸ•’ æ£€æµ‹åˆ° {len(need_sync_fids)} ä¸ªæ¿å—ä¿¡æ¯éœ€åŒæ­¥ï¼Œæ­£åœ¨æ›´æ–°...")
                needs_refresh = True
    
        if needs_refresh:
            logger.info("ğŸ”„ æ­£åœ¨å®æ—¶è·å–æ¿å—ä¿¡æ¯...")
            fresh_info = sht.get_all_forums_info()
            if fresh_info:
                # ä¿å­˜åˆ°æ•°æ®åº“
                Category.update_forum_info(fresh_info)
                all_forums_info = fresh_info
                logger.info(f"âœ… æ¿å—ä¿¡æ¯å·²æ›´æ–°å¹¶ä¿å­˜ï¼Œå…± {len(all_forums_info)} ä¸ªæ¿å—")
            else:
                logger.warning("âš ï¸ è·å–æ¿å—ä¿¡æ¯å¤±è´¥ï¼Œä½¿ç”¨ç°æœ‰æ•°æ®")
                if not all_forums_info:
                    all_forums_info = {}
    
        # é‡æ–°è®¡ç®—é¢„ä¼°æ€»é¡µæ•°ï¼ˆåŸºäºå®é™…æ¿å—ä¿¡æ¯å’Œé¡µæ•°æ¨¡å¼ï¼‰
        try:
            corrected_estimated_total = 0
            for fid, section_name in chosen_items:
                if not section_name:
                    continue
    
                forum_info = all_forums_info.get(fid)
                actual_total_pages = forum_info.get('total_pages', 0) if forum_info else 0
    
                # æ ¹æ®é¡µæ•°æ¨¡å¼è®¡ç®—è¯¥æ¿å—çš„å®é™…çˆ¬å–é¡µæ•°
                if page_range:
                    # èŒƒå›´æ¨¡å¼ï¼šä½¿ç”¨èŒƒå›´çš„å®é™…å·®å€¼
                    start_page_range = int(page_range[0])
                    end_page_range = int(page_range[1])
                    actual_end_page = min(end_page_range, max(actual_total_pages, 1))
                    actual_start_page = max(start_page_range, 1)
                    section_adjusted_pages = max(1, actual_end_page - actual_start_page + 1)
                else:
                    # å›ºå®šé¡µæ•°æˆ–å…¨éƒ¨æ¨¡å¼
                    if page_mode == 'full':
                        section_adjusted_pages = max(1, actual_total_pages)
                    else:
                        # å›ºå®šé¡µæ•°æ¨¡å¼ï¼Œä½†ä¸è¶…è¿‡æ¿å—æœ€å¤§é¡µæ•°
                        section_adjusted_pages = max(1, min(max_pages, max(actual_total_pages, 1)))
    
                corrected_estimated_total += section_adjusted_pages
    
            # æ›´æ–°é¢„ä¼°æ€»é¡µæ•°
            if corrected_estimated_total > 0:
                update_crawl_state({'estimated_total_pages': corrected_estimated_total})
                logger.info(f"ğŸ“Š æ ¡æ­£é¢„ä¼°æ€»é¡µæ•°: {estimated_total} -> {corrected_estimated_total} é¡µ")
        except Exception as e:
            logger.warning(f"âš ï¸ æ ¡æ­£é¢„ä¼°æ€»é¡µæ•°å¤±è´¥ï¼Œä½¿ç”¨åˆå§‹å€¼: {e}")
            # ä¿æŒä½¿ç”¨åˆå§‹è®¡ç®—çš„ estimated_total
    
        for fid, section_name in chosen_items:
            if not section_name:
                continue
            logger.info(f"ğŸ“‚ å¼€å§‹çˆ¬å–åˆ†ç±»: {section_name} (fid={fid})")
            # ä½¿ç”¨ç»Ÿä¸€çŠ¶æ€æ›´æ–°
            update_crawl_state({
                'current_section': section_name,
                'current_section_saved': 0,
                'current_section_skipped': 0
            })
            
            # --- [ä¼˜åŒ–] å¢é‡åŒæ­¥æ°´ä½çº¿é”šç‚¹ ---
            try:
                # è·å–è¯¥æ¿å—ç›®å‰æ•°æ®åº“é‡Œçš„æœ€å¤§ TID ä½œä¸ºç»ˆæ­¢é”šç‚¹
                stop_tid = db.session.query(func.max(Resource.tid)).filter(Resource.section == section_name).scalar() or 0
                logger.info(f"ğŸ“ [{section_name}] å¢é‡åŒæ­¥æ°´ä½çº¿: {stop_tid}")
            except Exception as e:
                stop_tid = 0
                logger.warning(f"âš ï¸ è·å–æ°´ä½çº¿å¤±è´¥: {e}")
    
            # æ¿å—é”™è¯¯è®¡æ•°å™¨ï¼Œç”¨äºä¸¥é‡é”™è¯¯é€šçŸ¥
            section_error_count = 0
            section_error_notified = False  # é¿å…é‡å¤é€šçŸ¥
            
            # ä»æ‰¹é‡è·å–çš„ä¿¡æ¯ä¸­æŸ¥æ‰¾æ¿å—ä¿¡æ¯ï¼Œé¿å…é‡å¤è¯·æ±‚
            forum_info = all_forums_info.get(fid)
            if forum_info:
                actual_total_pages = forum_info.get('total_pages') or 0
                total_topics = forum_info.get('total_topics') or 0
    
                logger.info(f"ğŸ“Š [{section_name}] æ¿å—ç»Ÿè®¡: æ€»è®¡{total_topics}ä¸»é¢˜, å…±{actual_total_pages}é¡µ")
    
                # ç‰¹æ®Šå¤„ç†ï¼š0ä¸»é¢˜çš„æ¿å—
                if total_topics == 0 or actual_total_pages == 0:
                    logger.warning(f"âš ï¸ [{section_name}] æ¿å—æ˜¾ç¤º0ä¸»é¢˜/0é¡µï¼ˆå¯èƒ½æ˜¯æ–°æ¿å—æˆ–ä¿¡æ¯æœªæ›´æ–°ï¼‰")
                    logger.info(f"ğŸ’¡ [{section_name}] å°†å¼ºåˆ¶çˆ¬å–ç¬¬1é¡µä»¥éªŒè¯æ¿å—çŠ¶æ€")
                    # å¼ºåˆ¶çˆ¬å–è‡³å°‘1é¡µæ¥éªŒè¯
                    adjusted_pages = 1
                    actual_total_pages = max(1, actual_total_pages)  # ç¡®ä¿actual_total_pagesä¹Ÿè‡³å°‘ä¸º1
                else:
                    # æ­£å¸¸æƒ…å†µï¼šæ™ºèƒ½è°ƒæ•´çˆ¬å–é¡µæ•°
                    if max_pages > actual_total_pages:
                        adjusted_pages = actual_total_pages
                        logger.info(f"ğŸ“‰ [{section_name}] è°ƒæ•´çˆ¬å–é¡µæ•°: {max_pages} -> {adjusted_pages} (æ¿å—æ€»é¡µæ•°é™åˆ¶)")
                    else:
                        adjusted_pages = max_pages
    
                    # å†…å®¹ä¸°å¯Œåº¦æç¤º
                    if total_topics > 1000:
                        logger.info(f"ğŸ”¥ [{section_name}] å†…å®¹ä¸°å¯Œ({total_topics}ä¸»é¢˜)ï¼Œå»ºè®®é€‚å½“å¢åŠ çˆ¬å–é¡µæ•°")
    
                    # æ™ºèƒ½é¡µæ•°ä¼˜åŒ–ï¼šå¦‚æœä¸»é¢˜å¾ˆå°‘ï¼Œå‡å°‘çˆ¬å–é¡µæ•°
                    if total_topics < 50 and adjusted_pages > 2:
                        adjusted_pages = 2
                        logger.info(f"ğŸ“‰ [{section_name}] ä¸»é¢˜è¾ƒå°‘({total_topics})ï¼Œä¼˜åŒ–é¡µæ•°ä¸º: {adjusted_pages}")
                    elif total_topics < 20 and adjusted_pages > 1:
                        adjusted_pages = 1
                        logger.info(f"ğŸ“‰ [{section_name}] ä¸»é¢˜å¾ˆå°‘({total_topics})ï¼Œä¼˜åŒ–é¡µæ•°ä¸º: {adjusted_pages}")
            else:
                adjusted_pages = max_pages
                actual_total_pages = max_pages  # é»˜è®¤å€¼
                logger.warning(f"âš ï¸ [{section_name}] æ— æ³•è·å–æ¿å—ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤é¡µæ•°: {max_pages}")
            
            # ç¡®ä¿ display_total_pages æœ‰é»˜è®¤å€¼
            display_total_pages = actual_total_pages
    
            # æ ¹æ®é¡µæ•°æ¨¡å¼å†³å®šçˆ¬å–ç­–ç•¥
            start_page = 1
            page_order = "å‡åº"  # ä»ç¬¬1é¡µåˆ°ç¬¬Né¡µ
    
            if page_range: # å†æ¬¡ç¡®è®¤èŒƒå›´æ¨¡å¼
                start_page_range = int(page_range[0])
                end_page_range = int(page_range[1])
                # è®¡ç®—å®é™…ç»“æŸé¡µï¼ˆä¸èƒ½è¶…è¿‡è®ºå›æ€»é¡µæ•°ï¼‰
                actual_end_page = min(end_page_range, actual_total_pages)
                # é‡æ–°è®¡ç®—å®é™…çˆ¬å–é¡µæ•°
                adjusted_pages = actual_end_page - start_page_range + 1
                # æ˜¾ç¤ºçš„æ€»é¡µæ•°ç”¨è®ºå›å®é™…æ€»é¡µæ•°
                display_total_pages = actual_total_pages
                # é‡æ–°ç”Ÿæˆé¡µç åˆ—è¡¨ï¼ˆé‡è¦ï¼ï¼‰
                pages_to_crawl = range(start_page_range, actual_end_page + 1)
                logger.info(f"ğŸ“Š èŒƒå›´æ¨¡å¼ç¡®è®¤:")
                logger.info(f"   - å®é™…çˆ¬å–: ç¬¬{start_page_range}é¡µ åˆ° ç¬¬{actual_end_page}é¡µ")
                logger.info(f"   - æ€»ä»»åŠ¡é¡µæ•°: {adjusted_pages}é¡µ")
                logger.info(f"   - æ˜¾ç¤ºæ ¼å¼: ç¬¬X/{display_total_pages}é¡µ")
            
            if not page_range:
                if page_mode == 'fixed':
                # å›ºå®šé¡µæ•°æ¨¡å¼ï¼šä»ç¬¬1é¡µå¼€å§‹çˆ¬å–æŒ‡å®šé¡µæ•°
                # æ³¨æ„ï¼šå¦‚æœæ˜¯0ä¸»é¢˜çš„ç‰¹æ®Šæƒ…å†µï¼Œadjusted_pageså·²ç»è¢«è®¾ç½®ä¸º1ï¼Œè¿™é‡Œä¸å†è¦†ç›–
                    start_page = 1
                    page_order = "å‡åº"
                    display_total_pages = actual_total_pages
                    logger.info(f"ğŸ“Š [{section_name}] å›ºå®šé¡µæ•°æ¨¡å¼: ä»ç¬¬1é¡µå¼€å§‹çˆ¬å–{adjusted_pages}é¡µ")
                
                elif page_mode == 'full':
                    # å…¨éƒ¨é¡µé¢æ¨¡å¼ï¼šçˆ¬å–æ‰€æœ‰é¡µé¢
                    start_page = 1
                    adjusted_pages = actual_total_pages
                    page_order = "å‡åº"
                    display_total_pages = actual_total_pages
                    logger.info(f"ğŸ“Š [{section_name}] å…¨éƒ¨é¡µé¢æ¨¡å¼: çˆ¬å–å…¨éƒ¨{adjusted_pages}é¡µ")
    
            # æ›´æ–°è¿›åº¦ä¿¡æ¯
            # ä½¿ç”¨ç»Ÿä¸€çŠ¶æ€æ›´æ–°
            update_crawl_state({
                'current_section_pages': adjusted_pages,
                'current_section_processed': 0
            })
    
            # å‘é€æ¿å—é€šçŸ¥
            current_board_index = chosen_items.index((fid, section_name))
    
            # è®¡ç®—å®é™…é¡µç èŒƒå›´
            if page_range and len(page_range) == 2:
                actual_page_range = f"ç¬¬{page_range[0]}-{actual_end_page}é¡µ"
            else:
                actual_page_range = f"ç¬¬1-{adjusted_pages}é¡µ"
    
            if current_board_index == 0:
                # å‘é€åˆå§‹ä»»åŠ¡é€šçŸ¥
                try:
                    # æ„å»ºå€™é€‰æ¿å—åˆ—è¡¨
                    pending_boards = [name for _, name in chosen_items[1:]]
                    pending_text = "ã€".join(pending_boards) if pending_boards else "æ— "
    
                    context = {
                        'task_type': initial_report_template.get('task_type'),
                        'all_boards': initial_report_template.get('all_boards'),
                        'time_range': initial_report_template.get('time_range'),
                        'page_mode': initial_report_template.get('page_mode'),
                        'page_desc': initial_report_template.get('page_desc'),
                        'mode': initial_report_template.get('mode'),
                        'section_name': section_name,
                        'actual_page_range': actual_page_range,
                        'pending_boards': pending_text,
                        'initial_report_template': initial_report_template
                    }
    
                    initial_msg, parse_mode = render_message_template('initial_report', context)
                    if not initial_msg:
                        initial_msg = f"""ğŸš€ *å¼€å§‹{initial_report_template['task_type']}ï¼Œæœ¬æ¬¡çˆ¬å–é…ç½®ï¼š*
    æ¿å—ï¼š{initial_report_template['all_boards']}
    æ—¶é—´ï¼š{initial_report_template['time_range']}
    é¡µæ•°ï¼š{initial_report_template['page_mode']} \\- {initial_report_template['page_desc']}
    æ¨¡å¼ï¼š{initial_report_template['mode']}

    â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    ğŸ“‚ å½“å‰è¿›è¡Œä¸­çš„æ¿å—ï¼š{section_name}
    ğŸ“„ æ¿å— {section_name} çš„å®é™…ä»»åŠ¡é¡µæ•°ï¼š{actual_page_range}
    â³ å€™é€‰ä¸­çš„æ¿å—ï¼š{pending_text}"""
                        parse_mode = 'Markdown'
    
                    _send_telegram_message(initial_msg, parse_mode=parse_mode)
                    logger.info(f"âœ… å·²å‘é€åˆå§‹ä»»åŠ¡é€šçŸ¥")
                except Exception as e:
                    logger.debug(f"å‘é€åˆå§‹ä»»åŠ¡é€šçŸ¥å¤±è´¥: {e}")
    
            else:
                # å‘é€æ¿å—åˆ‡æ¢é€šçŸ¥
                try:
                    # æ„å»ºå·²å®Œæˆæ¿å—åˆ—è¡¨
                    completed_boards = [name for _, name in chosen_items[:current_board_index]]
                    completed_text = "ã€".join(completed_boards)
    
                    # æ„å»ºå€™é€‰æ¿å—åˆ—è¡¨
                    pending_boards = [name for _, name in chosen_items[current_board_index + 1:]]
                    pending_text = "ã€".join(pending_boards) if pending_boards else "æ— "
    
                    # ä¸Šä¸€ä¸ªæ¿å—åç§°
                    prev_board_name = chosen_items[current_board_index - 1][1]
    
                    context = {
                        'task_type': initial_report_template.get('task_type'),
                        'all_boards': initial_report_template.get('all_boards'),
                        'time_range': initial_report_template.get('time_range'),
                        'page_mode': initial_report_template.get('page_mode'),
                        'page_desc': initial_report_template.get('page_desc'),
                        'mode': initial_report_template.get('mode'),
                        'section_name': section_name,
                        'actual_page_range': actual_page_range,
                        'completed_boards': completed_text,
                        'pending_boards': pending_text,
                        'prev_board_name': prev_board_name,
                        'initial_report_template': initial_report_template
                    }
    
                    board_switch_msg, parse_mode = render_message_template('board_switch', context)
                    if not board_switch_msg:
                        board_switch_msg = f"""âœ… *{prev_board_name} æ¿å—å·²å®Œæˆï¼Œå¼€å§‹çˆ¬å–å€™é€‰æ¿å—*
    
    æœ¬æ¬¡çˆ¬å–é…ç½®ï¼š
    æ¿å—ï¼š{initial_report_template['all_boards']}
    æ—¶é—´ï¼š{initial_report_template['time_range']}
    é¡µæ•°ï¼š{initial_report_template['page_mode']} - {initial_report_template['page_desc']}
    æ¨¡å¼ï¼š{initial_report_template['mode']}
    
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    ğŸ“‚ å½“å‰è¿›è¡Œä¸­çš„æ¿å—ï¼š{section_name}
    ğŸ“„ æ¿å— {section_name} çš„å®é™…ä»»åŠ¡é¡µæ•°ï¼š{actual_page_range}
    âœ… å·²å®Œæˆçš„æ¿å—ï¼š{completed_text}
    â³ å€™é€‰ä¸­çš„æ¿å—ï¼š{pending_text}"""
                        parse_mode = 'Markdown'
    
                    _send_telegram_message(board_switch_msg, parse_mode=parse_mode)
                    logger.info(f"âœ… å·²å‘é€æ¿å—åˆ‡æ¢é€šçŸ¥ ({prev_board_name} â†’ {section_name})")
                except Exception as e:
                    logger.debug(f"å‘é€æ¿å—åˆ‡æ¢é€šçŸ¥å¤±è´¥: {e}")
    
            # æ ¹æ®é¡µæ•°é¡ºåºç”Ÿæˆé¡µç åˆ—è¡¨ï¼ˆä»…åœ¨éèŒƒå›´æ¨¡å¼ä¸‹ï¼‰
            if not page_range:
                if page_order == "é™åº":
                    pages_to_crawl = range(actual_total_pages, start_page - 1, -1)
                    logger.info(f"ğŸ“„ [{section_name}] çˆ¬å–é¡ºåº: é™åº (ç¬¬{actual_total_pages}é¡µ -> ç¬¬{start_page}é¡µ)")
                else:
                    pages_to_crawl = range(start_page, start_page + adjusted_pages)
                    logger.info(f"ğŸ“„ [{section_name}] çˆ¬å–é¡ºåº: å‡åº (ç¬¬{start_page}é¡µ -> ç¬¬{start_page + adjusted_pages - 1}é¡µ)")
            else:
                # èŒƒå›´æ¨¡å¼ä¸‹ï¼Œpages_to_crawl å·²ç»åœ¨ç¬¬399è¡Œæ­£ç¡®è®¾ç½®
                logger.info(f"ğŸ“„ [{section_name}] çˆ¬å–é¡ºåº: å‡åº (ç¬¬{start_page_range}é¡µ -> ç¬¬{actual_end_page}é¡µ)")
            
    
            # æ£€æŸ¥æ˜¯å¦ä»æš‚åœæ¢å¤ï¼Œå¦‚æœæ˜¯åˆ™ä»ä¿å­˜çš„ä½ç½®ç»§ç»­
            resume_offset = 0
            try:
                from crawler_control.cc_control_bridge import get_crawler_control_bridge
                bridge = get_crawler_control_bridge()
                saved_loop_state = bridge.coordinator.get_page_loop_state()
                
                if saved_loop_state and saved_loop_state.get('section_name') == section_name:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯åŒä¸€ä¸ªåˆ†ç±»çš„æ¢å¤
                    resume_offset = saved_loop_state.get('current_offset', 0)
                    if resume_offset > 0:
                        logger.info(f"ğŸ“ ä»æš‚åœç‚¹æ¢å¤: åˆ†ç±»={section_name}, ä»åç§»é‡={resume_offset} ç»§ç»­")
                        # æ¸…é™¤ä¿å­˜çš„çŠ¶æ€
                        bridge.coordinator._current_state.progress.pop('page_loop_state', None)
                        bridge.coordinator.force_persist()
            except Exception as e:
                logger.warning(f"âš ï¸ æ¢å¤æš‚åœçŠ¶æ€å¤±è´¥: {e}")
            
            # åœ¨å¾ªç¯ä¸­ä½¿ç”¨ï¼Œæ”¯æŒä»ä¿å­˜çš„åç§»é‡ç»§ç»­
            pages_to_crawl_list = list(pages_to_crawl)
            pages_to_process = pages_to_crawl_list[resume_offset:]
            
            # v1.4.0: åŠ é€ŸæŸ¥æ¼æ¨¡å¼ (Burst Mode)
            # å¦‚æœæ˜¯å¼‚æ­¥æ¨¡å¼ä¸”é¡µæ•°è¾ƒå¤šï¼Œå¼€å¯åˆ†é¡µæ‰¹å¤„ç†ï¼Œå¹¶å‘è·å–å¤šé¡µ TID
            crawler_mode = config_manager.get('CRAWLER_MODE', 'async').lower()
            page_batch_size = 5 if crawler_mode == 'async' else 1
            
            if page_batch_size > 1:
                logger.info(f"âš¡ [{section_name}] å¯ç”¨åŠ é€ŸæŸ¥æ¼æ¨¡å¼: æ¯æ‰¹æ¬¡å¹¶å‘å¤„ç† {page_batch_size} é¡µåˆ—è¡¨")
            
            i = 0
            while i < len(pages_to_process):
                batch_indices = pages_to_process[i:i + page_batch_size]
                from .utils import check_stop_and_pause
                if check_stop_and_pause(): break
                burst_results = []
                if page_batch_size > 1:
                    # v1.4.6: [ä¼˜åŒ–] æ„é€ URLå¹¶åŒæ­¥UIçŠ¶æ€
                    burst_urls = [f'https://sehuatang.org/forum.php?mod=forumdisplay&fid={fid}&mobile=2&page={p}' for p in batch_indices]
                    if dateline and str(dateline).strip() and str(dateline).strip() != '0':
                        dl_v = str(dateline).strip()
                        burst_urls = [f'{u}&orderby=dateline&filter=dateline&dateline={dl_v}' for u in burst_urls]
                    
                    target_pages_desc = f"ç¬¬{batch_indices[0]}-{batch_indices[-1]}é¡µ"
                    update_crawl_state({
                        'message': f'æ­£åœ¨å¹¶å‘æ‰«æ [{section_name}] {target_pages_desc}...',
                        'current_page_actual': batch_indices[0]
                    })

                    try:
                        p_a = sht.proxies.get('http') if (hasattr(sht, 'proxies') and sht.proxies) else None
                        c_a = sht.cookie if hasattr(sht, 'cookie') else {'_safe': ''}
                        async def f_b():
                            async with AsyncSHTCrawler(max_connections=page_batch_size, proxy=p_a, cookies=c_a) as c:
                                return await c.crawl_tids_batch(burst_urls)
                        burst_results = run_async(f_b(), timeout=60.0)
                    except Exception as burst_err:
                        import traceback
                        logger.error(f"âŒ [BURST] åˆ—è¡¨æ‰¹é‡è·å–è‡´å‘½å¼‚å¸¸: {burst_err}")
                        logger.debug(traceback.format_exc())
                        burst_results = [[] for _ in batch_indices]
                else:
                    p_idx = batch_indices[0]
                    u = f'https://sehuatang.org/forum.php?mod=forumdisplay&fid={fid}&mobile=2&page={p_idx}'
                    if dateline and str(dateline).strip() and str(dateline).strip() != '0':
                        u += f'&orderby=dateline&filter=dateline&dateline={str(dateline).strip()}'
                    try: 
                        burst_results = [sht.crawler_tid_list(u) or []]
                    except Exception as sync_err:
                        logger.error(f"âŒ [SYNC] åŒæ­¥è·å–TIDåˆ—è¡¨å¤±è´¥: {sync_err}")
                        burst_results = [[]]
                
                batch_tasks = []
                reached_boundary = False
                
                # --- æ­¥éª¤ 2: æ±‡æ€»ç¼ºå¤±è¯¦æƒ…ä»»åŠ¡ ---
                for offset, page_tids in enumerate(burst_results):
                    curr_p = batch_indices[offset]
                    p_idx_curr = resume_offset + i + offset + 1
                    
                    # è¿›åº¦æŠ¥å‘Š
                    sect_prog_curr = (p_idx_curr / adjusted_pages) * 100
                    pg_disp_curr = f"ç¬¬{curr_p}/{display_total_pages}é¡µ"
                    
                    update_crawl_state({
                        'current_page_actual': curr_p,
                        'max_pages_actual': display_total_pages,
                        'current_page_task': p_idx_curr,
                        'max_pages_task': adjusted_pages,
                        'current_page': curr_p,
                        'progress_percent': round(sect_prog_curr, 1),
                        'message': f'æ­£åœ¨æ‰«æ [{section_name}] {pg_disp_curr} ({p_idx_curr}/{adjusted_pages})...'
                    })

                    # å¿ƒè·³ç›‘æ§
                    try:
                        heartbeat_interval = int(config_manager.get('HEARTBEAT_INTERVAL', 60))
                        cur_t = time.time()
                        if cur_t - last_notification_time >= heartbeat_interval:
                            elapsed_m = int((cur_t - start_time) / 60)
                            total_prog = int((crawl_progress.get('processed_pages', 0) / max(crawl_progress.get('estimated_total_pages', 1), 1)) * 100)
                            
                            heart_ctx = {
                                'elapsed_minutes': elapsed_m, 'section_name': section_name,
                                'page_display': pg_disp_curr, 'section_progress_percent': f"{sect_prog_curr:.1f}",
                                'task_progress_display': f"{p_idx_curr}/{adjusted_pages}",
                                'total_progress_percent': total_prog,
                                'processed_pages': crawl_progress.get('processed_pages', 0),
                                'estimated_total_pages': crawl_progress.get('estimated_total_pages', 0),
                                'total_saved': total_saved, 'total_skipped': total_skipped,
                                'total_failed': total_failed, 'timestamp': _dt.datetime.now().strftime('%H:%M:%S')
                            }
                            h_msg, p_mode = render_message_template('heartbeat', heart_ctx)
                            if not h_msg: # Fallback
                                h_msg = f"ğŸ’“ *Burst Mode è¿è¡Œä¸­*\nâ±ï¸ å·²è¿è¡Œ: {elapsed_m}m\nğŸ“‚ æ¿å—: {section_name}\nğŸ“„ è¿›åº¦: {pg_disp_curr} ({sect_prog_curr:.1f}%)\nâœ… å·²å­˜: {total_saved}\nâŒ å¤±è´¥: {total_failed}"
                                p_mode = 'Markdown'
                            _send_telegram_message(h_msg, parse_mode=p_mode)
                            last_notification_time = cur_t
                    except: pass

                    if not page_tids:
                        logger.warning(f"âš ï¸ [{section_name}] {pg_disp_curr} æ‰«æå¤±è´¥ï¼Œå·²è®°å½•ä»¥å¾…åç»­é‡è¯•")
                        page_stats['total_pages_failed'] += 1
                        
                        # è®°å½•åˆ°é¡µé¢ç»Ÿè®¡ç”¨äºæœ€åæ±‡æ€»
                        page_stats['failed_pages'].append({
                            'section': section_name,
                            'page': curr_p,
                            'reason': 'åˆ—è¡¨æ‰«æå¤±è´¥'
                        })
                        
                        # è®°å½•åˆ°å…¨å±€å¾…é‡è¯•åˆ—è¡¨
                        if 'failed_pages' not in crawl_progress:
                            crawl_progress['failed_pages'] = []
                        
                        # æ„å»ºè¯¥é¡µé¢çš„å®Œæ•´URLç”¨äºé‡è¯•æ—¶å®šä½
                        retry_url = f"https://sehuatang.org/forum.php?mod=forumdisplay&fid={fid}&mobile=2&page={curr_p}"
                        if dateline and str(dateline).strip() and str(dateline).strip() != '0':
                            retry_url += f"&orderby=dateline&filter=dateline&dateline={str(dateline).strip()}"
                            
                        crawl_progress['failed_pages'].append({
                            'section_name': section_name,
                            'section_fid': fid,
                            'page': curr_p,
                            'url': retry_url
                        })
                        continue
                    
                    page_stats['total_pages_attempted'] += 1
                    
                    # å†å²è¾¹ç•Œæ£€æŸ¥
                    if stop_tid > 0 and curr_p == 1:
                        if sum(1 for t in page_tids if t <= stop_tid) > 3 or (page_tids and max(page_tids) <= stop_tid):
                            logger.info(f"â­ï¸ [{section_name}] è§¦ç¢°å¢é‡æ°´ä½çº¿ (TID <= {stop_tid})")
                            reached_boundary = True
                    
                    # æ‰¹é‡è¿‡æ»¤
                    try:
                        e_tids = db.session.query(Resource.tid).filter(Resource.tid.in_(page_tids)).all()
                        e_set = {t[0] for t in e_tids}
                        f_cnt = 0
                        for tid in page_tids:
                            if tid not in e_set:
                                batch_tasks.append((tid, f'https://sehuatang.org/forum.php?mod=viewthread&tid={tid}'))
                            else:
                                total_skipped += 1
                                per_section[section_name]['skipped'] += 1
                                f_cnt += 1
                        if f_cnt > 0:
                            logger.info(f"ğŸ” [{section_name}] {pg_disp_curr} è¿‡æ»¤æ‰ {f_cnt} ä¸ªæ•°æ®åº“å·²æœ‰èµ„æº")
                    except Exception as e:
                        logger.warning(f"âš ï¸ åº“è¿‡æ»¤å¤±è´¥: {e}")
                    
                    if reached_boundary: break

                # --- æ­¥éª¤ 3: æå–è¯¦æƒ… (å¹¶å‘æ‰§è¡Œ) ---
                if batch_tasks:
                    logger.info(f"ğŸš€ [{section_name}] å‘ç° {len(batch_tasks)} ä¸ªæ–°å¢èµ„æºï¼Œå¼€å§‹å¹¶å‘è¯¦æƒ…é‡‡é›†...")
                    m_urls = [t[1] for t in batch_tasks]
                    m_results = []
                    
                    try:
                        # v1.5.3: [æ ¹æœ¬ä¿®å¤] è¯¦æƒ…é‡‡é›†å¼ºåˆ¶ä½¿ç”¨çº¿ç¨‹æ± æ¨¡å¼
                        # åŸå› ï¼šasync + curl_cffi åœ¨æŸäº›ç½‘ç»œæ¡ä»¶ä¸‹ä¼šè¿›å…¥æ— æ³•æ¢å¤çš„æ­»é”
                        # çº¿ç¨‹æ± è™½ç„¶æ…¢ä¸€ç‚¹ï¼Œä½†ç»å¯¹ä¸ä¼šå¡æ­»
                        force_thread_mode = config_manager.get('FORCE_THREAD_DETAIL_CRAWL', True)
                        
                        if crawler_mode == 'async' and not force_thread_mode:
                            # ä»…åœ¨ç”¨æˆ·æ˜ç¡®ç¦ç”¨å¼ºåˆ¶çº¿ç¨‹æ¨¡å¼æ—¶æ‰ä½¿ç”¨å¼‚æ­¥
                            logger.warning(f"âš ï¸ [{section_name}] ä½¿ç”¨å¼‚æ­¥æ¨¡å¼é‡‡é›†è¯¦æƒ…ï¼ˆå¯èƒ½å­˜åœ¨å¡æ­»é£é™©ï¼‰")
                            max_c = config_manager.get('CRAWLER_MAX_CONCURRENCY', 20)
                            p_a = sht.proxies.get('http') if sht.proxies else None
                            c_a = sht.cookie if hasattr(sht, 'cookie') else {'_safe': ''}
                            
                            batch_start_time = time.time()
                            logger.info(f"ğŸ“¡ [{section_name}] å¼€å§‹å¼‚æ­¥æ‰¹é‡é‡‡é›† {len(m_urls)} ä¸ªè¯¦æƒ…é¡µ...")
                            
                            async def fetch_details():
                                async with AsyncSHTCrawler(max_connections=max_c, proxy=p_a, cookies=c_a) as c:
                                    return await c.crawl_details_batch(m_urls)
                            
                            detail_timeout = min(120, len(m_urls) * 10)
                            
                            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                                future = executor.submit(run_async, fetch_details(), detail_timeout)
                                poll_start = time.time()
                                task_abandoned = False
                                
                                while True:
                                    if stop_event.is_set() or check_stop_and_pause():
                                        logger.warning(f"ğŸ›‘ [{section_name}] è¯¦æƒ…é‡‡é›†æœŸé—´æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œæ”¾å¼ƒæœ¬æ‰¹æ¬¡")
                                        stop_event.set()
                                        task_abandoned = True
                                        m_results = [None] * len(m_urls)
                                        break
                                    
                                    try:
                                        m_results = future.result(timeout=0.5)
                                        batch_elapsed = time.time() - batch_start_time
                                        logger.info(f"âœ… [{section_name}] æ‰¹é‡é‡‡é›†å®Œæˆï¼Œè€—æ—¶ {batch_elapsed:.1f}ç§’")
                                        break
                                    except concurrent.futures.TimeoutError:
                                        elapsed = time.time() - poll_start
                                        if int(elapsed) % 10 == 0 and int(elapsed) > 0:
                                            logger.info(f"â³ [{section_name}] è¯¦æƒ…é‡‡é›†è¿›è¡Œä¸­... å·²ç­‰å¾… {int(elapsed)}ç§’")
                                        if elapsed > detail_timeout:
                                            logger.error(f"ğŸ”´ [{section_name}] è¯¦æƒ…é‡‡é›†è¶…æ—¶ (>{detail_timeout}s)ï¼Œæ”¾å¼ƒæœ¬æ‰¹æ¬¡")
                                            task_abandoned = True
                                            m_results = [None] * len(m_urls)
                                            break
                                        continue
                                    except Exception as e:
                                        logger.error(f"âŒ [{section_name}] è¯¦æƒ…é‡‡é›†å¼‚å¸¸: {e}")
                                        m_results = [None] * len(m_urls)
                                        break
                                
                                if task_abandoned:
                                    logger.debug(f"â³ ç­‰å¾…åå°çº¿ç¨‹å“åº”åœæ­¢ä¿¡å·...")
                                    try:
                                        future.result(timeout=2.0)
                                    except:
                                        logger.debug(f"âš ï¸ åå°çº¿ç¨‹æœªåœ¨2ç§’å†…é€€å‡ºï¼Œç»§ç»­ä¸»æµç¨‹")
                        else:
                            # v1.5.3: é»˜è®¤ä½¿ç”¨çº¿ç¨‹æ± åŒæ­¥æ¨¡å¼ï¼ˆç¨³å®šå¯é ï¼‰
                            logger.info(f"ğŸ”§ [{section_name}] ä½¿ç”¨çº¿ç¨‹æ± æ¨¡å¼é‡‡é›† {len(m_urls)} ä¸ªè¯¦æƒ…é¡µï¼ˆç¨³å®šæ¨¡å¼ï¼‰")
                            m_results = sht.crawler_details_batch(m_urls, use_batch_mode=True)
                        
                        # --- æ­¥éª¤ 4: ä¿å­˜ç»“æœ ---
                        for idx, data in enumerate(m_results):
                            if idx % 5 == 0 and check_stop_and_pause(): break
                            
                            tid, u_d = batch_tasks[idx]
                            if not data or not data.get('magnet'):
                                reason = "è§£æå¤±è´¥" if not data else "æ— ç£åŠ›é“¾æ¥"
                                if FailedTID.add(tid=tid, section=section_name, url=u_d, reason=reason):
                                    total_failed += 1
                                    per_section[section_name]['failed'] += 1
                                    logger.debug(f"âš ï¸ {tid} è¿›å…¥é‡è¯•åˆ—è¡¨")
                                continue

                            # æ—¥æœŸè¿‡æ»¤
                            pub = (data.get('publish_date') or '').strip()
                            if date_mode == 'day' and date_value and pub != date_value: continue
                            if date_mode == 'month' and date_value and not pub.startswith(date_value): continue

                            with get_flask_app_context().app_context():
                                if sht.save_to_db(data, section_name, tid, u_d):
                                    total_saved += 1
                                    per_section[section_name]['saved'] += 1
                                    try: FailedTID.mark_success(tid)
                                    except: pass
                                    logger.info(f"âœ… [{section_name}] æ–°å¢: {data.get('title', '')[:40]}...")
                                else:
                                    total_skipped += 1
                                    per_section[section_name]['skipped'] += 1
                    except Exception as e:
                        logger.error(f"âŒ è¯¦æƒ…æ‰¹é‡é‡‡é›†é€»è¾‘å¼‚å¸¸: {e}")

                # --- æ‰¹æ¬¡ç»“ç®— ---
                processed_in_batch = len(batch_indices)
                i += processed_in_batch
                
                update_crawl_state({
                    'total_saved': total_saved,
                    'total_skipped': total_skipped,
                    'total_failed': total_failed,
                    'processed_pages': resume_offset + i
                })

                if reached_boundary:
                    logger.info(f"ğŸ [{section_name}] å¢é‡åŒæ­¥å®Œæˆ")
                    break
                
                if page_batch_size == 1 and i < len(pages_to_process):
                    delay = random.uniform(2, 5)
                    if sleep_interruptible(delay): break
                
                if total_failed >= config_manager.get('GLOBAL_ERROR_THRESHOLD', 300):
                    logger.error("ğŸ›‘ å…¨å±€é”™è¯¯è¿‡å¤šï¼Œç»ˆæ­¢æ¿å—ä»»åŠ¡")
                    stop_event.set()
                    break
            sections_done = crawl_progress.get('sections_done', 0) + 1
            update_crawl_state({'sections_done': sections_done})
            sync_crawl_state()
            logger.info(f"âœ… åˆ†ç±» [{section_name}] çˆ¬å–å®Œæˆ - æ–°å¢: {per_section[section_name]['saved']}, è·³è¿‡: {per_section[section_name]['skipped']}")
        
        logger.info(f"ğŸ‰ ç­›é€‰çˆ¬å–å®Œæˆ - æ€»è®¡æ–°å¢: {total_saved}, æ€»è®¡è·³è¿‡: {total_skipped}")

        # --- ğŸš€ [ä¿®å¤] æœ€ç»ˆé‡è¯•é˜¶æ®µ ---
        failed_pages_list = crawl_progress.get('failed_pages', [])
        retry_stats = {'attempted': 0, 'successful': 0, 'failed': 0, 'saved': 0, 'skipped': 0}
        
        if failed_pages_list and not (stop_event.is_set() or check_stop_and_pause()):
            logger.info(f"ğŸ”„ æ­£åœ¨å¯¹ {len(failed_pages_list)} ä¸ªå¤±è´¥ä»»åŠ¡è¿›è¡Œæœ€ç»ˆé‡è¯•...")
            update_crawl_state({'message': f'æ­£åœ¨é‡è¯• {len(failed_pages_list)} ä¸ªé¡µé¢...'})
            
            for fail_item in list(failed_pages_list):
                if stop_event.is_set() or check_stop_and_pause():
                    break
                
                f_sect = fail_item['section_name']
                f_page = fail_item['page']
                f_url = fail_item['url']
                
                logger.info(f"ğŸ”„ é‡è¯• [{f_sect}] ç¬¬{f_page}é¡µ: {f_url}")
                retry_stats['attempted'] += 1
                
                # æ›´æ–°UIçŠ¶æ€æ˜¾ç¤ºå½“å‰æ­£åœ¨é‡è¯•
                update_crawl_state({
                    'message': f'æ­£åœ¨é‡è¯• [{f_sect}] ç¬¬{f_page}é¡µ...',
                    'current_section': f_sect,
                    'current_page_actual': f_page,
                    'total_saved': total_saved,
                    'total_skipped': total_skipped
                })
                
                # v1.4.9: ä½¿ç”¨æ ‡å¿—ä½æ§åˆ¶å¤–å±‚å¾ªç¯é€€å‡º
                should_stop_retry = False
                
                try:
                    # v1.4.3b: æ‰«å°¾é‡è¯•ä¹ŸåŠ å…¥è§‚å¯Ÿå»¶è¿Ÿå’Œä¸­æ–­æ£€æŸ¥
                    if sleep_interruptible(random.uniform(2, 4)): 
                        should_stop_retry = True
                    
                    if not should_stop_retry:
                        # v1.4.8: [å…³é”®] ç½‘ç»œæ“ä½œå‰å†æ¬¡æ£€æŸ¥åœæ­¢ä¿¡å·
                        if stop_event.is_set() or check_stop_and_pause():
                            logger.info(f"ğŸ›‘ æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢é‡è¯•å¾ªç¯")
                            should_stop_retry = True
                    
                    if not should_stop_retry:
                        # åŒæ­¥è·å– TID åˆ—è¡¨
                        tid_list = sht.crawler_tid_list(f_url)
                        
                        # v1.4.8: ç½‘ç»œæ“ä½œåç«‹å³æ£€æŸ¥
                        if stop_event.is_set() or check_stop_and_pause():
                            logger.info(f"ğŸ›‘ TIDè·å–å®Œæˆåæ£€æµ‹åˆ°åœæ­¢ä¿¡å·")
                            should_stop_retry = True
                    
                    if not should_stop_retry and tid_list:
                        retry_stats['successful'] += 1
                        
                        # æ›´æ–°é¡µé¢ç»Ÿè®¡ï¼šä»åŸæœ¬çš„å¤±è´¥åˆ—è¡¨ç§»é™¤ï¼Œåç»­è®°å½•æˆåŠŸ
                        for idx_f, f_p in enumerate(page_stats['failed_pages']):
                            if f_p['section'] == f_sect and f_p['page'] == f_page:
                                page_stats['failed_pages'].pop(idx_f)
                                page_stats['total_pages_failed'] -= 1
                                break
                        
                        with get_flask_app_context().app_context():
                            existing_tids = db.session.query(Resource.tid).filter(Resource.tid.in_(tid_list)).all()
                            ex_set = {t[0] for t in existing_tids}
                        
                        to_crawl = []
                        f_cnt_retry = 0
                        for tid in tid_list:
                            if tid not in ex_set:
                                to_crawl.append((tid, f"https://sehuatang.org/forum.php?mod=viewthread&tid={tid}"))
                            else:
                                total_skipped += 1
                                per_section[f_sect]['skipped'] += 1
                                retry_stats['skipped'] += 1
                                f_cnt_retry += 1
                        
                        if f_cnt_retry > 0:
                            logger.info(f"ğŸ” [{f_sect}] ç¬¬{f_page}é¡µ (é‡è¯•) è¿‡æ»¤æ‰ {f_cnt_retry} ä¸ªæ•°æ®åº“å·²æœ‰èµ„æº")
                        
                        p_saved = 0
                        if to_crawl and not should_stop_retry:
                            # v1.4.8: æ‰¹é‡é‡‡é›†å‰çš„æœ€åæ£€æŸ¥
                            if stop_event.is_set() or check_stop_and_pause():
                                logger.info(f"ğŸ›‘ è¯¦æƒ…é‡‡é›†å‰æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œè·³è¿‡å‰©ä½™ {len(to_crawl)} ä¸ªèµ„æº")
                                should_stop_retry = True
                            
                            if not should_stop_retry:
                                b_urls = [u for t, u in to_crawl]
                                # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘é‡‡é›†è¯¦æƒ…
                                res = sht.crawler_details_batch(b_urls, use_batch_mode=True)
                                
                                # v1.4.8: æ‰¹é‡é‡‡é›†å®Œæˆåç«‹å³æ£€æŸ¥
                                if stop_event.is_set() or check_stop_and_pause():
                                    logger.info(f"ğŸ›‘ è¯¦æƒ…é‡‡é›†å®Œæˆåæ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œä¸ä¿å­˜ç»“æœ")
                                    should_stop_retry = True
                                
                                if not should_stop_retry:
                                    for idx, d in enumerate(res):
                                        # æ¯ 5 ä¸ªæ£€æŸ¥ä¸€æ¬¡
                                        if idx % 5 == 0:
                                            if stop_event.is_set() or check_stop_and_pause():
                                                logger.info(f"ğŸ›‘ ä¿å­˜è¿‡ç¨‹ä¸­æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œå·²ä¿å­˜ {idx}/{len(res)} ä¸ª")
                                                should_stop_retry = True
                                                break
                                        
                                        if should_stop_retry:
                                            break
                                            
                                        tid_r, url_r = to_crawl[idx]
                                        
                                        # v1.4.4: å¯¹é½ä¸»å¾ªç¯é€»è¾‘
                                        if not d or not d.get('magnet'):
                                            reason = "é‡è¯•è§£æå¤±è´¥" if not d else "é‡è¯•æ— ç£åŠ›é“¾æ¥"
                                            if FailedTID.add(tid=tid_r, section=f_sect, url=url_r, reason=reason):
                                                total_failed += 1
                                                per_section[f_sect]['failed'] += 1
                                            continue

                                        # æ—¥æœŸè¿‡æ»¤ (é‡è¦ï¼šé˜²æ­¢é‡è¯•æ•‘å›äº†ä¸ç¬¦åˆæ—¥æœŸè¦æ±‚çš„èµ„æº)
                                        pub = (d.get('publish_date') or '').strip()
                                        if date_mode == 'day' and date_value and pub != date_value: continue
                                        if date_mode == 'month' and date_value and not pub.startswith(date_value): continue

                                        with get_flask_app_context().app_context():
                                            if sht.save_to_db(d, f_sect, tid_r, url_r):
                                                total_saved += 1
                                                per_section[f_sect]['saved'] += 1
                                                retry_stats['saved'] += 1
                                                p_saved += 1
                                                logger.info(f"âœ… [{f_sect}] æ–°å¢ (é‡è¯•): {d.get('title', '')[:40]}...")
                                                try: FailedTID.mark_success(tid_r)
                                                except: pass
                                            else:
                                                total_skipped += 1
                                                per_section[f_sect]['skipped'] += 1
                                                retry_stats['skipped'] += 1
                        
                        if not should_stop_retry:
                            page_stats['successful_pages'].append({
                                'section': f_sect, 'page': f_page, 'saved': p_saved, 
                                'skipped': len(tid_list) - p_saved, 'is_retry': True
                            })
                            page_stats['total_pages_successful'] += 1
                            if fail_item in crawl_progress['failed_pages']:
                                crawl_progress['failed_pages'].remove(fail_item)
                                
                except Exception as e:
                    logger.warning(f"âŒ é‡è¯•ä»å¤±è´¥: {e}")
                    retry_stats['failed'] += 1
                
                # v1.4.9: æ£€æŸ¥æ˜¯å¦éœ€è¦ç»ˆæ­¢æ•´ä¸ªé‡è¯•å¾ªç¯
                if should_stop_retry:
                    logger.info(f"ğŸ›‘ é‡è¯•å¾ªç¯å› åœæ­¢ä¿¡å·è€Œç»ˆæ­¢")
                    break
            
            update_crawl_state({'total_saved': total_saved})
            sync_crawl_state()
    
        # åˆ¤æ–­ä»»åŠ¡å®ŒæˆçŠ¶æ€
        completion_status = "çˆ¬å–å®Œæˆ"  # é»˜è®¤
        exception_reason = None
    
        # æ£€æŸ¥æ˜¯å¦è¢«æ‰‹åŠ¨åœæ­¢
        from .utils import check_stop_and_pause
        if stop_event.is_set() or check_stop_and_pause():
            completion_status = "æ‰‹åŠ¨ç»ˆæ­¢"
        elif total_failed > 0 and (total_saved + total_skipped) == 0:
            completion_status = "å¼‚å¸¸ç»ˆæ­¢"
            exception_reason = "çˆ¬å–åˆ°çš„èµ„æºå…¨éƒ¨å¤±è´¥ï¼Œæ— æœ‰æ•ˆæ•°æ®"
    
        # æ¸…é™¤æ‰€æœ‰ç›¸å…³ç¼“å­˜ï¼Œç¡®ä¿æ•°æ®ç«‹å³å¯è§
        try:
            from cache_manager import cache_manager, CacheKeys
            cache_manager.delete(CacheKeys.STATS)  # æ¸…é™¤ç»Ÿè®¡ç¼“å­˜
            cache_manager.delete(CacheKeys.CATEGORIES)  # æ¸…é™¤åˆ†ç±»ç¼“å­˜
            logger.info("âœ… å·²æ¸…é™¤ç»Ÿè®¡å’Œåˆ†ç±»ç¼“å­˜ï¼Œæ–°æ•°æ®å°†ç«‹å³å¯è§")
        except Exception as e:
            logger.warning(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")
    
        # æ„å»ºè¯¦ç»†çš„çˆ¬å–æ‘˜è¦
        # ç§»é™¤å†—ä½™å±€éƒ¨å¯¼å…¥ï¼Œæ”¹ç”¨å…¨å±€å¯¼å…¥
        
        # è®¡ç®—çˆ¬å–æ—¶é•¿
        end_time = time.time()
        start_time = crawl_progress.get('start_time', end_time)  # å¦‚æœæ²¡æœ‰å¼€å§‹æ—¶é—´ï¼Œä½¿ç”¨ç»“æŸæ—¶é—´
        duration_seconds = int(end_time - start_time)
        duration_minutes = duration_seconds // 60
        duration_seconds_remainder = duration_seconds % 60
        
        # æ„å»ºçˆ¬å–æ¡ä»¶æè¿°
        conditions = []
        if section_fids:
            if len(section_fids) == len(SECTION_MAP):
                conditions.append("æ‰€æœ‰æ¿å—")
            else:
                section_names = []
                for fid in section_fids:
                    if fid in SECTION_MAP:
                        section_names.append(SECTION_MAP[fid])
                    else:
                        # å¯èƒ½ä¼ å…¥çš„æ˜¯åç§°
                        section_names.append(str(fid))
                conditions.append(f"æ¿å—: {', '.join(section_names)}")
        else:
            conditions.append("æ‰€æœ‰æ¿å—")
        
        conditions.append(f"æœ€å¤§é¡µæ•°: {max_pages}")
        
        if date_mode and date_mode != 'all' and date_value:
            if date_mode == 'day':
                conditions.append(f"æ—¥æœŸè¿‡æ»¤: {date_value}")
            elif date_mode == 'month':
                conditions.append(f"æœˆä»½è¿‡æ»¤: {date_value}")
            else:
                conditions.append(f"æ—¥æœŸè¿‡æ»¤: {date_mode}={date_value}")
        
        if dateline:
            seconds = int(dateline)
            if seconds == 86400:
                time_desc = "è¿‘1å¤©"
            elif seconds == 604800:
                time_desc = "è¿‘1å‘¨"
            elif seconds == 2592000:
                time_desc = "è¿‘1æœˆ"
            elif seconds == 31536000:
                time_desc = "è¿‘1å¹´"
            else:
                days = seconds // 86400
                time_desc = f"è¿‘{days}å¤©"
            conditions.append(f"æ—¶é—´èŒƒå›´: {time_desc}")
        
        # ç»Ÿè®¡å®é™…çˆ¬å–çš„é¡µæ•°
        actual_pages_crawled = crawl_progress.get('processed_pages', 0)
        
        # ç”Ÿæˆé¡µé¢çº§åˆ«çš„è¯¦ç»†ç»Ÿè®¡
        page_summary = {
            'total_attempted': page_stats['total_pages_attempted'],
            'total_successful': page_stats['total_pages_successful'],
            'total_failed': page_stats['total_pages_failed'],
            'success_rate': round((page_stats['total_pages_successful'] / max(1, page_stats['total_pages_attempted'])) * 100, 1),
            'successful_pages_detail': page_stats['successful_pages'],
            'failed_pages_detail': page_stats['failed_pages'],
            'retry_summary': retry_stats if retry_stats['attempted'] > 0 else None
        }
        
        # æŒ‰æ¿å—åˆ†ç»„çš„é¡µé¢ç»Ÿè®¡
        section_page_stats = {}
        for page_info in page_stats['successful_pages']:
            section = page_info['section']
            if section not in section_page_stats:
                section_page_stats[section] = {
                    'successful_pages': [],
                    'total_pages': 0,
                    'total_saved': 0,
                    'total_skipped': 0
                }
            section_page_stats[section]['successful_pages'].append(page_info['page'])
            section_page_stats[section]['total_pages'] += 1
            section_page_stats[section]['total_saved'] += page_info['saved']
            section_page_stats[section]['total_skipped'] += page_info['skipped']
        
        for page_info in page_stats['failed_pages']:
            section = page_info['section']
            if section not in section_page_stats:
                section_page_stats[section] = {
                    'successful_pages': [],
                    'failed_pages': [],
                    'total_pages': 0,
                    'total_saved': 0,
                    'total_skipped': 0
                }
            if 'failed_pages' not in section_page_stats[section]:
                section_page_stats[section]['failed_pages'] = []
            section_page_stats[section]['failed_pages'].append(page_info['page'])
            section_page_stats[section]['total_pages'] += 1
        
        summary = {
            'timestamp': datetime.now().isoformat(),
            'unix_time': int(time.time()),
            'task_type_text': "æ‰‹åŠ¨çˆ¬å–ä»»åŠ¡" if task_type == 'manual' else "è‡ªåŠ¨å®šæ—¶çˆ¬å–ä»»åŠ¡",
            'completion_status': completion_status, 
            'exception_reason': exception_reason,
            'engine_set': {
                'mode': config_manager.get('CRAWLER_MODE', 'async'),
                'concurrency': config_manager.get('CRAWLER_MAX_CONCURRENCY', 20) if config_manager.get('CRAWLER_MODE') == 'async' else config_manager.get('CRAWLER_THREAD_COUNT', 10),
                'delay_min': config_manager.get('CRAWLER_ASYNC_DELAY_MIN', 0.5) if config_manager.get('CRAWLER_MODE') == 'async' else config_manager.get('CRAWLER_SYNC_DELAY_MIN', 0.3),
                'delay_max': config_manager.get('CRAWLER_ASYNC_DELAY_MAX', 1.5) if config_manager.get('CRAWLER_MODE') == 'async' else config_manager.get('CRAWLER_SYNC_DELAY_MAX', 0.8),
                'proxy_active': bool(getattr(sht, 'proxies', {}).get('http'))
            },
            'duration': {
                'total_seconds': duration_seconds + duration_minutes * 60,
                'minutes': duration_minutes,
                'seconds': duration_seconds_remainder,
                'formatted': f"{duration_minutes}åˆ†{duration_seconds_remainder}ç§’" if duration_minutes > 0 else f"{duration_seconds_remainder}ç§’"
            },
            'results': {
                'total_saved': total_saved,
                'total_skipped': total_skipped,
                'total_failed': total_failed,
                'total_processed': total_saved + total_skipped + total_failed,
                'success_rate': round((total_saved / max(1, total_saved + total_skipped + total_failed)) * 100, 1)
            },
            'crawl_conditions': {
                'description': ' | '.join(conditions),
                'target_sections': section_fids or list(SECTION_MAP.keys()),
                'max_pages_per_section': max_pages,
                'actual_pages_crawled': actual_pages_crawled,
                'date_filter': {
                    'mode': date_mode,
                    'value': date_value,
                    'dateline': dateline
                }
            },
            'page_statistics': page_summary,
            'section_page_breakdown': section_page_stats,
            'per_section_results': per_section,
            'performance': {
                'avg_time_per_item': round((duration_seconds + duration_minutes * 60) / max(1, total_saved + total_skipped), 2),
                'items_per_minute': round((total_saved + total_skipped) / max(1, (duration_seconds + duration_minutes * 60) / 60), 1)
            },
            'raw_options': {
                'fids': section_fids,
                'date_mode': date_mode,
                'date_value': date_value,
                'dateline': dateline,
                'max_pages': max_pages
            }
        }
    
        try:
            from configuration import Config
            summary_json_path = Config.get_path('summary_json')
            log_dir = Config.get_path('log_dir')
            os.makedirs(log_dir, exist_ok=True)
            with open(summary_json_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“Š è¯¦ç»†çˆ¬å–æ‘˜è¦å·²ä¿å­˜åˆ° {summary_json_path}")
            logger.info(f"ğŸ“Š çˆ¬å–è€—æ—¶: {summary['duration']['formatted']}, å¹³å‡æ¯é¡¹: {summary['performance']['avg_time_per_item']}ç§’")
        except Exception as e:
            logger.error(f"å†™å…¥çˆ¬å–æ‘˜è¦å¤±è´¥: {e}")

        # å‘é€æœºå™¨äººé€šçŸ¥
        try:
            success = _send_crawl_report(summary)
            notification_sent = success  # è®°å½•å‘é€çŠ¶æ€
        except Exception as e:
            logger.error(f"âŒ Telegramé€šçŸ¥æ¨é€å¤±è´¥: {e}")

        # é€šçŸ¥çŠ¶æ€æœºå›åˆ°idleçŠ¶æ€
        try:
            from crawler_control.cc_control_bridge import get_crawler_control_bridge
            bridge = get_crawler_control_bridge()
            bridge.coordinator.transition_state('idle', {'stopped_at': time.time()})
            logger.info("âœ… å·²é€šçŸ¥çŠ¶æ€æœºï¼šçˆ¬è™«å›åˆ°idleçŠ¶æ€")
        except Exception as e:
            logger.warning(f"âš ï¸ é€šçŸ¥çŠ¶æ€æœºå¤±è´¥: {e}")
        
        # ä½¿ç”¨ç»Ÿä¸€çŠ¶æ€æ›´æ–°ï¼Œå¹¶æ¸…ç†ä»»åŠ¡ç‰¹å®šå­—æ®µ
        final_message = f'çˆ¬å–å®Œæˆ - æ–°å¢: {total_saved}, è·³è¿‡: {total_skipped}'
        if completion_status == "æ‰‹åŠ¨ç»ˆæ­¢":
            final_message = f'ä»»åŠ¡å·²æ‰‹åŠ¨ç»ˆæ­¢ - æ–°å¢: {total_saved}'
            
        update_crawl_state({
            'is_crawling': False,
            'is_paused': False,
            'message': final_message,
            # å½»åº•æ¸…ç†æ‰€æœ‰è¿›åº¦æŒ‡æ ‡
            'current_section': '',
            'current_page': 0,
            'progress_percent': 100,
            'sections_total': 0,
            'sections_done': 0,
            'current_section_pages': 0,
            'current_section_processed': 0,
            'processed_pages': 0,
            'estimated_total_pages': 0,
            # åŒºåˆ†é¡µç å­—æ®µä¸€å¹¶å½’é›¶
            'current_page_actual': 0,
            'max_pages_actual': 0,
            'current_page_task': 0,
            'max_pages_task': 0
        })
        sync_crawl_state()
    
        # Task completed successfully (continue to summary and final status)

    except Exception as e:
        import traceback
        logger.error(f"âŒ çˆ¬è™«ä»»åŠ¡å¼‚å¸¸: {traceback.format_exc()}")

        # å³ä½¿å¼‚å¸¸ä¹Ÿè¦å‘é€é€šçŸ¥
        try:
            error_msg = f"""âŒ *çˆ¬è™«ä»»åŠ¡å¼‚å¸¸ç»ˆæ­¢*
â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ é”™è¯¯ä¿¡æ¯ï¼š{str(e)[:200]}
â° ç»ˆæ­¢æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯"""
            _send_telegram_message(error_msg, parse_mode='Markdown')
        except Exception as notify_err:
            logger.error(f"å‘é€å¼‚å¸¸é€šçŸ¥å¤±è´¥: {notify_err}")
        
        # æ¸…ç†çŠ¶æ€
        try:
            from crawler_control.cc_control_bridge import get_crawler_control_bridge
            bridge = get_crawler_control_bridge()
            bridge.reset_to_idle()
        except:
            pass
        
        raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸

    finally:
        # finally ç¡®ä¿å®Œæˆé€šçŸ¥ä¸€å®šä¼šå‘é€
        logger.info("ğŸ‰ çˆ¬è™«ä»»åŠ¡ç»“æŸ (finally å—æ‰§è¡Œ)")

        # æ£€æŸ¥æ˜¯å¦å·²ç»å‘é€è¿‡é€šçŸ¥
        if not notification_sent:
            logger.warning("âš ï¸ æ£€æµ‹åˆ°é€šçŸ¥æœªå‘é€ï¼Œå°è¯•åœ¨ finally å—ä¸­å‘é€")

            # æ— è®ºæ­£å¸¸è¿˜æ˜¯å¼‚å¸¸é€€å‡ºï¼Œéƒ½å‘é€å®Œæˆé€šçŸ¥
            try:
                from crawler_control.cc_control_bridge import get_crawler_control_bridge

                # è¯»å–ä¿å­˜çš„æ‘˜è¦æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                from configuration import Config
                summary_json_path = Config.get_path('summary_json')

                summary_data = {}
                if os.path.exists(summary_json_path):
                    with open(summary_json_path, 'r', encoding='utf-8') as f:
                        summary_data = json.load(f)

                # å¦‚æœæœ‰æ‘˜è¦æ•°æ®ï¼Œå‘é€æŠ¥å‘Š
                if summary_data and summary_data.get('results'):
                    try:
                        success = _send_crawl_report(summary_data)
                        logger.info(f"âœ… finally å—ä¸­å®Œæˆé€šçŸ¥å‘é€: {success}")
                    except Exception as e:
                        logger.error(f"finally å—å‘é€æŠ¥å‘Šå¤±è´¥: {e}")

                # å¼ºåˆ¶æ›´æ–°çŠ¶æ€ä¸ºç©ºé—²
                try:
                    bridge = get_crawler_control_bridge()
                    bridge.reset_to_idle()
                    logger.info("âœ… å·²å¼ºåˆ¶é‡ç½®çŠ¶æ€åˆ°ç©ºé—²")
                except Exception as reset_err:
                    logger.warning(f"finally å—é‡ç½®çŠ¶æ€å¤±è´¥: {reset_err}")

            except Exception as notify_err:
                logger.error(f"finally å—å‘é€å®Œæˆé€šçŸ¥å¤±è´¥: {notify_err}")