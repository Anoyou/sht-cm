#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€ç»´æŠ¤å·¥å…· - æ•´åˆæ•°æ®åº“ç»´æŠ¤ã€å¤±è´¥TIDæ¸…ç†å’Œé‡è¯•åŠŸèƒ½

åˆå¹¶è‡ªï¼š
- database_maintenance.py (270è¡Œ)
- cleanup_failed_tids.py (273è¡Œ)
- retry_failed_tids.py (328è¡Œ)

ä½¿ç”¨æ–¹æ³•ï¼š
    python maintenance_tools.py db-info              # æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯
    python maintenance_tools.py db-cleanup           # æ¸…ç†é‡å¤æ•°æ®
    python maintenance_tools.py db-optimize          # ä¼˜åŒ–æ•°æ®åº“
    python maintenance_tools.py failed-analyze       # åˆ†æå¤±è´¥TID
    python maintenance_tools.py failed-cleanup       # æ¸…ç†å¤±è´¥TID
    python maintenance_tools.py failed-retry         # é‡è¯•å¤±è´¥TID
    python maintenance_tools.py full-maintenance     # å®Œæ•´ç»´æŠ¤æµç¨‹
"""

import os
import sys
import time
import logging
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional
from sqlalchemy import text, func

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from utils import get_flask_app, db_session_context, get_database_paths, setup_logging
from models import db, Resource, Category, FailedTID
# å»¶è¿Ÿå¯¼å…¥ crawler ä»¥é¿å…å¾ªç¯ä¾èµ–
# from crawler import SHT
from cache_manager import cache_manager

# è®¾ç½®æ—¥å¿—
setup_logging()
logger = logging.getLogger(__name__)


# ==================== æ•°æ®åº“ç»´æŠ¤ç±» ====================

class DatabaseMaintenance:
    """æ•°æ®åº“ç»´æŠ¤å·¥å…·ç±»"""
    
    def __init__(self):
        self.app = get_flask_app()
        self.stats = {
            'cleaned_duplicates': 0,
            'normalized_dates': 0,
            'optimized_indexes': 0,
            'cleaned_orphans': 0
        }
    
    def run_full_maintenance(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®åº“ç»´æŠ¤"""
        logger.info("å¼€å§‹æ•°æ®åº“ç»´æŠ¤...")

        with self.app.app_context():
            try:
                # 1. æ¸…ç†é‡å¤æ•°æ®
                self.clean_duplicates()

                # 2. æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
                self.normalize_dates()

                # 3. æ¸…ç†å­¤ç«‹æ•°æ®
                self.clean_orphaned_data()

                # 4. æ¸…ç†WAL/SHMæ–‡ä»¶
                self.cleanup_wal_shm()

                # 5. ä¼˜åŒ–æ•°æ®åº“
                self.optimize_database()

                # 6. æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self.update_statistics()

                # 7. æ¸…ç†ç¼“å­˜
                self.clear_cache()

                # 8. æ¸…ç†æ—§æ•°æ®ï¼ˆä¿ç•™æŒ‡å®šå¤©æ•°å†…çš„è®°å½•ï¼‰
                days = 30  # é»˜è®¤ä¿ç•™30å¤©
                self.cleanup_old_records(days)

                logger.info("æ•°æ®åº“ç»´æŠ¤å®Œæˆ")
                self.print_maintenance_report()

            except Exception as e:
                logger.error(f"æ•°æ®åº“ç»´æŠ¤å¤±è´¥: {e}")
                db.session.rollback()
                raise
    
    def clean_duplicates(self):
        """æ¸…ç†é‡å¤æ•°æ®"""
        logger.info("æ¸…ç†é‡å¤æ•°æ®...")
        
        # ä½¿ç”¨Resourceæ¨¡å‹çš„æ¸…ç†æ–¹æ³•
        removed_count = Resource.cleanup_duplicates()
        self.stats['cleaned_duplicates'] = removed_count
        
        logger.info(f"æ¸…ç†äº† {removed_count} æ¡é‡å¤è®°å½•")
    
    def normalize_dates(self):
        """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼"""
        logger.info("æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼...")
        
        # æŸ¥æ‰¾éœ€è¦æ ‡å‡†åŒ–çš„æ—¥æœŸ
        resources_with_invalid_dates = Resource.query.filter(
            Resource.publish_date.isnot(None),
            Resource.publish_date != '',
            ~Resource.publish_date.like('____-__-__')
        ).all()
        
        normalized_count = 0
        for resource in resources_with_invalid_dates:
            old_date = resource.publish_date
            new_date = self._normalize_date_string(old_date)
            
            if new_date != old_date:
                resource.publish_date = new_date
                normalized_count += 1
                logger.debug(f"æ—¥æœŸæ ‡å‡†åŒ–: '{old_date}' -> '{new_date}'")
        
        if normalized_count > 0:
            db.session.commit()
        
        self.stats['normalized_dates'] = normalized_count
        logger.info(f"æ ‡å‡†åŒ–äº† {normalized_count} æ¡æ—¥æœŸè®°å½•")
    
    def clean_orphaned_data(self):
        """æ¸…ç†å­¤ç«‹æ•°æ®"""
        logger.info("æ¸…ç†å­¤ç«‹æ•°æ®...")
        
        # æ¸…ç†ç©ºæ ‡é¢˜æˆ–ç©ºç£åŠ›é“¾æ¥çš„è®°å½•
        orphaned_resources = Resource.query.filter(
            (Resource.title.is_(None)) | 
            (Resource.title == '') |
            (Resource.magnet.is_(None)) |
            (Resource.magnet == '')
        ).all()
        
        cleaned_count = 0
        for resource in orphaned_resources:
            db.session.delete(resource)
            cleaned_count += 1
            logger.debug(f"åˆ é™¤å­¤ç«‹è®°å½•: ID={resource.id}")
        
        if cleaned_count > 0:
            db.session.commit()
        
        self.stats['cleaned_orphans'] = cleaned_count
        logger.info(f"æ¸…ç†äº† {cleaned_count} æ¡å­¤ç«‹è®°å½•")
    
    def optimize_database(self):
        """ä¼˜åŒ–æ•°æ®åº“ - ç‹¬å æ¨¡å¼ä¿®å¤ç‰ˆ"""
        logger.info("ä¼˜åŒ–æ•°æ®åº“ (å¼ºåˆ¶ç‹¬å æ¨¡å¼)...")

        try:
            # 1. é‡è¦ï¼šå¼ºåˆ¶åˆ‡æ–­æ‰€æœ‰å½“å‰æ´»è·ƒçš„ Session æ•°æ®åº“å¼•ç”¨
            # è¿™èƒ½é‡Šæ”¾å¯èƒ½å¯¼è‡´é”å®šçš„æŒ‚èµ·è¿æ¥
            db.session.remove()
            db.session.close_all()
            
            # 2. ç›´æ¥åœ¨åŸå§‹è¿æ¥ä¸Šæ‰§è¡Œ (è§„é¿æ‰€æœ‰ SQLAlchemy äº‹åŠ¡å¹²æ‰°)
            # ä½¿ç”¨ raw_connection ç»•è¿‡æ‰€æœ‰ ORM å±‚
            import sqlite3
            raw_conn = db.engine.raw_connection()
            try:
                # è®¾ç½®è¶…æ—¶æ—¶é—´æ›´é•¿ä¸€äº›
                raw_conn.isolation_level = None  # æ¿€æ´» Autocommit
                cursor = raw_conn.cursor()
                
                logger.debug("æ­£åœ¨æ‰§è¡Œ VACUUM (æ­¤æ“ä½œå¯èƒ½è€—æ—¶å‡ ç§’)...")
                cursor.execute('VACUUM')
                cursor.execute('ANALYZE')
                cursor.execute('PRAGMA optimize')
                cursor.close()
                logger.info("âœ… VACUUM å’Œç‰©ç†ä¼˜åŒ–å®Œæˆ")
                self.stats['optimized_indexes'] = 1
            finally:
                raw_conn.close()

            logger.info("æ•°æ®åº“ä¼˜åŒ–æˆåŠŸ")

        except Exception as e:
            # ç‰¹æ®Šå¤„ç† busy é”™è¯¯
            if "locked" in str(e).lower():
                logger.warning("ğŸ•’ æ•°æ®åº“ç›®å‰è¾ƒå¿™ï¼Œæ­£åœ¨é”å®šä¸­ã€‚è·³è¿‡ VACUUM ä»¥ä¿æŒç³»ç»Ÿå¯ç”¨æ€§ã€‚")
            else:
                logger.error(f"æ•°æ®åº“ä¼˜åŒ–å¤±è´¥: {e}")

        except Exception as e:
            logger.error(f"æ•°æ®åº“ä¼˜åŒ–å¤±è´¥: {e}")

    def cleanup_wal_shm(self):
        """æ¸…ç†SQLiteçš„WALå’ŒSHMæ–‡ä»¶"""
        logger.info("æ¸…ç†WAL/SHMæ–‡ä»¶...")

        try:
            # è·å–æ•°æ®åº“è·¯å¾„
            db_path = self.app.config['SQLALCHEMY_DATABASE_URI'].replace('sqlite:///', '')

            # WALå’ŒSHMæ–‡ä»¶è·¯å¾„
            wal_path = f"{db_path}-wal"
            shm_path = f"{db_path}-shm"

            total_freed = 0

            # æ¸…ç†WALæ–‡ä»¶
            if os.path.exists(wal_path):
                wal_size = os.path.getsize(wal_path)
                try:
                    os.remove(wal_path)
                    total_freed += wal_size
                    logger.info(f"å·²åˆ é™¤WALæ–‡ä»¶: {wal_path} ({self._format_size(wal_size)})")
                except Exception as e:
                    logger.error(f"åˆ é™¤WALæ–‡ä»¶å¤±è´¥: {e}")

            # æ¸…ç†SHMæ–‡ä»¶
            if os.path.exists(shm_path):
                shm_size = os.path.getsize(shm_path)
                try:
                    os.remove(shm_path)
                    total_freed += shm_size
                    logger.info(f"å·²åˆ é™¤SHMæ–‡ä»¶: {shm_path} ({self._format_size(shm_size)})")
                except Exception as e:
                    logger.error(f"åˆ é™¤SHMæ–‡ä»¶å¤±è´¥: {e}")

            if total_freed > 0:
                logger.info(f"æ€»è®¡é‡Šæ”¾ç©ºé—´: {self._format_size(total_freed)}")
            else:
                logger.info("æ²¡æœ‰éœ€è¦æ¸…ç†çš„WAL/SHMæ–‡ä»¶")

            return total_freed

        except Exception as e:
            logger.error(f"æ¸…ç†WAL/SHMæ–‡ä»¶å¤±è´¥: {e}")
            return 0

    def _format_size(self, size_bytes: int) -> str:
        """
        æ ¼å¼åŒ–æ–‡ä»¶å¤§å°

        Args:
            size_bytes: å­—èŠ‚æ•°

        Returns:
            str: æ ¼å¼åŒ–åçš„å¤§å°å­—ç¬¦ä¸²
        """
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.2f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.2f} TB"
    
    def cleanup_old_records(self, days: int = 30):
        """æ¸…ç†æ—§æ•°æ®è®°å½•ï¼Œä¿ç•™æŒ‡å®šå¤©æ•°å†…çš„æ•°æ®"""
        logger.info(f"å¼€å§‹æ¸…ç†è¶…è¿‡ {days} å¤©çš„æ—§æ•°æ®...")
        
        try:
            cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)
            
            # æ¸…ç†æ—§çš„èµ„æºè®°å½•
            deleted_count = Resource.query.filter(
                Resource.created_at < cutoff_date
            ).delete(synchronize_session=False)
            logger.info(f"âœ… æ¸…ç†äº† {deleted_count} æ¡è¶…è¿‡ {days} å¤©çš„èµ„æºè®°å½•")
            
            # æ¸…ç†æ—§çš„å¤±è´¥TIDè®°å½•
            cutoff_date_for_failed = datetime.now(timezone.utc) - timedelta(days=days*2)
            
            deleted_failed_count = FailedTID.query.filter(
                FailedTID.created_at < cutoff_date_for_failed
            ).delete(synchronize_session=False)
            
            # æäº¤åˆ é™¤æ“ä½œ
            db.session.commit()
            
            logger.info(f"âœ… æ¸…ç†äº† {deleted_failed_count} æ¡è¶…è¿‡ {days*2} å¤©çš„å¤±è´¥TIDè®°å½•")
            
            return {
                'deleted_resources': deleted_count,
                'deleted_failed_tids': deleted_failed_count
            }
        
        except Exception as e:
            logger.error(f"æ¸…ç†æ—§æ•°æ®å¤±è´¥: {e}")
            return None
    
    def update_statistics(self):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("æ›´æ–°ç»Ÿè®¡ä¿¡æ¯...")
        
        # å¼ºåˆ¶é‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = Resource.get_statistics()
        logger.info(f"å½“å‰ç»Ÿè®¡: æ€»è®¡ {stats['total_count']} æ¡è®°å½•")
    
    def clear_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        logger.info("æ¸…ç†ç¼“å­˜...")
        cache_manager.clear()
        logger.info("ç¼“å­˜å·²æ¸…ç†")
    
    def _normalize_date_string(self, date_str):
        """æ ‡å‡†åŒ–æ—¥æœŸå­—ç¬¦ä¸²"""
        if not date_str:
            return None
        
        # å¦‚æœå·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼Œç›´æ¥è¿”å›
        if len(date_str) == 10 and date_str.count('-') == 2:
            try:
                datetime.strptime(date_str, '%Y-%m-%d')
                return date_str
            except ValueError:
                pass
        
        # å°è¯•è§£æå„ç§æ—¥æœŸæ ¼å¼
        date_formats = [
            '%Yå¹´%mæœˆ%dæ—¥',
            '%Y/%m/%d',
            '%Y.%m.%d',
            '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%d %H:%M',
        ]
        
        for fmt in date_formats:
            try:
                parsed_date = datetime.strptime(date_str, fmt)
                return parsed_date.strftime('%Y-%m-%d')
            except ValueError:
                continue
        
        # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›åŸå§‹å€¼çš„å‰10ä¸ªå­—ç¬¦
        return date_str[:10] if len(date_str) >= 10 else date_str
    
    def print_maintenance_report(self):
        """æ‰“å°ç»´æŠ¤æŠ¥å‘Š"""
        print("\n" + "="*50)
        print("æ•°æ®åº“ç»´æŠ¤æŠ¥å‘Š")
        print("="*50)
        print(f"æ¸…ç†é‡å¤è®°å½•: {self.stats['cleaned_duplicates']} æ¡")
        print(f"æ ‡å‡†åŒ–æ—¥æœŸ: {self.stats['normalized_dates']} æ¡")
        print(f"æ¸…ç†å­¤ç«‹æ•°æ®: {self.stats['cleaned_orphans']} æ¡")
        print(f"æ•°æ®åº“ä¼˜åŒ–: {'å®Œæˆ' if self.stats['optimized_indexes'] else 'è·³è¿‡'}")
        print("="*50)
    
    def get_database_info(self):
        """è·å–æ•°æ®åº“ä¿¡æ¯"""
        with self.app.app_context():
            try:
                # è·å–æ•°æ®åº“æ–‡ä»¶å¤§å°
                db_path = self.app.config['SQLALCHEMY_DATABASE_URI'].replace('sqlite:///', '')
                db_size = os.path.getsize(db_path) if os.path.exists(db_path) else 0
                
                # è·å–è¡¨ä¿¡æ¯
                total_resources = Resource.query.count()
                total_categories = Category.query.count()
                
                # è·å–æœ€æ–°å’Œæœ€æ—§çš„è®°å½•
                latest_resource = Resource.query.order_by(Resource.created_at.desc()).first()
                oldest_resource = Resource.query.order_by(Resource.created_at.asc()).first()
                
                return {
                    'database_size_mb': round(db_size / (1024 * 1024), 2),
                    'total_resources': total_resources,
                    'total_categories': total_categories,
                    'latest_record': latest_resource.created_at if latest_resource else None,
                    'oldest_record': oldest_resource.created_at if oldest_resource else None,
                    'cache_stats': cache_manager.get_stats()
                }
                
            except Exception as e:
                logger.error(f"è·å–æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {e}")
                return {}


# ==================== å¤±è´¥TIDæ¸…ç†ç±» ====================

class FailedTidCleaner:
    """å¤±è´¥TIDæ¸…ç†å·¥å…·ç±»"""
    
    def __init__(self):
        self.main_db_path, self.failed_db_path = get_database_paths()
    
    def analyze_failed_tids(self):
        """åˆ†æå¤±è´¥TIDçš„æƒ…å†µ"""
        logger.info("ğŸ” åˆ†æå¤±è´¥TIDæƒ…å†µ")
        
        try:
            with db_session_context():
                # æ€»æ•°ç»Ÿè®¡
                total_count = FailedTID.query.filter(FailedTID.status.in_(['pending', 'retrying'])).count()
                
                # æŒ‰æ¿å—åˆ†å¸ƒ
                section_stats = db.session.query(
                    FailedTID.section, func.count(FailedTID.id)
                ).filter(FailedTID.status.in_(['pending', 'retrying'])).group_by(FailedTID.section).all()
                section_stats = dict(section_stats)
                
                # æŒ‰å¤±è´¥åŸå› åˆ†å¸ƒ
                reason_stats = db.session.query(
                    FailedTID.failure_reason, func.count(FailedTID.id)
                ).filter(FailedTID.status.in_(['pending', 'retrying'])).group_by(FailedTID.failure_reason).all()
                reason_stats = dict(reason_stats)
            
            logger.info(f"ğŸ“Š å¤±è´¥TIDç»Ÿè®¡: æ€»æ•°: {total_count}")
            return {'total_count': total_count, 'by_section': section_stats, 'by_reason': reason_stats}
        except Exception as e:
            logger.error(f"åˆ†æå¤±è´¥: {e}")
            return None
    
    def cleanup_existing_tids(self):
        """æ¸…ç†å¤±è´¥åˆ—è¡¨ä¸­å·²å­˜åœ¨äºæœ¬åœ°æ•°æ®åº“çš„TID"""
        logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†å¤±è´¥TIDåˆ—è¡¨ä¸­çš„é‡å¤é¡¹")
        
        try:
            # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(self.main_db_path):
                logger.error(f"âŒ ä¸»æ•°æ®åº“ä¸å­˜åœ¨: {self.main_db_path}")
                return None
                
            if not os.path.exists(self.failed_db_path):
                logger.error(f"âŒ å¤±è´¥TIDæ•°æ®åº“ä¸å­˜åœ¨: {self.failed_db_path}")
                return None
            
            # è·å–æ‰€æœ‰å¤±è´¥çš„TID
            failed_tids = self._get_failed_tids()
            
            if not failed_tids:
                logger.info("âœ… æ²¡æœ‰å¤±è´¥çš„TIDéœ€è¦æ¸…ç†")
                return {'total_checked': 0, 'already_exists': 0, 'cleaned_up': 0, 'errors': 0}
            
            logger.info(f"ğŸ“‹ æ£€æŸ¥ {len(failed_tids)} ä¸ªå¤±è´¥çš„TID")
            
            cleanup_stats = {
                'total_checked': len(failed_tids),
                'already_exists': 0,
                'cleaned_up': 0,
                'errors': 0
            }
            
            for item in failed_tids:
                tid = item['tid']
                
                try:
                    # æ£€æŸ¥æœ¬åœ°æ•°æ®åº“æ˜¯å¦å·²å­˜åœ¨
                    if self._check_tid_exists(tid):
                        # æœ¬åœ°å·²å­˜åœ¨ï¼Œä»å¤±è´¥åˆ—è¡¨ä¸­ç§»é™¤
                        success = self._mark_tid_success(tid)
                        
                        if success:
                            cleanup_stats['cleaned_up'] += 1
                            title = self._get_resource_title(tid)
                            logger.info(f"âœ… æ¸…ç†TID {tid}: æœ¬åœ°å·²å­˜åœ¨ '{title[:50]}...'")
                        else:
                            cleanup_stats['errors'] += 1
                            logger.warning(f"âš ï¸ æ¸…ç†TID {tid} å¤±è´¥")
                        
                        cleanup_stats['already_exists'] += 1
                    
                except Exception as e:
                    cleanup_stats['errors'] += 1
                    logger.error(f"âŒ æ£€æŸ¥TID {tid} æ—¶å‡ºé”™: {e}")
            
            # è¾“å‡ºæ¸…ç†ç»“æœ
            logger.info("ğŸ‰ æ¸…ç†å®Œæˆ!")
            logger.info(f"   æ€»æ£€æŸ¥æ•°: {cleanup_stats['total_checked']}")
            logger.info(f"   æœ¬åœ°å·²å­˜åœ¨: {cleanup_stats['already_exists']}")
            logger.info(f"   æˆåŠŸæ¸…ç†: {cleanup_stats['cleaned_up']}")
            logger.info(f"   é”™è¯¯æ•°: {cleanup_stats['errors']}")
            
            if cleanup_stats['cleaned_up'] > 0:
                logger.info(f"ğŸ’¡ å·²ä»å¤±è´¥åˆ—è¡¨ä¸­ç§»é™¤ {cleanup_stats['cleaned_up']} ä¸ªé‡å¤çš„TID")
            
            return cleanup_stats
            
        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡ç¨‹å‡ºé”™: {e}")
            return None
    
    def _get_failed_tids(self) -> List[Dict]:
        """è·å–æ‰€æœ‰å¤±è´¥çš„TID"""
        try:
            with db_session_context():
                records = FailedTID.get_pending_tids(limit=1000)
                return [{
                    'tid': r.tid,
                    'section': r.section,
                    'detail_url': r.detail_url,
                    'failure_reason': r.failure_reason,
                    'retry_count': r.retry_count
                } for r in records]
        except Exception as e:
            logger.error(f"è·å–å¤±è´¥TIDåˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def _check_tid_exists(self, tid: int) -> bool:
        """æ£€æŸ¥TIDæ˜¯å¦å­˜åœ¨äºä¸»æ•°æ®åº“ä¸­"""
        try:
            with db_session_context():
                return Resource.query.filter_by(tid=tid).first() is not None
        except Exception as e:
            logger.error(f"æ£€æŸ¥TID {tid} æ—¶å‡ºé”™: {e}")
            return False
    
    def _mark_tid_success(self, tid: int) -> bool:
        """åœ¨å¤±è´¥æ•°æ®åº“ä¸­æ ‡è®°TIDä¸ºæˆåŠŸ"""
        return FailedTID.mark_success(tid)
    
    def _get_resource_title(self, tid: int) -> str:
        """è·å–èµ„æºæ ‡é¢˜"""
        try:
            with db_session_context():
                res = Resource.query.filter_by(tid=tid).first()
                return res.title if res else "æœªçŸ¥æ ‡é¢˜"
        except Exception as e:
            logger.debug(f"è·å–TID {tid} æ ‡é¢˜æ—¶å‡ºé”™: {e}")
            return "æœªçŸ¥æ ‡é¢˜"


# ==================== å¤±è´¥TIDé‡è¯•ç±» ====================

class FailedTidRetryService:
    """å¤±è´¥TIDé‡è¯•æœåŠ¡"""
    
    def __init__(self):
        self.sht = None
        self.app = None
    
    def init_crawler(self):
        """åˆå§‹åŒ–çˆ¬è™«"""
        try:
            # å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–å’Œ certifi é—®é¢˜
            # ä½¿ç”¨æ¨¡å—çº§å¯¼å…¥ä»¥æ‰“ç ´ crawler/__init__.py è§¦å‘çš„å¾ªç¯ä¾èµ–
            from crawler.sync_crawler import SHT
            self.sht = SHT()
            self.app = get_flask_app()
            logger.info("çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False
    
    def retry_failed_tids(self, section: str = None, limit: int = 50,
                         batch_size: int = 10, max_retries: int = 3, 
                         continuous: bool = False, max_rounds: int = 20) -> Dict:
        """é‡è¯•å¤±è´¥çš„TID
        
        Args:
            section: æ¿å—åç§°ï¼ˆå¯é€‰ï¼‰
            limit: æ¯è½®è·å–çš„æœ€å¤§TIDæ•°é‡
            batch_size: æ¯æ‰¹å¤„ç†çš„TIDæ•°é‡
            max_retries: å•ä¸ªTIDçš„æœ€å¤§é‡è¯•æ¬¡æ•°
            continuous: æ˜¯å¦å¾ªç¯é‡è¯•ç›´åˆ°é˜Ÿåˆ—æ¸…ç©ºï¼ˆé»˜è®¤Falseï¼Œåªå¤„ç†ä¸€è½®ï¼‰
            max_rounds: å¾ªç¯æ¨¡å¼ä¸‹çš„æœ€å¤§è½®æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
        
        Returns:
            Dict: é‡è¯•ç»“æœç»Ÿè®¡
        """
        if not self.init_crawler():
            return {'success': False, 'error': 'çˆ¬è™«åˆå§‹åŒ–å¤±è´¥'}

        # æ€»ä½“ç»Ÿè®¡ï¼ˆè·¨è½®æ¬¡ï¼‰
        total_stats = {
            'success': True,
            'total_retry': 0,
            'success_count': 0,
            'failed_count': 0,
            'skipped_count': 0,
            'rounds': 0
        }

        round_num = 0
        while True:
            round_num += 1
            
            # è·å–å¾…é‡è¯•çš„TID
            failed_entries = FailedTID.get_pending_tids(section=section, limit=limit)
            
            if not failed_entries:
                if round_num == 1:
                    logger.info("æ²¡æœ‰éœ€è¦é‡è¯•çš„TID")
                    return {
                        'success': True,
                        'total_retry': 0,
                        'success_count': 0,
                        'failed_count': 0,
                        'skipped_count': 0,
                        'rounds': 0,
                        'message': 'æ²¡æœ‰éœ€è¦é‡è¯•çš„TID'
                    }
                else:
                    logger.info(f"âœ… ç¬¬ {round_num} è½®ï¼šé˜Ÿåˆ—å·²æ¸…ç©º")
                    break

            logger.info(f"{'='*50}")
            logger.info(f"ğŸ”„ ç¬¬ {round_num} è½®é‡è¯•å¼€å§‹ï¼Œæœ¬è½®å¤„ç† {len(failed_entries)} ä¸ªTID")
            logger.info(f"{'='*50}")

            # å‘é€é‡è¯•å¼€å§‹é€šçŸ¥ï¼ˆä»…ç¬¬ä¸€è½®ï¼‰
            if round_num == 1:
                try:
                    from scheduler.notifier import _send_telegram_message
                    section_desc = f"æ¿å—: {section}" if section else "æ‰€æœ‰æ¿å—"
                    mode_desc = f"å¾ªç¯æ¨¡å¼ï¼ˆæœ€å¤š{max_rounds}è½®ï¼‰" if continuous else "å•è½®æ¨¡å¼"
                    notify_msg = f"""ğŸ”„ *å¼€å§‹é‡è¯•å¤±è´¥çš„TID*
â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ {section_desc}
ğŸ”¢ æœ¬è½®æ•°é‡: {len(failed_entries)} ä¸ª
ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size} ä¸ª/æ‰¹
âš™ï¸ æœ€å¤§é‡è¯•æ¬¡æ•°: {max_retries} æ¬¡
ğŸ” æ¨¡å¼: {mode_desc}"""
                    _send_telegram_message(notify_msg, parse_mode='Markdown')
                except Exception as e:
                    logger.debug(f"å‘é€é‡è¯•å¼€å§‹é€šçŸ¥å¤±è´¥: {e}")

            # æœ¬è½®ç»Ÿè®¡
            round_stats = {
                'total_retry': len(failed_entries),
                'success_count': 0,
                'failed_count': 0,
                'skipped_count': 0
            }

            # æ›´æ–°çŠ¶æ€åè°ƒå™¨è¿›åº¦
            try:
                from crawler_control.cc_control_bridge import get_crawler_control_bridge
                bridge = get_crawler_control_bridge()
                # æ ‡è®°ä¸ºè¿è¡Œä¸­ï¼Œå¹¶åˆå§‹åŒ–è¿›åº¦
                bridge.start_crawling()
                bridge.update_progress({
                    'current_section': f'é‡è¯•å¤±è´¥TID (ç¬¬ {round_num} è½®)',
                    'total_saved': total_stats['success_count'],
                    'total_skipped': total_stats['skipped_count'],
                    'processed_pages': round_num,
                    'max_pages': max_rounds if continuous else 1,
                    'message': f'æ­£åœ¨é‡è¯•ç¬¬ {round_num} è½®ï¼Œæœ¬è½® {len(failed_entries)} ä¸ª'
                })
            except Exception as bridge_err:
                logger.debug(f"æ›´æ–°çŠ¶æ€åè°ƒå™¨å¤±è´¥: {bridge_err}")

            # æ ‡è®°ä¸ºé‡è¯•ä¸­çŠ¶æ€
            for f in failed_entries:
                f.status = 'retrying'
            db.session.commit()

            # åˆ†æ‰¹é‡è¯•
            for i in range(0, len(failed_entries), batch_size):
                # æ£€æŸ¥åœæ­¢/æš‚åœä¿¡å·
                try:
                    from crawler_control.cc_control_bridge import get_crawler_control_bridge
                    bridge = get_crawler_control_bridge()
                    if bridge.check_stop_and_pause():
                        logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢é‡è¯•ä»»åŠ¡")
                        break
                except Exception as bridge_err:
                    logger.debug(f"æ£€æŸ¥æ§åˆ¶ä¿¡å·å¤±è´¥: {bridge_err}")

                batch = failed_entries[i:i + batch_size]
                logger.info(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(failed_entries) + batch_size - 1)//batch_size}")
                
                # æ›´æ–°åˆ†æ‰¹è¯¦ç»†è¿›åº¦
                try:
                    bridge.update_progress({
                        'current_section_processed': i,
                        'current_section_pages': len(failed_entries),
                        'message': f'é‡è¯•ä¸­: è½®æ¬¡ {round_num}, è¿›åº¦ {i}/{len(failed_entries)}'
                    })
                except: pass
                detail_urls = []
                for item in batch:
                    tid = item.tid
                    detail_url = item.detail_url
                    if not detail_url:
                        detail_url = f"https://sehuatang.org/forum.php?mod=viewthread&tid={tid}"
                    detail_urls.append((tid, detail_url))
                
                # æ‰¹é‡çˆ¬å–
                try:
                    from configuration import config_manager
                    crawler_mode = config_manager.get('CRAWLER_MODE', 'async')
                    use_batch_mode = crawler_mode in ['async', 'thread']
                    batch_urls = [url for tid, url in detail_urls]

                    logger.info(f"ä½¿ç”¨ {crawler_mode} æ¨¡å¼é‡è¯•TID (æ‰¹é‡æ¨¡å¼: {'æ˜¯' if use_batch_mode else 'å¦'})")
                    batch_results = self.sht.crawler_details_batch(batch_urls, use_batch_mode=use_batch_mode)
                    
                except Exception as e:
                    logger.error(f"æ‰¹é‡çˆ¬å–å¼‚å¸¸: {e}")
                    batch_results = [None] * len(detail_urls)
                
                # å¤„ç†æ‰¹é‡ç»“æœ
                for j, ((tid, detail_url), data) in enumerate(zip(detail_urls, batch_results)):
                    item = batch[j]
                    section_name = item.section or 'æœªçŸ¥æ¿å—'
                    
                    if not data or not data.get('magnet'):
                        # é‡è¯•ä»ç„¶å¤±è´¥
                        failure_reason = "é‡è¯•å¤±è´¥: æ•°æ®æ— æ•ˆæˆ–ç¼ºå°‘ç£åŠ›é“¾æ¥"
                        
                        # è¯†åˆ«æ‹¦æˆªé¡µé¢
                        is_antibot = False
                        if isinstance(data, dict) and data.get('error_type') == 'antibot_detected':
                            is_antibot = True
                            failure_reason = f"è§¦å‘åçˆ¬æ‹¦æˆª: {data.get('error_msg', 'æœªçŸ¥æ‹¦æˆª')}"

                        if item.retry_count >= max_retries:
                            logger.warning(f"âŒ TID {tid} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæš‚æ—¶æ”¾å¼ƒé‡è¯•")
                            failure_reason += f" (å·²è¾¾ä¸Šé™)"
                            item.status = 'abandoned'
                        else:
                            item.status = 'pending'
                            if is_antibot:
                                logger.info(f"â³ TID {tid} è¢«æ‹¦æˆªï¼Œå·²å°†å…¶æ ‡å›ç­‰å¾…é˜Ÿåˆ—")
                        
                        item.failure_reason = failure_reason
                        item.retry_count += 1
                        item.last_retry_time = datetime.now(timezone.utc)
                        db.session.commit()
                        
                        round_stats['failed_count'] += 1
                        logger.warning(f"âŒ TID {tid} é‡è¯•å¤±è´¥: {failure_reason}")
                        
                    else:
                        # é‡è¯•æˆåŠŸï¼Œå°è¯•ä¿å­˜
                        try:
                            with self.app.app_context():
                                saved = self.sht.save_to_db(data, section_name, tid, detail_url)

                                if saved:
                                    FailedTID.mark_success(tid)
                                    round_stats['success_count'] += 1
                                    logger.info(f"âœ… TID {tid} é‡è¯•æˆåŠŸ: {data.get('title', '')[:50]}...")
                                else:
                                    existing = Resource.query.filter_by(tid=tid).first()
                                    if existing:
                                        FailedTID.mark_success(tid)
                                        round_stats['skipped_count'] += 1
                                        logger.info(f"â­ï¸ TID {tid} æ•°æ®å·²å­˜åœ¨")
                                    else:
                                        failure_reason = "é‡è¯•å¤±è´¥: ä¿å­˜å¤±è´¥ä½†åŸå› æœªçŸ¥"
                                        FailedTID.add(tid=tid, section=section_name, url=detail_url, reason=failure_reason)
                                        round_stats['failed_count'] += 1
                                        logger.warning(f"âŒ TID {tid} ä¿å­˜å¤±è´¥")

                        except Exception as e:
                            failure_reason = f"é‡è¯•å¤±è´¥: ä¿å­˜å¼‚å¸¸ - {str(e)}"
                            if "database is locked" in str(e):
                                failure_reason = "é‡è¯•å¤±è´¥: æ•°æ®åº“é”å®šï¼Œç¨åé‡è¯•"
                            FailedTID.add(tid=tid, section=section_name, url=detail_url, reason=failure_reason)
                            round_stats['failed_count'] += 1
                            logger.error(f"âŒ TID {tid} ä¿å­˜å¼‚å¸¸: {e}")
                
                # æ‰¹æ¬¡é—´ä¼‘æ¯
                if i + batch_size < len(failed_entries):
                    time.sleep(2)
            
            # ç´¯åŠ åˆ°æ€»ä½“ç»Ÿè®¡
            total_stats['total_retry'] += round_stats['total_retry']
            total_stats['success_count'] += round_stats['success_count']
            total_stats['failed_count'] += round_stats['failed_count']
            total_stats['skipped_count'] += round_stats['skipped_count']
            total_stats['rounds'] = round_num
            
            # è¾“å‡ºæœ¬è½®ç»“æœ
            logger.info(f"ğŸ“Š ç¬¬ {round_num} è½®å®Œæˆ:")
            logger.info(f"   æœ¬è½®å¤„ç†: {round_stats['total_retry']}")
            logger.info(f"   æˆåŠŸ: {round_stats['success_count']}")
            logger.info(f"   å¤±è´¥: {round_stats['failed_count']}")
            logger.info(f"   å·²å­˜åœ¨: {round_stats['skipped_count']}")
            
            # æ£€æŸ¥æ˜¯å¦ç»§ç»­
            if not continuous:
                logger.info("å•è½®æ¨¡å¼ï¼Œé‡è¯•å®Œæˆ")
                break
            
            if round_num >= max_rounds:
                logger.warning(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§è½®æ•° {max_rounds}ï¼Œåœæ­¢é‡è¯•")
                break
            
            # è½®æ¬¡é—´ä¼‘æ¯
            logger.info("ç­‰å¾… 5 ç§’åå¼€å§‹ä¸‹ä¸€è½®...")
            # æ›´æ–°ä¼‘æ¯çŠ¶æ€
            try:
                bridge.update_progress({
                    'message': 'è½®æ¬¡é—´ä¼‘æ¯ï¼Œ5ç§’åç»§ç»­...'
                })
            except: pass
            time.sleep(5)
        
        # ç»“æŸæ ‡è®°
        try:
            from crawler_control.cc_control_bridge import get_crawler_control_bridge
            bridge = get_crawler_control_bridge()
            bridge.stop_crawling()
        except: pass

        # è¾“å‡ºæ€»ä½“ç»“æœ
        logger.info(f"{'='*50}")
        logger.info(f"ğŸ‰ é‡è¯•å…¨éƒ¨å®Œæˆ!")
        logger.info(f"   æ€»è½®æ•°: {total_stats['rounds']}")
        logger.info(f"   æ€»å¤„ç†æ•°: {total_stats['total_retry']}")
        logger.info(f"   æˆåŠŸæ•°: {total_stats['success_count']}")
        logger.info(f"   å¤±è´¥æ•°: {total_stats['failed_count']}")
        logger.info(f"   å·²å­˜åœ¨: {total_stats['skipped_count']}")
        logger.info(f"{'='*50}")

        success_rate = total_stats['success_count'] / total_stats['total_retry'] * 100 if total_stats['total_retry'] > 0 else 0
        logger.info(f"   æˆåŠŸç‡: {success_rate:.1f}%")

        # å‘é€é‡è¯•å®Œæˆé€šçŸ¥
        try:
            from scheduler.notifier import _send_telegram_message
            section_desc = f"æ¿å—: {section}" if section else "æ‰€æœ‰æ¿å—"
            notify_msg = f"""âœ… *é‡è¯•å¤±è´¥TIDå®Œæˆï¼*
â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ {section_desc}
ğŸ“Š æ€»é‡è¯•æ•°: {total_stats['total_retry']} ä¸ª
âœ… æˆåŠŸæ•°: {total_stats['success_count']} ä¸ª
â­ï¸ å·²å­˜åœ¨: {total_stats['skipped_count']} ä¸ª
âŒ ä»å¤±è´¥: {total_stats['failed_count']} ä¸ª
ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%"""
            _send_telegram_message(notify_msg, parse_mode='Markdown')
        except Exception as e:
            logger.debug(f"å‘é€é‡è¯•å®Œæˆé€šçŸ¥å¤±è´¥: {e}")

        return {
            'success': True,
            **total_stats,
            'success_rate': success_rate
        }


# ==================== å…ƒæ•°æ®ä¿®å¤åŠŸèƒ½ ====================

def recycle_incomplete_resources(limit: int = 100, hours: int = 2, dry_run: bool = False):
    """
    å¤–ç§‘æ‰‹æœ¯å¼å›æ”¶ï¼šåªé’ˆå¯¹åˆšåˆšäº§ç”Ÿçš„æ®‹ç¼ºè®°å½•ã€‚
    dry_run ä¸º True æ—¶ï¼Œåªåˆ—å‡ºæ¸…å•ä¸å®é™…æ‰§è¡Œã€‚
    """
    from models import Resource, FailedTID, db
    from datetime import datetime, timedelta, timezone
    app = get_flask_app()
    
    with app.app_context():
        cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours)
        
        query = Resource.query.filter(
            Resource.created_at >= cutoff_time,
            (Resource.magnet != None),
            (
                (Resource.sub_type == None) | (Resource.sub_type == '') | 
                (Resource.sub_type == 'æœªçŸ¥') | (Resource.sub_type == 'é»˜è®¤')
            )
        )
        total_matched = query.count()
        
        mode_prefix = "[è¯•è¿è¡Œ] " if dry_run else ""
        logger.info(f"ğŸ“Š {mode_prefix}æ•°æ®åº“æ‰«ææŠ¥å‘Šï¼šå‘ç° {total_matched} æ¡æœ€è¿‘ {hours} å°æ—¶å…¥åº“çš„æ®‹ç¼ºèµ„æºã€‚")
        
        if total_matched == 0:
            return {"recycled_count": 0}
        
        targets = query.order_by(Resource.created_at.desc()).limit(limit).all()
        logger.info(f"âš™ï¸ {mode_prefix}å€™é€‰åå• (æ˜¾ç¤ºå‰ {len(targets)} æ¡):")
        print("\n" + "-"*80)
        print(f"{'TID':<10} | {'åˆ†ç±»':<10} | {'å¤§å°':<10} | {'å…¥åº“æ—¶é—´':<20} | {'æ ‡é¢˜'}")
        print("-"*80)
        
        recycled_count = 0
        for res in targets:
            local_time = res.created_at.astimezone() if res.created_at.tzinfo else res.created_at
            time_str = local_time.strftime('%Y-%m-%d %H:%M:%S')
            size_str = f"{res.size}MB" if res.size else "0MB"
            print(f"{res.tid:<10} | {str(res.sub_type):<10} | {size_str:<10} | {time_str:<20} | {res.title[:40]}...")
            
            if not dry_run:
                try:
                    FailedTID.add(
                        tid=res.tid,
                        section=res.section,
                        url=res.detail_url or f"https://sehuatang.org/forum.php?mod=viewthread&tid={res.tid}",
                        reason=f"ç²¾å‡†å›ç‚‰(å…¥åº“äº {local_time.strftime('%H:%M')})"
                    )
                    db.session.delete(res)
                    recycled_count += 1
                except Exception as e:
                    logger.error(f"âŒ æ’¤å›å¤±è´¥: {res.tid}, {e}")

        if not dry_run:
            db.session.commit()
            logger.info(f"ğŸ‰ ä»»åŠ¡å·²å®Œæˆï¼šæœ¬æ¬¡æˆåŠŸæ’¤å› {recycled_count} æ¡æ•°æ®ã€‚")
        else:
            print("-"*80)
            logger.info(f"ğŸ’¡ ä»¥ä¸Šä¸ºé¢„è§ˆç»“æœï¼Œæ•°æ®åº“æœªåšä»»ä½•æ›´æ”¹ã€‚å¦‚éœ€æ­£å¼æ‰§è¡Œï¼Œè¯·å»æ‰ --dry-run å‚æ•°ã€‚")
            
        return {"recycled_count": recycled_count}


# ==================== å‘½ä»¤è¡Œæ¥å£ ====================

def main():
    """ä¸»å‡½æ•° - ç»Ÿä¸€çš„å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='SHTèµ„æºèšåˆç³»ç»Ÿ - ç»Ÿä¸€ç»´æŠ¤å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
å‘½ä»¤ç¤ºä¾‹:
  %(prog)s db-info                  æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯
  %(prog)s db-cleanup               æ¸…ç†é‡å¤æ•°æ®
  %(prog)s db-optimize              ä¼˜åŒ–æ•°æ®åº“
  %(prog)s db-normalize             æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
  %(prog)s failed-analyze           åˆ†æå¤±è´¥TID
  %(prog)s failed-cleanup           æ¸…ç†å¤±è´¥TID
  %(prog)s failed-retry --limit 50  é‡è¯•å¤±è´¥TIDï¼ˆé™åˆ¶50ä¸ªï¼‰
  %(prog)s full-maintenance         è¿è¡Œå®Œæ•´ç»´æŠ¤æµç¨‹
        """
    )
    
    parser.add_argument('command', 
                       choices=[
                           'db-info', 'db-cleanup', 'db-optimize', 'db-normalize',
                           'failed-analyze', 'failed-cleanup', 'failed-retry',
                           'full-maintenance', 'recycle-data'
                       ],
                       help='è¦æ‰§è¡Œçš„ç»´æŠ¤å‘½ä»¤')
    
    # é€šç”¨å‚æ•°
    parser.add_argument('--log-level', '-l', default='INFO', 
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       help='æ—¥å¿—çº§åˆ«')
    parser.add_argument('--cleanup-days', '-d', type=int, default=30,
                       metavar='DAYS',
                       help='æ¸…ç†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„æ—§æ•°æ®ï¼ˆé»˜è®¤: 30å¤©ï¼‰')
    
    # é‡è¯•ç›¸å…³å‚æ•°
    parser.add_argument('--limit', type=int, default=50, 
                       help='é‡è¯•TIDæ•°é‡é™åˆ¶ (ä»…ç”¨äº failed-retry)')
    parser.add_argument('--section', '-s', 
                       help='æŒ‡å®šæ¿å—åç§° (ä»…ç”¨äº failed-retry)')
    parser.add_argument('--batch-size', '-b', type=int, default=10,
                       help='æ‰¹é‡å¤§å° (ä»…ç”¨äº failed-retry)')
    parser.add_argument('--dry-run', action='store_true',
                       help='è¯•è¿è¡Œæ¨¡å¼ï¼Œåªåˆ—å‡ºå—å½±å“çš„æ•°æ®è€Œä¸å®é™…æ‰§è¡Œ (ä»…ç”¨äº recycle-data)')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    setup_logging(args.log_level)
    
    # æ‰§è¡Œå‘½ä»¤
    try:
        if args.command == 'db-info':
            # æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯
            maintenance = DatabaseMaintenance()
            info = maintenance.get_database_info()
            print("\n" + "="*50)
            print("æ•°æ®åº“ä¿¡æ¯")
            print("="*50)
            for key, value in info.items():
                print(f"  {key}: {value}")
            print("="*50)
            
        elif args.command == 'db-cleanup':
            # æ¸…ç†é‡å¤æ•°æ®
            maintenance = DatabaseMaintenance()
            with maintenance.app.app_context():
                maintenance.clean_duplicates()
                maintenance.print_maintenance_report()
                
        elif args.command == 'db-optimize':
            # ä¼˜åŒ–æ•°æ®åº“
            maintenance = DatabaseMaintenance()
            with maintenance.app.app_context():
                maintenance.optimize_database()
                
        elif args.command == 'db-normalize':
            # æ ‡å‡†åŒ–æ—¥æœŸ
            maintenance = DatabaseMaintenance()
            with maintenance.app.app_context():
                maintenance.normalize_dates()
                
        elif args.command == 'failed-analyze':
            # åˆ†æå¤±è´¥TID
            cleaner = FailedTidCleaner()
            cleaner.analyze_failed_tids()
            
        elif args.command == 'failed-cleanup':
            # æ¸…ç†å¤±è´¥TID
            cleaner = FailedTidCleaner()
            cleaner.cleanup_existing_tids()
            
        elif args.command == 'failed-retry':
            # é‡è¯•å¤±è´¥TID
            retry_service = FailedTidRetryService()
            result = retry_service.retry_failed_tids(
                section=args.section,
                limit=args.limit,
                batch_size=args.batch_size
            )
            
            if result['success']:
                print(f"\nğŸ‰ é‡è¯•å®Œæˆ!")
                print(f"   æ€»é‡è¯•æ•°: {result['total_retry']}")
                print(f"   æˆåŠŸæ•°: {result['success_count']}")
                print(f"   å¤±è´¥æ•°: {result['failed_count']}")
                print(f"   å·²å­˜åœ¨: {result['skipped_count']}")
                print(f"   æˆåŠŸç‡: {result.get('success_rate', 0):.1f}%")
            else:
                print(f"âŒ é‡è¯•å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
        elif args.command == 'full-maintenance':
            # å®Œæ•´ç»´æŠ¤æµç¨‹
            print("ğŸš€ å¼€å§‹å®Œæ•´ç»´æŠ¤æµç¨‹...")
            print("\næ­¥éª¤ 1/4: æ•°æ®åº“ç»´æŠ¤")
            maintenance = DatabaseMaintenance()
            maintenance.run_full_maintenance()
            
            print("\næ­¥éª¤ 2/4: æ¸…ç†æ—§æ•°æ®ï¼ˆå¯é€‰ï¼‰")
            days = args.cleanup_days if hasattr(args, 'cleanup_days') else None
            if days:
                cleanup_result = maintenance.cleanup_old_records(days)
                print(f"\nâœ… æ¸…ç†äº† {cleanup_result['deleted_resources']} ä¸ªæ—§èµ„æºè®°å½•ï¼ˆä¿ç•™ {days} å¤©å†…çš„æ•°æ®ï¼‰")
                print(f"\nâœ… æ¸…ç†äº† {cleanup_result['deleted_failed_tids']} ä¸ªå¤±è´¥TIDè®°å½•")
            
            print("\næ­¥éª¤ 3/4: æ•°æ®åº“ä¼˜åŒ–")
            maintenance.optimize_database()
            print("\nâœ… æ•°æ®åº“ä¼˜åŒ–å®Œæˆ")
            
            print("\næ­¥éª¤ 4/4: æ¸…ç†ç¼“å­˜")
            maintenance.clear_cache()
            print("\nâœ… ç¼“å­˜å·²æ¸…ç†")
            
            print("\næ­¥éª¤ 5/4: æ›´æ–°ç»Ÿè®¡")
            maintenance.update_statistics()
            print("\nâœ… ç»Ÿè®¡ä¿¡æ¯å·²æ›´æ–°")
            
            print("\n" + "="*50)
            days = args.cleanup_days if hasattr(args, 'cleanup_days') else None
            if days:
                cleanup_result = maintenance.cleanup_old_records(days)
                print(f"\nâœ… æ¸…ç†äº† {cleanup_result['deleted_resources']} ä¸ªæ—§èµ„æºè®°å½•")
                print(f"âœ… æ¸…ç†äº† {cleanup_result['deleted_failed_tids']} ä¸ªæ—§å¤±è´¥TIDè®°å½•")
            
            print("\næ­¥éª¤ 3/4: æ•°æ®åº“ä¼˜åŒ–")
            maintenance.optimize_database()
            
            print("\næ­¥éª¤ 2/4: åˆ†æå¤±è´¥TID")
            cleaner = FailedTidCleaner()
            cleaner.analyze_failed_tids()
            
            print("\næ­¥éª¤ 3/4: æ¸…ç†å¤±è´¥TID")
            cleaner.cleanup_existing_tids()
            
            print("\næ­¥éª¤ 4/4: é‡è¯•éƒ¨åˆ†å¤±è´¥TID")
            retry_service = FailedTidRetryService()
            retry_service.retry_failed_tids(limit=20)
            
            print("\nâœ… å®Œæ•´ç»´æŠ¤æµç¨‹å·²å®Œæˆ!")

        elif args.command == 'recycle-data':
            # æ®‹ç¼ºæ•°æ®å›ç‚‰é‡é€ 
            action_text = "é¢„è§ˆ" if args.dry_run else "æ‰§è¡Œ"
            print(f"ï¸ å¼€å§‹{action_text}ï¼šå°†æ®‹ç¼ºèµ„æºé€€å›é‡è¯•é˜Ÿåˆ—...")
            result = recycle_incomplete_resources(limit=args.limit, dry_run=args.dry_run)
            if not args.dry_run:
                print(f"\nâœ… æˆåŠŸå¤„ç† {result['recycled_count']} æ¡è®°å½•ã€‚å®ƒä»¬å·²å‡ºç°åœ¨'å¤±è´¥é‡è¯•'åˆ—è¡¨ä¸­ã€‚")
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸ æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"æ‰§è¡Œå‘½ä»¤æ—¶å‡ºé”™: {e}", exc_info=True)
        sys.exit(1)


if __name__ == '__main__':
    main()
