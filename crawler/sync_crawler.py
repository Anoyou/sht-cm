#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SHT åŒæ­¥çˆ¬è™« - ä¸»çˆ¬è™«ç±»

æä¾›å®Œæ•´çš„çˆ¬è™«åŠŸèƒ½ï¼š
- æ¿å—ä¿¡æ¯è·å–
- TID åˆ—è¡¨çˆ¬å–
- è¯¦æƒ…é¡µçˆ¬å–
- æ•°æ®ä¿å­˜

ç»§æ‰¿è‡ª SHTBaseï¼Œå¤ç”¨ç½‘ç»œè¯·æ±‚å’Œé˜²å±è”½æœºåˆ¶
"""

import os
import re
import time
import random
import logging
from typing import List, Dict, Any, Optional
from urllib.parse import urlparse, parse_qs, urlencode, urljoin
from pyquery import PyQuery as pq
from datetime import datetime, timedelta
import bencodepy
import hashlib
import binascii

from .base import SHTBase
from .parser import (
    extract_and_convert_video_size,
    extract_safeid,
    extract_exact_datetime,
    extract_bracket_content
)
from .batch_processor import BatchProcessor

# è·å–æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)


class SHT(SHTBase):
    """SHT ä¸»çˆ¬è™«ç±» - ç»§æ‰¿è‡ª SHTBase
    
    æä¾›å®Œæ•´çš„çˆ¬è™«åŠŸèƒ½ï¼š
    - æ¿å—ä¿¡æ¯è·å–ï¼ˆä»æ‰‹æœºç‰ˆåˆ—è¡¨é¡µæˆ–æ¡Œé¢ç‰ˆé¦–é¡µï¼‰
    - TID åˆ—è¡¨çˆ¬å–
    - è¯¦æƒ…é¡µçˆ¬å–ï¼ˆå•ä¸ªæˆ–æ‰¹é‡ï¼‰
    - æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“
    """
    
    def __init__(self):
        """åˆå§‹åŒ– SHT çˆ¬è™«"""
        super().__init__()
        
        # æ¿å—ä¿¡æ¯ç¼“å­˜
        self._forums_cache = None
        self._forums_cache_time = 0
        self._cache_duration = 300  # 5åˆ†é’Ÿç¼“å­˜
        self._cache_expiry = 0  # ç¼“å­˜è¿‡æœŸæ—¶é—´æˆ³
        
        # é”™è¯¯ç±»å‹è®¡æ•°å™¨ - é¿å…æ—¥å¿—åˆ·å±å’Œè¢«åçˆ¬
        self._error_type_counter = {}
        self._error_threshold = 15  # ç›¸åŒé”™è¯¯ç±»å‹çš„é˜ˆå€¼
        self._should_stop_crawling = False  # åœæ­¢çˆ¬å–æ ‡å¿—
        
        logger.debug("SHT çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
    
    # ==================== æ¿å—ä¿¡æ¯è·å– ====================
    
    def get_all_forums_info(self) -> Dict[str, Dict]:
        """è·å–æ‰€æœ‰æ¿å—ä¿¡æ¯ - åŒé‡ç­–ç•¥ï¼šä¼˜å…ˆæ‰‹æœºç‰ˆåˆ—è¡¨é¡µï¼Œå¤±è´¥åˆ™å°è¯•æ¡Œé¢ç‰ˆé¦–é¡µ
        
        Returns:
            Dict[str, Dict]: æ¿å—ä¿¡æ¯å­—å…¸ï¼Œæ ¼å¼ï¼š
                {
                    'fid': {
                        'fid': str,
                        'name': str,
                        'total_topics': int or None,
                        'total_pages': int or None
                    }
                }
        """
        # æ£€æŸ¥å®ä¾‹ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        current_time = time.time()
        if self._forums_cache and (current_time - self._forums_cache_time) < self._cache_duration:
            logger.info(f"âœ“ ä½¿ç”¨å®ä¾‹ç¼“å­˜çš„æ¿å—ä¿¡æ¯ï¼ˆ{int(self._cache_duration - (current_time - self._forums_cache_time))}ç§’åè¿‡æœŸï¼‰")
            return self._forums_cache
        
        logger.info("[CRAWLER] å¼€å§‹è·å–æ‰€æœ‰æ¿å—ä¿¡æ¯ï¼ˆå‡†ç¡®æ•°æ®ï¼ŒåŒé‡ç­–ç•¥ï¼Œå¸¦é‡è¯•æœºåˆ¶ï¼‰")
        
        # ç­–ç•¥Aï¼šä¼˜å…ˆå°è¯•æ‰‹æœºç‰ˆæ¿å—åˆ—è¡¨é¡µï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼šæœ€å¤š3æ¬¡ï¼Œé—´éš”5ç§’ï¼‰
        forums_info = self._retry_with_delay(
            func=self._get_forums_from_mobile_list,
            func_name="ç­–ç•¥A-æ‰‹æœºç‰ˆåˆ—è¡¨é¡µ",
            max_attempts=3,
            delay_seconds=5
        )
        
        # ç­–ç•¥Bï¼šå¦‚æœæ‰‹æœºç‰ˆå¤±è´¥ï¼Œå°è¯•æ¡Œé¢ç‰ˆé¦–é¡µä½œä¸ºå¤‡ä»½ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        if not forums_info:
            logger.warning(f"! æ‰‹æœºç‰ˆæ¿å—åˆ—è¡¨é¡µè·å–å¤±è´¥ï¼Œå°è¯•æ¡Œé¢ç‰ˆé¦–é¡µä½œä¸ºå¤‡ä»½")
            forums_info = self._retry_with_delay(
                func=self._get_forums_from_desktop_home,
                func_name="ç­–ç•¥B-æ¡Œé¢ç‰ˆé¦–é¡µ",
                max_attempts=3,
                delay_seconds=5
            )
        
        # å¦‚æœä¸¤ç§æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨é¢„å®šä¹‰åˆ—è¡¨
        if not forums_info:
            logger.warning(f"! ä¸¤ç§æ–¹æ³•å‡å¤±è´¥ï¼Œä½¿ç”¨é¢„å®šä¹‰æ¿å—åˆ—è¡¨")
            forums_info = self._get_default_forums()
        
        # æ›´æ–°ç¼“å­˜
        self._forums_cache = forums_info
        self._forums_cache_time = current_time
        
        return forums_info
    
    def _get_forums_from_mobile_list(self) -> Dict[str, Dict]:
        """ç­–ç•¥Aï¼šä»æ‰‹æœºç‰ˆæ¿å—åˆ—è¡¨é¡µæå–å‡†ç¡®æ•°æ®
        
        Returns:
            Dict[str, Dict]: æ¿å—ä¿¡æ¯å­—å…¸
        """
        try:
            # åªè·å–é¢„è®¾çš„11ä¸ªæ¿å—
            from constants import VALID_FIDS
            
            logger.info(f"[CRAWLER] [ç­–ç•¥A] å°è¯•ä»æ‰‹æœºç‰ˆæ¿å—åˆ—è¡¨é¡µè·å–ï¼ˆä»…é™{len(VALID_FIDS)}ä¸ªé¢„è®¾æ¿å—ï¼‰")
            
            url = "https://sehuatang.org/forum.php?forumlist=1&mobile=2"
            html = self.get_original(url)
            
            if not html:
                logger.warning(f"! [ç­–ç•¥A] æ— æ³•è·å–æ¿å—åˆ—è¡¨é¡µ")
                return {}
            
            # ç»Ÿä¸€ä¼ é€’ bytes ç»™ pq
            html_bytes = html if isinstance(html, bytes) else html.encode('utf-8')
            doc = pq(html_bytes)
            forum_items = doc('div.sub_forum ul li')
            
            if len(forum_items) == 0:
                logger.warning(f"! [ç­–ç•¥A] æœªæ‰¾åˆ°æ¿å—åˆ—è¡¨é¡¹")
                return {}
            
            logger.debug(f"[CRAWLER] [ç­–ç•¥A] åœ¨æ¿å—åˆ—è¡¨é¡µæ‰¾åˆ° {len(forum_items)} ä¸ªæ¿å—ï¼Œç­›é€‰é¢„è®¾æ¿å—")
            forums_info = {}
            
            for item in forum_items:
                try:
                    item_pq = pq(item)
                    
                    # æå–æ¿å—é“¾æ¥å’Œfid
                    link_elem = item_pq('a.btdb').eq(0)
                    if not link_elem:
                        continue
                    
                    href = link_elem.attr('href')
                    if not href:
                        continue
                    
                    fid_match = re.search(r'fid=(\d+)', href)
                    if not fid_match:
                        continue
                    
                    fid = fid_match.group(1)
                    
                    # åªå¤„ç†é¢„è®¾çš„æ¿å—
                    if fid not in VALID_FIDS:
                        continue
                    
                    # æå–æ¿å—åç§°ï¼ˆå»é™¤<span class="num">ï¼‰
                    name_text = link_elem.text().strip()
                    num_span = link_elem.find('span.num')
                    if num_span:
                        num_text = num_span.text().strip()
                        name = name_text.replace(num_text, '').strip()
                    else:
                        name = name_text
                    
                    if not name:
                        continue
                    
                    # æå–ä¸»é¢˜æ•°ï¼š<i>ä¸»é¢˜:<span title="41167">4ä¸‡</span> å¸–æ•°:...</i>
                    total_topics = None
                    stats_elem = item_pq('i').eq(0)
                    if stats_elem:
                        stats_text = stats_elem.text()
                        if 'ä¸»é¢˜' in stats_text:
                            # ä¼˜å…ˆä»span titleæå–
                            topic_spans = stats_elem.find('span[title]')
                            for span in topic_spans:
                                span_pq = pq(span)
                                span_html = stats_elem.html()
                                span_outer = span_pq.outerHtml()
                                
                                span_index = span_html.find(span_outer) if span_outer else -1
                                if span_index > 0:
                                    before_text = span_html[:span_index]
                                    if 'ä¸»é¢˜' in before_text and before_text.rfind('ä¸»é¢˜') > before_text.rfind('å¸–æ•°'):
                                        title_value = span_pq.attr('title')
                                        if title_value and title_value.isdigit():
                                            total_topics = int(title_value)
                                            logger.debug(f"âœ… [ç­–ç•¥A] [{name}] ä»titleå±æ€§æå–: {total_topics}")
                                            break
                            
                            # å¤‡ç”¨ï¼šä»æ–‡æœ¬æå–
                            if total_topics is None:
                                topic_match = re.search(r'ä¸»é¢˜[ï¼š:]\s*(\d+)', stats_text)
                                if topic_match:
                                    match_pos = topic_match.start()
                                    if 'ä¸»é¢˜' in stats_text[:match_pos + 10]:
                                        total_topics = int(topic_match.group(1))
                                        logger.debug(f"âœ… [ç­–ç•¥A] [{name}] ä»æ–‡æœ¬æå–: {total_topics}")
                    
                    forums_info[fid] = {
                        'fid': fid,
                        'name': name,
                        'total_topics': total_topics,
                        'total_pages': None,
                    }
                    
                    if total_topics is not None:
                        logger.info(f"[CRAWLER] [ç­–ç•¥A] {name} (fid={fid}) - {total_topics}ä¸»é¢˜")
                
                except Exception as e:
                    logger.debug(f"! [ç­–ç•¥A] è§£ææ¿å—é¡¹å¤±è´¥: {e}")
                    continue
            
            if forums_info:
                logger.info(f"âœ“ [ç­–ç•¥A] æˆåŠŸè·å– {len(forums_info)} ä¸ªæ¿å—ä¿¡æ¯")
            return forums_info
        
        except Exception as e:
            logger.error(f"âœ— [ç­–ç•¥A] å¤±è´¥: {e}")
            return {}
    
    def _get_forums_from_desktop_home(self) -> Dict[str, Dict]:
        """ç­–ç•¥Bï¼šä»æ¡Œé¢ç‰ˆé¦–é¡µæå–å‡†ç¡®æ•°æ®ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        
        Returns:
            Dict[str, Dict]: æ¿å—ä¿¡æ¯å­—å…¸
        """
        try:
            # åªè·å–é¢„è®¾çš„11ä¸ªæ¿å—
            from constants import VALID_FIDS
            
            logger.info(f"[CRAWLER] [ç­–ç•¥B] å°è¯•ä»æ¡Œé¢ç‰ˆé¦–é¡µè·å–ï¼ˆä»…é™{len(VALID_FIDS)}ä¸ªé¢„è®¾æ¿å—ï¼‰")
            
            # ä¸´æ—¶åˆ‡æ¢åˆ°æ¡Œé¢ç‰ˆUA
            original_ua = self.headers.get('User-Agent')
            desktop_ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            self.headers['User-Agent'] = desktop_ua
            
            try:
                url = "https://sehuatang.org/forum.php"
                html = self.get_original(url)
            finally:
                self.headers['User-Agent'] = original_ua
            
            if not html:
                logger.warning(f"! [ç­–ç•¥B] æ— æ³•è·å–é¦–é¡µ")
                return {}
            
            # ç»Ÿä¸€ä¼ é€’ bytes ç»™ pqï¼Œè®© lxml è‡ªè¡Œå¤„ç†ç¼–ç å£°æ˜
            html_bytes = html if isinstance(html, bytes) else html.encode('utf-8')
            doc = pq(html_bytes)
            forum_cards = doc('td.fl_g')
            
            if len(forum_cards) == 0:
                logger.warning(f"! [ç­–ç•¥B] æœªæ‰¾åˆ°æ¿å—å¡ç‰‡")
                return {}
            
            logger.debug(f"ğŸ“‹ [ç­–ç•¥B] åœ¨é¦–é¡µæ‰¾åˆ° {len(forum_cards)} ä¸ªæ¿å—å¡ç‰‡ï¼Œç­›é€‰é¢„è®¾æ¿å—")
            forums_info = {}
            
            for card in forum_cards:
                try:
                    card_pq = pq(card)
                    
                    # æå–æ¿å—é“¾æ¥å’Œåç§°
                    link_elem = card_pq('dt a').eq(0)
                    if not link_elem:
                        continue
                    
                    name = link_elem.text().strip()
                    href = link_elem.attr('href')
                    
                    if not href or not name:
                        continue
                    
                    # ä»hrefæå–fid: forum-2-1.html -> fid=2
                    fid_match = re.search(r'forum-(\d+)-', href)
                    if not fid_match:
                        continue
                    
                    fid = fid_match.group(1)
                    
                    # åªå¤„ç†é¢„è®¾çš„æ¿å—
                    if fid not in VALID_FIDS:
                        continue
                    
                    # æå–ä¸»é¢˜æ•°ï¼š<dd><em>ä¸»é¢˜: <span title="68250">6ä¸‡</span></em>...
                    total_topics = None
                    topic_dd = card_pq('dd').eq(0)
                    if topic_dd:
                        topic_text = topic_dd.text()
                        if 'ä¸»é¢˜' in topic_text:
                            # ä¼˜å…ˆä»span titleæå–
                            topic_spans = topic_dd.find('span[title]')
                            for span in topic_spans:
                                span_pq = pq(span)
                                parent_text = span_pq.parent().text()
                                
                                if 'ä¸»é¢˜' in parent_text and 'å¸–æ•°' not in parent_text:
                                    title_value = span_pq.attr('title')
                                    if title_value and title_value.isdigit():
                                        total_topics = int(title_value)
                                        logger.debug(f"âœ… [ç­–ç•¥B] [{name}] ä»titleå±æ€§æå–: {total_topics}")
                                        break
                            
                            # å¤‡ç”¨ï¼šä»æ–‡æœ¬æå–
                            if total_topics is None:
                                topic_match = re.search(r'ä¸»é¢˜[ï¼š:]\s*(\d+)', topic_text)
                                if topic_match:
                                    total_topics = int(topic_match.group(1))
                                    logger.debug(f"âœ… [ç­–ç•¥B] [{name}] ä»æ–‡æœ¬æå–: {total_topics}")
                    
                    forums_info[fid] = {
                        'fid': fid,
                        'name': name,
                        'total_topics': total_topics,
                        'total_pages': None,
                    }
                    
                    if total_topics is not None:
                        logger.info(f"ğŸ“‹ [ç­–ç•¥B] {name} (fid={fid}) - {total_topics}ä¸»é¢˜")
                
                except Exception as e:
                    logger.debug(f"âš ï¸ [ç­–ç•¥B] è§£ææ¿å—å¡ç‰‡å¤±è´¥: {e}")
                    continue
            
            if forums_info:
                logger.info(f"âœ“ [ç­–ç•¥B] æˆåŠŸè·å– {len(forums_info)} ä¸ªæ¿å—ä¿¡æ¯")
            return forums_info
        
        except Exception as e:
            logger.error(f"âœ— [ç­–ç•¥B] å¤±è´¥: {e}")
            return {}
    
    def _get_default_forums(self) -> Dict[str, Dict]:
        """è·å–é¢„å®šä¹‰çš„æ¿å—åˆ—è¡¨
        
        Returns:
            Dict[str, Dict]: æ¿å—ä¿¡æ¯å­—å…¸
        """
        default_forums = {
            '2': 'å›½äº§åŸåˆ›',
            '36': 'äºšæ´²æ— ç åŸåˆ›',
            '37': 'äºšæ´²æœ‰ç åŸåˆ›',
            '103': 'é«˜æ¸…ä¸­æ–‡å­—å¹•',
            '107': 'ä¸‰çº§å†™çœŸ',
            '160': 'VRè§†é¢‘åŒº',
            '104': 'ç´ äººæœ‰ç ç³»åˆ—',
            '38': 'æ¬§ç¾æ— ç ',
            '151': '4KåŸç‰ˆ',
            '152': 'éŸ©å›½ä¸»æ’­',
            '39': 'åŠ¨æ¼«åŸåˆ›'
        }
        
        forums_info = {}
        for fid, name in default_forums.items():
            forums_info[fid] = {
                'fid': fid,
                'name': name,
                'total_topics': None,  # ä¸ä½¿ç”¨ä¼°ç®—å€¼ï¼Œè¿”å›Noneè¡¨ç¤ºéœ€è¦åŒæ­¥
                'total_pages': None    # ä¸ä½¿ç”¨ä¼°ç®—å€¼ï¼Œè¿”å›Noneè¡¨ç¤ºéœ€è¦åŒæ­¥
            }
        
        return forums_info
    
    def get_forum_info(self, fid: str, all_forums_cache: Optional[Dict] = None) -> Optional[Dict]:
        """è·å–å•ä¸ªæ¿å—ä¿¡æ¯ï¼šæ€»é¡µæ•°ã€ä¸»é¢˜æ•°é‡ç­‰
        
        Args:
            fid: æ¿å—ID
            all_forums_cache: å¯é€‰çš„æ¿å—ä¿¡æ¯ç¼“å­˜ï¼Œé¿å…é‡å¤è·å–
        
        Returns:
            Dict: æ¿å—ä¿¡æ¯ï¼ŒåŒ…å« fid, name, total_topics, total_pages
        """
        logger.debug(f" è·å–æ¿å—ä¿¡æ¯: fid={fid}")
        
        try:
            # ä¼˜å…ˆä»é¦–é¡µè·å–å‡†ç¡®çš„ä¸»é¢˜æ•°ï¼Œè€Œä¸æ˜¯ä»å•ä¸ªæ¿å—é¡µé¢æ¨æ–­
            # å¦‚æœæ²¡æœ‰ä¼ å…¥ç¼“å­˜ï¼Œåˆ™è°ƒç”¨get_all_forums_info()è·å–é¦–é¡µæ•°æ®
            if not all_forums_cache:
                logger.debug("ğŸ“‹ æœªä¼ å…¥ç¼“å­˜ï¼Œä»é¦–é¡µè·å–æ‰€æœ‰æ¿å—ä¿¡æ¯")
                all_forums_cache = self.get_all_forums_info()
            
            # ä½¿ç”¨é¦–é¡µè·å–çš„å‡†ç¡®æ•°æ®
            if all_forums_cache and fid in all_forums_cache:
                forum_info = {
                    'fid': fid,
                    'name': all_forums_cache[fid]['name'],
                    'total_topics': all_forums_cache[fid].get('total_topics'),
                    'total_pages': all_forums_cache[fid].get('total_pages')
                }
                logger.debug(f"âœ… ä»é¦–é¡µæ•°æ®è·å–åˆ°å‡†ç¡®ä¿¡æ¯: total_topics={forum_info['total_topics']}")
            else:
                # å°è¯•ä»æŒä¹…åŒ–ç¼“å­˜è·å–
                try:
                    from models import Category
                    cat = Category.query.filter_by(fid=str(fid)).first()
                    cached_forum = cat.to_dict() if cat else None
                    
                    if cached_forum:
                        forum_info = {
                            'fid': fid,
                            'name': cached_forum['name'],
                            'total_topics': cached_forum.get('total_topics'),
                            'total_pages': cached_forum.get('total_pages')
                        }
                        logger.debug(f"âœ… ä»æŒä¹…åŒ–ç¼“å­˜è·å–åˆ°ä¿¡æ¯: {forum_info}")
                    else:
                        # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        forum_info = {
                            'fid': fid,
                            'name': f'æ¿å—{fid}',
                            'total_topics': None,
                            'total_pages': None
                        }
                        logger.warning(f"! ç¼“å­˜ä¸­æœªæ‰¾åˆ°fid={fid}ï¼Œä½¿ç”¨é»˜è®¤ä¿¡æ¯")
                except ImportError:
                    # å¦‚æœmodelsæ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    forum_info = {
                        'fid': fid,
                        'name': f'æ¿å—{fid}',
                        'total_topics': None,
                        'total_pages': None
                    }
            
            # åªåœ¨éœ€è¦æ—¶è·å–é¡µæ•°ä¿¡æ¯ï¼ˆé¦–é¡µæ²¡æœ‰é¡µæ•°ï¼Œéœ€è¦å•ç‹¬è·å–ï¼‰
            if forum_info['total_pages'] is None:
                url = f"https://sehuatang.org/forum.php?mod=forumdisplay&fid={fid}&mobile=2"
                logger.debug(f" è·å–é¡µæ•°ä¿¡æ¯: {url}")
                
                html = self.get_original(url)
                if html:
                    # ç»Ÿä¸€ä¼ é€’ bytes ç»™ pqï¼Œè®© lxml è‡ªè¡Œå¤„ç†ç¼–ç å£°æ˜
                    html_bytes = html if isinstance(html, bytes) else html.encode('utf-8')
                    doc = pq(html_bytes)
                    pages_found = False
                    
                    # æ–¹æ³•1: ä½¿ç”¨å…·ä½“é€‰æ‹©å™¨è·å–æ€»é¡µæ•°
                    page_span = doc('#fd_page_top > div > label > span')
                    if page_span and page_span.length > 0:
                        page_text = page_span.text()
                        logger.debug(f"ğŸ“„ é¡µæ•°æ–‡æœ¬: {page_text}")
                        
                        # ä» "/ 2272 é¡µ" æ ¼å¼ä¸­æå–é¡µæ•°
                        page_match = re.search(r'/\s*(\d+)\s*é¡µ', page_text)
                        if page_match:
                            forum_info['total_pages'] = int(page_match.group(1))
                            logger.debug(f"âœ… ä»é¡µæ•°spanæå–æ€»é¡µæ•°: {forum_info['total_pages']}")
                            pages_found = True
                        else:
                            # å¤‡ç”¨æ–¹æ³•ï¼šä»titleå±æ€§æå–
                            title_attr = page_span.attr('title')
                            if title_attr:
                                title_match = re.search(r'å…±\s*(\d+)\s*é¡µ', title_attr)
                                if title_match:
                                    forum_info['total_pages'] = int(title_match.group(1))
                                    logger.debug(f"âœ… ä»titleå±æ€§æå–æ€»é¡µæ•°: {forum_info['total_pages']}")
                                    pages_found = True
                    
                    # æ–¹æ³•2: å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨é€šç”¨åˆ†é¡µé€‰æ‹©å™¨
                    if not pages_found:
                        pg_elements = doc('div.pg, .pg, [class*="pg"], .pages, .pagination')
                        for pg_element in pg_elements:
                            pg_pq = pq(pg_element)
                            page_text = pg_pq.text()
                            logger.debug(f"ğŸ“„ å¤‡ç”¨åˆ†é¡µæ–‡æœ¬: {page_text}")
                            
                            # æŸ¥æ‰¾ "/ XX é¡µ" çš„æ–‡æœ¬
                            page_match = re.search(r'/\s*(\d+)\s*é¡µ', page_text)
                            if page_match:
                                forum_info['total_pages'] = int(page_match.group(1))
                                logger.debug(f"âœ… å¤‡ç”¨æ–¹æ³•è§£ææ€»é¡µæ•°: {forum_info['total_pages']}")
                                pages_found = True
                                break
                            
                            # æŸ¥æ‰¾æœ€åä¸€é¡µçš„é“¾æ¥
                            last_links = pg_pq.find('a.last, a[class*="last"], a:contains("æœ«é¡µ"), a:contains("æœ€å")')
                            for last_link in last_links:
                                last_href = pq(last_link).attr('href')
                                if last_href:
                                    page_match = re.search(r'page=(\d+)', last_href)
                                    if not page_match:
                                        page_match = re.search(r'-(\d+)\.html', last_href)
                                    if page_match:
                                        forum_info['total_pages'] = int(page_match.group(1))
                                        logger.debug(f"âœ… ä»é“¾æ¥è§£ææ€»é¡µæ•°: {forum_info['total_pages']}")
                                        pages_found = True
                                        break
                            
                            if pages_found:
                                break
                    
                    # æ–¹æ³•3: æŸ¥æ‰¾æ‰€æœ‰åŒ…å«é¡µæ•°çš„é“¾æ¥
                    if not pages_found:
                        all_links = doc('a[href*="page="], a[href*="forum.php"]')
                        max_page = 0
                        for link in all_links:
                            href = pq(link).attr('href')
                            if href:
                                page_match = re.search(r'page=(\d+)', href)
                                if page_match:
                                    page_num = int(page_match.group(1))
                                    if page_num > max_page:
                                        max_page = page_num
                        
                        if max_page > 1:
                            forum_info['total_pages'] = max_page
                            logger.debug(f"âœ… ä»é“¾æ¥ä¸­æ‰¾åˆ°æœ€å¤§é¡µæ•°: {forum_info['total_pages']}")
                            pages_found = True
                    
                    if not pages_found:
                        logger.warning(f"! æœªæ‰¾åˆ°é¡µæ•°ä¿¡æ¯ï¼Œä¿æŒä¸ºNone")
                else:
                    logger.warning(f"! æ— æ³•è·å–æ¿å—é¡µé¢ï¼Œé¡µæ•°ä¿æŒä¸ºNone")
            
            # æ„å»ºæ˜¾ç¤ºæ–‡æœ¬
            topics_display = "æœªçŸ¥" if forum_info['total_topics'] is None else str(forum_info['total_topics'])
            pages_display = "æœªçŸ¥" if forum_info['total_pages'] is None else str(forum_info['total_pages'])
            
            logger.info(f"ğŸ“‹ æ¿å—ä¿¡æ¯ [{forum_info['name']}]: æ€»è®¡{topics_display}ä¸»é¢˜, å…±{pages_display}é¡µ")
            
            return forum_info
        
        except Exception as e:
            logger.error(f"âœ— è·å–æ¿å—ä¿¡æ¯å¤±è´¥: fid={fid}, é”™è¯¯: {e}")
            logger.debug(f" è¯¦ç»†é”™è¯¯ä¿¡æ¯", exc_info=True)
            return None
    
    # ==================== çˆ¬å–åŠŸèƒ½ ====================
    
    def crawler_tid_list(self, url: str) -> List[int]:
        """çˆ¬å–é¡µé¢ä¸­çš„tidåˆ—è¡¨
        
        Args:
            url: æ¿å—é¡µé¢URL
        
        Returns:
            List[int]: TIDåˆ—è¡¨
        """
        # æ·»åŠ é‡è¯•æœºåˆ¶ï¼Œæé«˜æˆåŠŸç‡
        for retry in range(3):
            try:
                html = self.get_original(url)
                if html:
                    # ç»Ÿä¸€ä¼ é€’ bytes ç»™ pqï¼Œè®© lxml è‡ªè¡Œå¤„ç†ç¼–ç å£°æ˜
                    html_bytes = html if isinstance(html, bytes) else html.encode('utf-8')
                    doc = pq(html_bytes)
                    # ä½¿ç”¨ç²¾ç¡®é€‰æ‹©å™¨
                    items = doc("div.n5_htnrys.cl")[1:]  # è·³è¿‡ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆé€šå¸¸æ˜¯æ ‡é¢˜è¡Œï¼‰
                    id_list = []
                    for item in items:
                        pq_item = pq(item)
                        link = pq_item("div a").eq(0).attr('href')  # æå–hrefå±æ€§
                        if link:
                            parsed_url = urlparse(link)
                            query_params = parse_qs(parsed_url.query)  # è§£æä¸ºå­—å…¸ï¼ˆå€¼ä¸ºåˆ—è¡¨ï¼‰
                            tid = query_params.get('tid', [''])[0]
                            if tid and tid.isdigit():
                                id_list.append(int(tid))
                    
                    if id_list:
                        logger.debug(f"æˆåŠŸæå–åˆ° {len(id_list)} ä¸ªtid")
                        return id_list
                    else:
                        logger.warning(f"é¡µé¢æ— æœ‰æ•ˆtidï¼Œé‡è¯• {retry + 1}/3")
                else:
                    logger.warning(f"è·å–é¡µé¢å¤±è´¥ï¼Œé‡è¯• {retry + 1}/3")
            
            except Exception as e:
                logger.warning(f"çˆ¬å–{url}å¤±è´¥ï¼Œé‡è¯• {retry + 1}/3: {e}")
            
            # é‡è¯•å‰çŸ­æš‚å»¶è¿Ÿ
            if retry < 2:
                time.sleep(1)
        
        logger.error(f"è¿ç»­3æ¬¡çˆ¬å–å¤±è´¥: {url}")
        return []
    
    def _fix_mobile_session_and_retry(self, original_url: str) -> Optional[str]:
        """ä¿®å¤ä¼šè¯é—®é¢˜ - æ”¹ç”¨æ¡Œé¢ç‰ˆè®¿é—®ç­–ç•¥
        
        Args:
            original_url: åŸå§‹URL
        
        Returns:
            str or None: ä¿®å¤åçš„HTMLå†…å®¹
        """
        logger.info("ğŸ”§ å¼€å§‹ä¿®å¤ä¼šè¯é—®é¢˜...")
        
        try:
            # 1. å…ˆè®¿é—®æ¡Œé¢ç‰ˆè®ºå›é¦–é¡µ
            desktop_forum_url = "https://sehuatang.org/forum.php"
            logger.info(f"ğŸ–¥ï¸ è®¿é—®æ¡Œé¢ç‰ˆè®ºå›é¦–é¡µ: {desktop_forum_url}")
            
            forum_html = self.get_original(desktop_forum_url)
            if not forum_html:
                logger.error(f"âœ— æ¡Œé¢ç‰ˆè®ºå›é¦–é¡µè®¿é—®å¤±è´¥")
                return None
            
            logger.info(f"âœ“ æ¡Œé¢ç‰ˆè®ºå›é¦–é¡µè®¿é—®æˆåŠŸï¼Œé•¿åº¦: {len(forum_html)}")
            
            # 2. ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œæ¨¡æ‹Ÿç”¨æˆ·æµè§ˆ
            delay = random.uniform(3, 6)
            logger.debug(f"ğŸ˜´ ç­‰å¾… {delay:.1f} ç§’ï¼Œæ¨¡æ‹Ÿç”¨æˆ·æµè§ˆ...")
            time.sleep(delay)
            
            # 3. å°†åŸå§‹URLè½¬æ¢ä¸ºæ¡Œé¢ç‰ˆURLï¼ˆç§»é™¤mobile=2å‚æ•°ï¼‰
            desktop_url = original_url.replace('&mobile=2', '').replace('mobile=2&', '').replace('mobile=2', '')
            
            # å¦‚æœURLä¸­æœ‰backforums=1ï¼Œä¹Ÿç§»é™¤å®ƒï¼Œå› ä¸ºè¿™å¯èƒ½æ˜¯æ‰‹æœºç‰ˆç‰¹æœ‰çš„
            desktop_url = desktop_url.replace('&backforums=1', '').replace('backforums=1&', '').replace('backforums=1', '')
            
            logger.info(f" ä½¿ç”¨æ¡Œé¢ç‰ˆURLé‡æ–°è®¿é—®: {desktop_url}")
            
            retry_html = self.get_original(desktop_url)
            if not retry_html:
                logger.error(f"âœ— æ¡Œé¢ç‰ˆURLè®¿é—®å¤±è´¥")
                return None
            
            # 4. æ£€æŸ¥æ˜¯å¦ä¿®å¤æˆåŠŸ
            if len(retry_html) > 20000:  # æ­£å¸¸é¡µé¢åº”è¯¥æ¯”è¾ƒå¤§
                logger.info(f"âœ“ ä¼šè¯ä¿®å¤æˆåŠŸï¼Œè·å–åˆ°æ­£å¸¸æ¡Œé¢ç‰ˆé¡µé¢")
                return retry_html
            elif "æ‰‹æœºç‰ˆ" in retry_html and "ç°åœ¨å°±ç™»å½•" in retry_html:
                logger.warning(f"! ä»ç„¶æ˜¯æ‰‹æœºç‰ˆå¼•å¯¼é¡µé¢ï¼Œä¿®å¤å¤±è´¥")
                return None
            else:
                logger.info(f"âœ“ ä¼šè¯å¯èƒ½å·²ä¿®å¤ï¼Œè¿”å›æ–°å†…å®¹")
                return retry_html
        
        except Exception as e:
            logger.error(f"âœ— ä¼šè¯ä¿®å¤è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
            return None
    
    def _extract_tids_with_regex(self, html: str, url: str) -> List[int]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–tid - å¤‡ç”¨æ–¹æ³•
        
        Args:
            html: HTMLå†…å®¹
            url: é¡µé¢URLï¼ˆç”¨äºæ—¥å¿—ï¼‰
        
        Returns:
            List[int]: TIDåˆ—è¡¨
        """
        logger.debug("ğŸ”§ ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¤‡ç”¨æ–¹æ³•æå–tid")
        
        try:
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«tidçš„é“¾æ¥
            tid_pattern = r'mod=viewthread&tid=(\d+)'
            matches = re.findall(tid_pattern, html)
            
            id_list = []
            for match in matches:
                try:
                    tid_int = int(match)
                    if tid_int not in id_list:  # å»é‡
                        id_list.append(tid_int)
                except ValueError:
                    continue
            
            logger.info(f"ğŸ“‹ æ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•æå–åˆ° {len(id_list)} ä¸ªtid: {url}")
            return id_list
        
        except Exception as e:
            logger.error(f"âœ— æ­£åˆ™è¡¨è¾¾å¼æå–å¤±è´¥: {e}")
            return []
    
    def crawler_detail(self, url: str) -> Dict:
        """çˆ¬å–å•ä¸ªè¯¦æƒ…é¡µ
        
        Args:
            url: è¯¦æƒ…é¡µURL
        
        Returns:
            Dict: è¯¦æƒ…ä¿¡æ¯ï¼ŒåŒ…å« title, sub_type, publish_date, magnet, preview_images, size
        """
        try:
            html = self.get_original(url)
            if html:
                # ç»Ÿä¸€ä¼ é€’ bytes ç»™ pqï¼Œè®© lxml è‡ªè¡Œå¤„ç†ç¼–ç å£°æ˜
                html_bytes = html if isinstance(html, bytes) else html.encode('utf-8')
                doc = pq(html_bytes)
                
                # ä½¿ç”¨ç²¾ç¡®ç£åŠ›é“¾æ¥æå–æ–¹å¼
                all_text = doc('div.blockcode').text()
                magnet_pattern = r'magnet:\?xt=urn:btih:[0-9a-fA-F]+'
                match = re.search(magnet_pattern, all_text)
                
                magnet = None
                torrent_size = None # æå‰åˆå§‹åŒ–ï¼Œä¸ºäº†åé¢çš„ size è¡¥å…¨é€»è¾‘
                
                if match:
                    magnet = match.group()
                    logger.debug(f"æ‰¾åˆ°ç£åŠ›é“¾æ¥: {url}")
                
                # å¤‡ç”¨torrentå¤„ç†ï¼ˆç”¨äºè·å–ç£åŠ›é“¾æ¥æˆ–å¤§å°ï¼‰
                if not magnet:
                    torrent = doc("a:contains('.torrent')").eq(0)
                    if torrent:
                        torrent_url = torrent.attr('href')
                        logger.debug(f"å°è¯•è§£ætorrentæ–‡ä»¶: {url}")
                        res = self.parse_torrent_get_magnet(url, f"https://sehuatang.org/{torrent_url}")
                        if res:
                            magnet, torrent_size = res
                
                if magnet:
                    # æå–å…¶ä»–ä¿¡æ¯
                    date = extract_exact_datetime(html)
                    size = extract_and_convert_video_size(html)
                    
                    # å¦‚æœ HTML æ²¡æœ‰å¤§å°ä¿¡æ¯ï¼Œä¸”è¿˜æ²¡ä»ç§å­æå–è¿‡ï¼Œå°è¯•ä¸‹è½½ç§å­è·å–
                    if (size is None or size == 0) and torrent_size is None:
                        torrent = doc("a:contains('.torrent')").eq(0)
                        if torrent:
                            torrent_url = torrent.attr('href')
                            logger.info(f"ğŸ’¡ HTMLæ— å¤§å°ä¿¡æ¯ï¼Œå°è¯•ä»ç§å­æ–‡ä»¶æå–: {url}")
                            res = self.parse_torrent_get_magnet(url, f"https://sehuatang.org/{torrent_url}")
                            if res:
                                _, torrent_size = res
                    
                    # ä¼˜å…ˆåˆ©ç”¨ç‰©ç†ç§å­çš„å¤§å°è¡¥å…¨
                    if (size is None or size == 0) and torrent_size:
                        size = torrent_size
                        logger.info(f"ğŸ’¡ æˆåŠŸä»ç§å­ä¸­è¡¥å…¨å¤§å°: {size}MB")
                    
                    # å…œåº•åˆ©ç”¨é™„ä»¶åŒºæ˜æ–‡ä¿¡æ¯è¡¥å…¨
                    if (size is None or size == 0):
                        attachment_text = doc('.attnm, .pattl, .attachlib').text()
                        if attachment_text:
                            alt_size = extract_and_convert_video_size(attachment_text)
                            if alt_size:
                                size = alt_size
                                logger.info(f"ğŸ’¡ [è¯¦æƒ…é¡µ] æˆåŠŸä»é™„ä»¶è¯¦æƒ…åŒºæ¢æµ‹åˆ°å¤§å°: {size}MB")

                    sub_type = extract_bracket_content(html)
                    
                    # æ ‡é¢˜å¤„ç†
                    title = doc('h2.n5_bbsnrbt').text()
                    pattern = r"^\[.*?\]"
                    title = re.sub(pattern, "", title).strip()
                    
                    # é¢„è§ˆå›¾ç‰‡æå– (å¤šç‰ˆæœ¬å…¼å®¹)
                    img_elements = doc('div.message img, td.t_f img, .pcb img, .ignoreattcheck img, .pattl img')
                    img_src_list = []
                    filter_keywords = ['static/image/smiley', 'static/image/common', 'none.gif', 'zoom.png']
                    
                    for img in img_elements.items():
                        src = img.attr('src') or img.attr('file') or img.attr('zoomfile')
                        if src:
                            src = src.strip()
                            if any(k in src for k in filter_keywords): continue
                            if src not in img_src_list:
                                img_src_list.append(src)
                    
                    result = {
                        "title": title,
                        "sub_type": sub_type,
                        "publish_date": date,
                        "magnet": magnet,
                        "preview_images": ",".join(img_src_list),
                        "size": size
                    }
                    
                    logger.debug(f"è§£ææˆåŠŸ: {url}, æ ‡é¢˜: {title[:50]}...")
                    return result
                else:
                    logger.warning(f"æœªæ‰¾åˆ°ç£åŠ›é“¾æ¥: {url}")
                    return {}
            else:
                logger.warning(f"è·å–é¡µé¢å¤±è´¥: {url}")
                return {}
        
        except Exception as e:
            logger.error(f"è§£æè¯¦æƒ…é¡µå¤±è´¥: {url}, é”™è¯¯: {e}")
            return {}
    
    def crawler_details_batch(self, urls: List[str], use_batch_mode: bool = False) -> List[Dict]:
        """æ‰¹é‡çˆ¬å–è¯¦æƒ…é¡µ - å¯é€‰æ‹©æ‰¹é‡æˆ–å•ä¸ªå¤„ç†æ¨¡å¼
        
        Args:
            urls: è¯¦æƒ…é¡µURLåˆ—è¡¨
            use_batch_mode: æ˜¯å¦ä½¿ç”¨æ‰¹é‡æ¨¡å¼ï¼ˆé»˜è®¤Falseï¼Œä½¿ç”¨å•ä¸ªå¤„ç†æ¨¡å¼æ›´ç¨³å®šï¼‰
        
        Returns:
            List[Dict]: è¯¦æƒ…ä¿¡æ¯åˆ—è¡¨
        """
        logger.info(f" å¼€å§‹{'æ‰¹é‡' if use_batch_mode else 'å•ä¸ª'}çˆ¬å– {len(urls)} ä¸ªè¯¦æƒ…é¡µ")
        
        # æ¯æ¬¡æ‰¹é‡çˆ¬å–å¼€å§‹æ—¶é‡ç½®çŠ¶æ€
        self._consecutive_failures = 0
        self._slow_mode = False
        self._error_type_counter = {}
        self._should_stop_crawling = False
        logger.info(f"[CRAWLER] åˆå§‹åŒ–çˆ¬å–çŠ¶æ€ï¼šæ­£å¸¸æ¨¡å¼ï¼Œå»¶è¿Ÿ{self._normal_mode_delay[0]}-{self._normal_mode_delay[1]}ç§’")
        
        try:
            if use_batch_mode:
                # æ‰¹é‡å¤„ç†æ¨¡å¼
                try:
                    # è·å–é…ç½®çš„çº¿ç¨‹æ•°å’Œå»¶è¿Ÿ
                    from configuration import config_manager
                    max_workers = config_manager.get('CRAWLER_THREAD_COUNT', 10)
                    delay_min = config_manager.get('CRAWLER_SYNC_DELAY_MIN', 0.3)
                    delay_max = config_manager.get('CRAWLER_SYNC_DELAY_MAX', 0.8)
                    
                    # åˆ›å»ºæ–°çš„å¤„ç†å™¨å®ä¾‹
                    local_batch_processor = BatchProcessor(
                        batch_size=max_workers,
                        max_workers=max_workers,
                        delay_min=delay_min,
                        delay_max=delay_max
                    )
                    
                    def process_detail_html(url: str, html: str) -> Dict:
                        """å¤„ç†è¯¦æƒ…é¡µHTML"""
                        return self._parse_detail_html_stable(url, html)
                    
                    # æ‰¹é‡å¤„ç†
                    results = local_batch_processor.process_urls_in_batches(
                        urls,
                        process_detail_html,
                        headers=self.headers,
                        cookies=self.cookie,
                        proxies=self.proxies
                    )
                    
                    # å…³é—­å¤„ç†å™¨
                    local_batch_processor.close()
                    
                    # ç¡®ä¿ç»“æœæ•°é‡ä¸è¾“å…¥URLæ•°é‡ä¸€è‡´
                    if len(results) != len(urls):
                        logger.warning(f"ç»“æœæ•°é‡ä¸åŒ¹é…: è¾“å…¥{len(urls)}ä¸ªURLï¼Œè¿”å›{len(results)}ä¸ªç»“æœ")
                        while len(results) < len(urls):
                            results.append(None)
                    
                    # ç»Ÿè®¡æœ‰æ•ˆç»“æœ
                    valid_count = sum(1 for r in results if r and r.get('magnet'))
                    logger.info(f"âœ“ æ‰¹é‡çˆ¬å–å®Œæˆ: æˆåŠŸ {valid_count}/{len(urls)}")
                    
                    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
                    try:
                        stats = local_batch_processor.get_crawler_stats()
                        logger.info(f"[STATS] çˆ¬å–ç»Ÿè®¡: å¹³å‡å“åº”æ—¶é—´ {stats['avg_response_time']:.2f}s, "
                                   f"æˆåŠŸç‡ {stats['successful_requests']}/{stats['total_requests']}")
                    except:
                        pass
                    
                    return results
                
                except Exception as e:
                    logger.error(f"æ‰¹é‡çˆ¬å–å¤±è´¥: {e}")
                    logger.info("é™çº§åˆ°å•ä¸ªå¤„ç†æ¨¡å¼")
                    use_batch_mode = False
            
            if not use_batch_mode:
                # å•ä¸ªå¤„ç†æ¨¡å¼ - æ›´ç¨³å®šå¯é 
                logger.info("ä½¿ç”¨å•ä¸ªå¤„ç†æ¨¡å¼ï¼ˆæ›´ç¨³å®šï¼‰")
                results = []
                success_count = 0
                
                for i, url in enumerate(urls):
                    # æ£€æŸ¥æ§åˆ¶ä¿¡å·ï¼ˆåœæ­¢å’Œæš‚åœï¼‰
                    try:
                        from scheduler.utils import check_stop_and_pause
                        if check_stop_and_pause():
                            logger.info(f"â›” [CRAWLER] æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œå‰©ä½™ {len(urls) - i} ä¸ªURLæœªçˆ¬å–")
                            results.extend([None] * (len(urls) - i))
                            break
                    except Exception as e:
                        logger.debug(f"æ£€æŸ¥æ§åˆ¶ä¿¡å·å¤±è´¥: {e}")
                    
                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢çˆ¬å–ï¼ˆæ—§æ ‡å¿—ï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰
                    if self._should_stop_crawling:
                        logger.error(f"â›” [CRAWLER] æ£€æµ‹åˆ°åœæ­¢æ ‡å¿—ï¼Œå‰©ä½™ {len(urls) - i} ä¸ªURLæœªçˆ¬å–")
                        results.extend([None] * (len(urls) - i))
                        break
                    
                    try:
                        result = self.crawler_detail(url)
                        if result and result.get('magnet'):
                            results.append(result)
                            success_count += 1
                        else:
                            results.append(None)
                        
                        # è¿›åº¦æ—¥å¿—
                        if (i + 1) % 10 == 0 or (i + 1) == len(urls):
                            logger.info(f"å•ä¸ªå¤„ç†è¿›åº¦: {i + 1}/{len(urls)}, æˆåŠŸ: {success_count}")
                            mode_info = "æ…¢é€Ÿæ¨¡å¼" if self._slow_mode else "æ­£å¸¸æ¨¡å¼"
                            logger.info(f"[CRAWLER] å½“å‰çˆ¬å–æ¨¡å¼: {mode_info}ï¼Œè¿ç»­å¤±è´¥æ¬¡æ•°: {self._consecutive_failures}")
                        
                        # ä½¿ç”¨è‡ªé€‚åº”å»¶è¿Ÿ
                        if i < len(urls) - 1:
                            self._adaptive_delay()
                    
                    except Exception as detail_e:
                        logger.warning(f"å•ä¸ªå¤„ç†å¤±è´¥: {url}, é”™è¯¯: {detail_e}")
                        results.append(None)
                
                logger.info(f"âœ“ å•ä¸ªå¤„ç†å®Œæˆ: æˆåŠŸ {success_count}/{len(urls)}")
                return results
        
        finally:
            # çˆ¬å–ç»“æŸæ—¶æ¸…ç†Session
            self._close_session()
            logger.debug("çˆ¬å–å®Œæˆï¼Œå·²æ¸…ç†Session")
    
    def _parse_detail_html_stable(self, url: str, html) -> Dict:
        """è§£æè¯¦æƒ…é¡µHTMLå†…å®¹ - ç¨³å®šè§£æé€»è¾‘
        
        Args:
            url: é¡µé¢URL
            html: HTMLå†…å®¹ï¼ˆå¯ä»¥æ˜¯bytesæˆ–strï¼‰
        
        Returns:
            Dict: è§£æç»“æœ
        """
        try:
            # ç»Ÿä¸€ä¼ é€’ bytes ç»™ pqï¼Œè®© lxml è‡ªè¡Œå¤„ç†ç¼–ç å£°æ˜
            html_bytes = html if isinstance(html, bytes) else html.encode('utf-8')
            doc = pq(html_bytes)
            # åŸºæœ¬æ£€æŸ¥
            if not html or len(html) < 100:
                logger.warning(f"é¡µé¢å†…å®¹è¿‡çŸ­æˆ–ä¸ºç©º: {url}, é•¿åº¦: {len(html) if html else 0}")
                return {}
            
            # æ£€æŸ¥å¸¸è§é”™è¯¯é¡µé¢
            if "æŠ±æ­‰ï¼ŒæŒ‡å®šçš„ä¸»é¢˜ä¸å­˜åœ¨æˆ–å·²è¢«åˆ é™¤æˆ–æ­£åœ¨è¢«å®¡æ ¸" in html:
                logger.warning(f"ä¸»é¢˜ä¸å­˜åœ¨æˆ–å·²åˆ é™¤: {url}")
                return {}
            
            if "æ‚¨æ— æƒè¿›è¡Œå½“å‰æ“ä½œ" in html:
                logger.warning(f"æ— æƒè®¿é—®é¡µé¢: {url}")
            
            # æ£€æŸ¥é¡µé¢æ ‡é¢˜ç¡®è®¤æœ‰æ•ˆæ€§
            page_title = doc('head>title').text()
            valid_keywords = ["98å ‚", "é—¨æˆ·", "forum", "Discuz"]
            if not any(k in page_title for k in valid_keywords):
                logger.warning(f"âš ï¸ [ANTIBOT] é¡µé¢è¢«æ‹¦æˆª(æ ‡é¢˜: {page_title})")
                return {"error_type": "antibot_detected", "error_msg": f"æ‹¦æˆªé¡µ: {page_title}"}
            
            # æå–æ ‡é¢˜ (é€‚é…å…¨ç‰ˆæœ¬)
            title = doc('h2.n5_bbsnrbt').text() or doc('#thread_subject').text() or doc('h1.ts').text()
            if not title:
                title_elem = doc('title').text()
                if " - 98å ‚" in title_elem:
                    title = title_elem.split(" - 98å ‚")[0].strip()

            if not title:
                logger.warning(f"âš ï¸ [ANTIBOT] æœªæ‰¾åˆ°å¸–å­æ ‡é¢˜: {url}")
                return {"error_type": "antibot_detected", "error_msg": "æœªæ‰¾åˆ°æ ‡é¢˜"}
            
            # æ¸…ç†æ ‡é¢˜
            title = re.sub(r"^\[.*?\]", "", title).strip()

            # --- 2. ç£åŠ›é“¾æ¥/ç§å­æŒ–æ˜ (å…¨ç‰ˆæœ¬é€‚é…) ---
            magnet = None
            torrent_size = None # æå‰åˆå§‹åŒ–ï¼Œä¿è¯å˜é‡å®‰å…¨
            
            # æ–¹æ¡ˆA: æœç´¢ç²¾ç¡®ä»£ç å—
            magnet_pattern = r'magnet:\?xt=urn:btih:[0-9a-fA-F]+'
            all_potential_text = doc('div.blockcode, div.message, td.t_f, .pcb').text()
            match = re.search(magnet_pattern, all_potential_text)
            
            if match:
                magnet = match.group()
                logger.debug(f"æ‰¾åˆ°ç£åŠ›é“¾æ¥: {url}")
            else:
                # æ–¹æ¡ˆB: å…¨ç›˜æ·±æ§é™„ä»¶ç§å­ (P1ä¼˜å…ˆ)
                logger.debug(f"æ­£æ–‡æœªè§ç£é“¾ï¼Œå¯åŠ¨å…¨ç›˜é™„ä»¶æ‰«æ...")
                torrent_link = (
                    doc("a:contains('.torrent')").eq(0) or 
                    doc("a[href*='attachment.php'][href*='aid=']").filter(lambda i, e: '.torrent' in pq(e).text().lower()).eq(0) or
                    doc("div.attnm a, div.pattl a, .ignoreattcheck a").filter(lambda i, e: 
                        '.torrent' in pq(e).text().lower() or 'torrent' in (pq(e).attr('href') or '').lower()
                    ).eq(0) or
                    doc("a[href*='.torrent']").eq(0)
                )

                if torrent_link:
                    torrent_url = torrent_link.attr('href')
                    if torrent_url:
                        full_torrent_url = urljoin(url, torrent_url)
                        logger.info(f"ğŸ” æŒ–æ˜åˆ°ç§å­é“¾æ¥: {full_torrent_url}ï¼Œæ­£åœ¨è‡ªåŠ¨è½¬åŒ–...")
                        res = self.parse_torrent_get_magnet(url, full_torrent_url)
                        if res:
                            magnet, torrent_size = res
                
                if not magnet:
                    logger.warning(f"æœªæ‰¾åˆ°å¯ç”¨æ•°æ®: {url}")
            
            if magnet:
                # æå–å…¶ä»–ä¿¡æ¯
                date = extract_exact_datetime(html)
                size = extract_and_convert_video_size(html)
                
                # å¦‚æœ HTML æ²¡æœ‰å¤§å°ä¿¡æ¯ï¼Œä¸”è¿˜æ²¡ä»ç§å­æå–è¿‡ï¼Œä¸»åŠ¨ä¸‹è½½ç§å­è·å–
                if (size is None or size == 0) and torrent_size is None:
                    # å°è¯•æŸ¥æ‰¾ç§å­é™„ä»¶
                    torrent_link = (
                        doc("a:contains('.torrent')").eq(0) or 
                        doc("a[href*='attachment.php'][href*='aid=']").filter(lambda i, e: '.torrent' in pq(e).text().lower()).eq(0) or
                        doc("div.attnm a, div.pattl a, .ignoreattcheck a").filter(lambda i, e: 
                            '.torrent' in pq(e).text().lower() or 'torrent' in (pq(e).attr('href') or '').lower()
                        ).eq(0)
                    )
                    if torrent_link:
                        torrent_url = torrent_link.attr('href')
                        if torrent_url:
                            full_torrent_url = urljoin(url, torrent_url)
                            logger.info(f"ğŸ’¡ [å…¨åŸŸè§£æ] HTMLæ— å¤§å°ä¿¡æ¯ï¼Œå°è¯•ä»ç§å­æ–‡ä»¶æå–: {full_torrent_url}")
                            res = self.parse_torrent_get_magnet(url, full_torrent_url)
                            if res:
                                _, torrent_size = res
                
                # è¡¥å…¨é€»è¾‘1ï¼šç‰©ç†ç§å­æå– (é«˜å¯ä¿¡åº¦)
                if (size is None or size == 0) and torrent_size:
                    size = torrent_size
                    logger.info(f"ğŸ’¡ [å…¨åŸŸè§£æ] æˆåŠŸä»ç§å­ä¸­è¡¥å…¨å¤§å°: {size}MB")
                
                # è¡¥å…¨é€»è¾‘2ï¼šé™„ä»¶è¯¦æƒ…åŒºæ˜æ–‡ (è¾…åŠ©å¯ä¿¡åº¦)
                if (size is None or size == 0):
                    attachment_text = doc('.attnm, .pattl, .attachlib').text()
                    if attachment_text:
                        alt_size = extract_and_convert_video_size(attachment_text)
                        if alt_size:
                            size = alt_size
                            logger.info(f"ğŸ’¡ [å…¨åŸŸè§£æ] æˆåŠŸä»é™„ä»¶è¯¦æƒ…åŒºæ¢æµ‹åˆ°å¤§å°: {size}MB")

                sub_type = extract_bracket_content(html)
                
                # é¢„è§ˆå›¾ç‰‡æå– (å…¨åŸŸæ¢æµ‹)
                img_elements = doc('div.message img, td.t_f img, .pcb img, .ignoreattcheck img, .pattl img')
                img_src_list = []
                filter_keywords = ['static/image/smiley', 'static/image/common', 'none.gif', 'zoom.png']
                
                for img in img_elements.items():
                    # æ¢æµ‹çœŸå®å›¾ç‰‡åœ°å€å±æ€§
                    src = img.attr('src') or img.attr('file') or img.attr('zoomfile')
                    if src:
                        src = src.strip()
                        if any(k in src for k in filter_keywords): continue
                        if src not in img_src_list:
                            img_src_list.append(src)
                
                result = {
                    "title": title,
                    "sub_type": sub_type,
                    "publish_date": date,
                    "magnet": magnet,
                    "preview_images": ",".join(img_src_list),
                    "size": size
                }
                
                logger.debug(f"è§£ææˆåŠŸ: {url}, æ ‡é¢˜: {title[:50]}...")
                return result
            else:
                logger.warning(f"é¡µé¢è§£æå¤±è´¥ï¼Œæ— ç£åŠ›é“¾æ¥: {url}")
                return {}
        
        except Exception as e:
            error_msg = str(e)
            if "Unicode strings with encoding declaration" in error_msg:
                error_type = "encoding_declaration_error"
            elif "ValueError" in str(type(e)):
                error_type = "value_error"
            elif "PyQueryError" in str(type(e)) or "etree" in error_msg:
                error_type = "parse_error"
            else:
                error_type = "unknown_error"
            
            should_stop = self._record_error_type(error_type)
            logger.error(f"è§£æè¯¦æƒ…é¡µå¤±è´¥ ({error_type}): {url}, é”™è¯¯: {e}")
            if not should_stop:
                import traceback
                logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        
        return {}
    
    # ==================== æ•°æ®å¤„ç† ====================
    
    def parse_torrent_get_magnet(self, refer: str, torrent_source: str, is_local: bool = False) -> Optional[tuple]:
        """è§£ætorrentæ–‡ä»¶è·å–ç£åŠ›é“¾æ¥å’Œå¤§å°
        
        Args:
            refer: å¼•ç”¨é¡µé¢URL
            torrent_source: torrentæ–‡ä»¶URLæˆ–æœ¬åœ°è·¯å¾„
            is_local: æ˜¯å¦ä¸ºæœ¬åœ°æ–‡ä»¶
        
        Returns:
            tuple: (magnet_link, size_mb) æˆ– None
        """
        try:
            from curl_cffi import requests
            
            torrent_bin = None
            if is_local:
                with open(torrent_source, "rb") as f:
                    torrent_bin = f.read()
                if len(torrent_bin) == 0:
                    logger.error("é”™è¯¯ï¼šæœ¬åœ° torrent æ–‡ä»¶ä¸ºç©º")
                    return None
            else:
                header = self.headers.copy()
                header['Referer'] = refer
                resp = requests.get(
                    torrent_source,
                    proxies=self.proxies,
                    cookies=self.cookie,
                    headers=header,
                    allow_redirects=True,
                    timeout=10,
                    impersonate="chrome110"
                )
                resp.raise_for_status()
                torrent_bin = resp.content
                
                if torrent_bin.lstrip().startswith(b'<!DOCTYPE') or torrent_bin.lstrip().startswith(b'<html'):
                    html_text = torrent_bin.decode('utf-8', errors='ignore')
                    if "var safeid" in html_text:
                        logger.info(f"ä¸‹è½½ç§å­æ—¶æ£€æµ‹åˆ°å¹´é¾„éªŒè¯ï¼Œè§£æå¹¶é‡è¯•: {torrent_source}")
                        safeid = extract_safeid(torrent_bin)
                        if safeid:
                            self.cookie['_safe'] = safeid
                            header['Cookie'] = f"_safe={safeid}"
                            resp = requests.get(
                                torrent_source,
                                proxies=self.proxies,
                                headers=header,
                                allow_redirects=True,
                                timeout=10,
                                impersonate="chrome110"
                            )
                            resp.raise_for_status()
                            torrent_bin = resp.content
                    elif "æŠ±æ­‰ï¼Œæ‚¨éœ€è¦ç™»å½•" in html_text:
                        logger.warning(f"ç§å­ä¸‹è½½å¤±è´¥ï¼šéœ€è¦ç™»å½• - {torrent_source}")
                        return None

                if len(torrent_bin) < 50:
                    logger.warning(f"è­¦å‘Šï¼šä¸‹è½½å†…å®¹è¿‡å°ï¼ˆ{len(torrent_bin)} å­—èŠ‚ï¼‰ï¼Œéåˆæ³• torrent æ–‡ä»¶")
                    return None
            
            # ä½¿ç”¨bencodepyè§£ætorrentæ–‡ä»¶
            try:
                torrent_dict = bencodepy.decode(torrent_bin)
            except Exception as b_err:
                logger.error(f"Bencodeè§£ç å¤±è´¥: {b_err}")
                return None
                
            info_dict = None
            # å…¼å®¹ bytes å’Œ str key
            for k in [b"info", "info"]:
                if k in torrent_dict:
                    info_dict = torrent_dict[k]
                    break
            
            if not info_dict:
                logger.error("é”™è¯¯ï¼šç§å­ç¼ºå°‘ info æ ¸å¿ƒå­—æ®µ")
                return None
            
            # --- æå–å¤§å° (ç‰©ç†æå–ï¼Œå¢å¼ºè°ƒè¯•) ---
            total_size_bytes = 0
            found_length = False
            
            # è°ƒè¯•ï¼šè¾“å‡º info_dict çš„æ‰€æœ‰é”®
            info_keys = list(info_dict.keys())
            logger.debug(f"ç§å­ info_dict åŒ…å«çš„é”®: {info_keys}")
            
            # å•æ–‡ä»¶ç§å­ï¼šç›´æ¥æœ‰ length å­—æ®µ
            for k in [b"length", "length"]:
                if k in info_dict:
                    total_size_bytes = int(info_dict[k])
                    found_length = True
                    logger.debug(f"âœ… å•æ–‡ä»¶ç§å­ï¼Œlength å­—æ®µå€¼: {total_size_bytes} bytes")
                    break
            
            # å¤šæ–‡ä»¶ç§å­ï¼šfiles åˆ—è¡¨
            if not found_length:
                for k in [b"files", "files"]:
                    if k in info_dict:
                        files_list = info_dict[k]
                        logger.debug(f"âœ… å¤šæ–‡ä»¶ç§å­ï¼Œfiles åˆ—è¡¨åŒ…å« {len(files_list)} ä¸ªæ–‡ä»¶")
                        for idx, f in enumerate(files_list):
                            for fk in [b"length", "length"]:
                                if fk in f:
                                    file_size = int(f[fk])
                                    total_size_bytes += file_size
                                    if idx < 3:  # åªè®°å½•å‰3ä¸ªæ–‡ä»¶ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                                        logger.debug(f"  æ–‡ä»¶ {idx+1}: {file_size} bytes")
                                    break
                        found_length = True
                        break
            
            # è®¡ç®— MB
            if total_size_bytes > 0:
                size_mb = int(total_size_bytes / (1024 * 1024))
                logger.info(f"âœ… ç‰©ç†ç§å­è§£æå®Œæˆï¼Œæ€»å­—èŠ‚: {total_size_bytes}, å¤§å°: {size_mb}MB")
            else:
                size_mb = None
                logger.warning(f"âš ï¸ ç§å­è§£ææˆåŠŸä½†æœªæå–åˆ°å¤§å°ä¿¡æ¯ (total_size_bytes={total_size_bytes})")
            
            # è®¡ç®— Hash å¹¶æå–åç§°
            info_bin = bencodepy.encode(info_dict)
            info_hash_hex = hashlib.sha1(info_bin).hexdigest()
            
            torrent_name = "Unknown_Torrent"
            for k in [b"name", "name"]:
                if k in info_dict:
                    torrent_name = info_dict[k]
                    break
            
            if isinstance(torrent_name, bytes):
                torrent_name = torrent_name.decode("utf-8", errors="ignore")
            
            encoded_name = urlencode({"dn": torrent_name})[3:]
            magnet_link = f"magnet:?xt=urn:btih:{info_hash_hex}&dn={encoded_name}"
            
            logger.debug(f"ç§å­åç§°: {torrent_name[:50]}...")
            return (magnet_link, size_mb)
        
        except Exception as e:
            logger.error(f"è§£ætorrentæ–‡ä»¶å¤±è´¥ï¼š{e}")
            return None
    
    def save_to_db(self, data: Dict, section: str, tid: int, detail_url: str) -> bool:
        """å°†çˆ¬å–çš„æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“"""
        return self._save_to_db(data, tid, section, detail_url)
    
    def _save_to_db(self, data: Dict, tid: int, section: str = None, detail_url: str = None) -> bool:
        """ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“ (åŒ…å«è‡ªåŠ¨é‡è¯•æœºåˆ¶)"""
        try:
            from models import db, Resource
            from utils import retry_on_lock
            
            @retry_on_lock(max_retries=3, initial_delay=0.5)
            def _do_save():
                # éªŒè¯æ•°æ®
                try:
                    from health import validator
                    validation_result = validator._validate_single(tid, detail_url, data)
                    if not validation_result['valid']:
                        logger.warning(f"âŒ ä¿å­˜å‰éªŒè¯å¤±è´¥: tid={tid}, åŸå› : {', '.join(validation_result['reasons'])}")
                except Exception as e:
                    logger.warning(f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
                    pass
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing_resource = Resource.query.filter_by(tid=tid).first()
                if existing_resource:
                    # å¦‚æœå·²å­˜åœ¨ï¼Œæ›´æ–°å¯èƒ½ç¼ºå¤±çš„ä¿¡æ¯ï¼ˆå¦‚ size æˆ– imagesï¼‰
                    modified = False
                    if not existing_resource.size and data.get('size'):
                        existing_resource.size = data.get('size')
                        modified = True
                    if not existing_resource.preview_images and data.get('preview_images'):
                        existing_resource.preview_images = data.get('preview_images')
                        modified = True
                    
                    if modified:
                        db.session.commit()
                        logger.info(f"âœ“ æˆåŠŸæ›´æ–°å­˜é‡èµ„æºä¿¡æ¯: tid={tid}")
                    else:
                        logger.debug(f"èµ„æºå·²å­˜åœ¨ï¼Œè·³è¿‡: tid={tid}")
                    return False
                
                # æ•°æ®æ¸…ç†
                title = data.get('title', '').strip()[:500]
                sub_type = data.get('sub_type', '').strip()[:200] if data.get('sub_type') else None
                publish_date = self._normalize_date(data.get('publish_date', ''))
                
                # åˆ›å»ºæ–°èµ„æº
                resource = Resource(
                    title=title,
                    sub_type=sub_type,
                    publish_date=publish_date,
                    magnet=data.get('magnet'),
                    preview_images=data.get('preview_images'),
                    size=data.get('size'),
                    tid=tid,
                    section=section[:100] if section else None,
                    detail_url=detail_url[:500] if detail_url else None
                )
                
                db.session.add(resource)
                db.session.commit()
                
                # æ¸…ç†ç»Ÿè®¡ç¼“å­˜
                try:
                    from cache_manager import cache_manager, CacheKeys
                    cache_manager.delete(CacheKeys.STATS)
                    cache_manager.delete(CacheKeys.CATEGORIES)
                except: pass
                
                logger.info(f"âœ“ æˆåŠŸä¿å­˜èµ„æº: tid={tid}, title={title[:50]}...")
                return True
            
            return _do_save()
        
        except Exception as e:
            if "database is locked" in str(e).lower(): raise e
            logger.error(f"ä¿å­˜èµ„æºåˆ°æ•°æ®åº“å¤±è´¥: tid={tid}, é”™è¯¯: {e}")
            try:
                from models import db
                db.session.rollback()
            except: pass
            return False
    
    def _normalize_date(self, date_str: str) -> Optional[str]:
        """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼"""
        if not date_str: return None
        if len(date_str) == 10 and date_str.count('-') == 2: return date_str
        return date_str[:20]
    
    def _record_error_type(self, error_type: str) -> bool:
        """è®°å½•é”™è¯¯ç±»å‹å¹¶æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢çˆ¬å–"""
        if self._should_stop_crawling: return True
        self._error_type_counter[error_type] = self._error_type_counter.get(error_type, 0) + 1
        count = self._error_type_counter[error_type]
        if count >= self._error_threshold:
            self._should_stop_crawling = True
            logger.error(f"â›” [CRAWLER] é”™è¯¯ '{error_type}' è¾¾ä¸Šé™ï¼Œåœæ­¢çˆ¬å–")
            return True
        return False
    
    def _adaptive_delay(self):
        """è‡ªé€‚åº”å»¶è¿Ÿ"""
        delay = random.uniform(self._normal_mode_delay[0], self._normal_mode_delay[1])
        time.sleep(delay)

    def _retry_with_delay(self, func, func_name: str, max_attempts: int = 3, delay_seconds: int = 5):
        """å¸¦é‡è¯•æœºåˆ¶çš„å‡½æ•°è°ƒç”¨åŒ…è£…å™¨"""
        result = None
        for attempt in range(1, max_attempts + 1):
            try:
                logger.info(f" [{func_name}] ç¬¬ {attempt}/{max_attempts} æ¬¡å°è¯•")
                result = func()
                if result: return result
            except Exception as e:
                logger.error(f"âœ— [{func_name}] å‡ºé”™: {e}")
            if attempt < max_attempts: time.sleep(delay_seconds)
        return {} if isinstance(result, dict) else None

    def _close_session(self):
        """æ¸…ç†ä¼šè¯"""
        pass
