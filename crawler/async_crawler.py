#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SHT å¼‚æ­¥çˆ¬è™«æ¨¡å— (v1.3.0)
æä¾›é«˜æ€§èƒ½å¹¶å‘çˆ¬å–èƒ½åŠ›ï¼Œä½¿ç”¨ httpx å®ç°å¼‚æ­¥HTTPè¯·æ±‚
"""

from __future__ import annotations  # å¯ç”¨å»¶è¿Ÿç±»å‹æ³¨è§£è¯„ä¼°ï¼Œé¿å…å¾ªç¯å¯¼å…¥

import asyncio
from curl_cffi.requests import AsyncSession
from typing import List, Dict, Any, Optional
from datetime import datetime, timezone
from collections import deque
import logging
import time
import random
from pyquery import PyQuery as pq
import re
from urllib.parse import urlparse, parse_qs

# å¯¼å…¥åŸæœ‰çš„è§£æå‡½æ•°å’ŒSHTç±»
from .sync_crawler import SHT
from .parser import (
    extract_and_convert_video_size,
    extract_safeid,
    extract_exact_datetime,
    extract_bracket_content
)

logger = logging.getLogger(__name__)


class AsyncSHTCrawler:
    """å¼‚æ­¥SHTçˆ¬è™« - ä½¿ç”¨httpxå®ç°é«˜æ€§èƒ½å¹¶å‘çˆ¬å–"""

    def __init__(
        self,
        max_connections: int = 20,
        timeout: float = 30.0,
        headers: Optional[Dict[str, str]] = None,
        cookies: Optional[Dict[str, str]] = None,
        proxy: Optional[str] = None,
        delay_min: float = 0.5,
        delay_max: float = 1.5
    ):
        """
        åˆå§‹åŒ–å¼‚æ­¥çˆ¬è™«

        Args:
            max_connections: æœ€å¤§å¹¶å‘è¿æ¥æ•°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            headers: è‡ªå®šä¹‰è¯·æ±‚å¤´
            cookies: Cookieå­—å…¸
            proxy: ä»£ç†URLï¼ˆå¦‚ "http://proxy.example.com:8080"ï¼‰
            delay_min: æœ€å°éšæœºå»¶è¿Ÿ(ç§’)
            delay_max: æœ€å¤§éšæœºå»¶è¿Ÿ(ç§’)
        """
        self.max_connections = max_connections
        self.timeout = timeout
        self.semaphore = asyncio.Semaphore(max_connections)
        self.client: Optional[AsyncSession] = None
        self.impersonate = "chrome110"  # æµè§ˆå™¨æŒ‡çº¹ä¼ªè£…,ç»•è¿‡ Cloudflare
        
        self.delay_min = delay_min
        self.delay_max = delay_max

        # é»˜è®¤è¯·æ±‚å¤´ï¼ˆä½¿ç”¨iPhone UAä»¥æé«˜æˆåŠŸç‡ï¼‰
        self.headers = headers or {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 18_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.5 Mobile/15E148 Safari/604.1'
        }

        # Cookieé…ç½®ï¼ˆå…³é”®ï¼š_safe cookieç”¨äºç»•è¿‡å¹´é¾„éªŒè¯ï¼‰
        self.cookies = cookies or {'_safe': ''}

        self.proxy = proxy

        # åˆ›å»ºSHTå®ä¾‹ç”¨äºå¤ç”¨è§£æé€»è¾‘
        self._parser = SHT()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_requests': 0,
            'success_count': 0,
            'failed_count': 0,
            'total_time': 0.0,
            'avg_response_time': 0.0
        }

        # v1.3.0: é”™è¯¯æ—¶é—´çª—è®¡æ•°å™¨ - é¿å…æ—¥å¿—åˆ·å±å’Œè¢«åçˆ¬
        self._error_window = {}  # æ ¼å¼: {error_type: [(timestamp, count), ...]}
        self._error_threshold = 15  # æ—¶é—´çª—å†…é”™è¯¯é˜ˆå€¼
        self._time_window_seconds = 300  # æ—¶é—´çª—: 5åˆ†é’Ÿ
        self._should_stop_crawling = False
        
        # Cookieæ›´æ–°é”ï¼ˆé˜²æ­¢å¹¶å‘ä¿®æ”¹å¯¼è‡´ç«äº‰ï¼‰
        self._cookie_lock = asyncio.Lock()
        self._control_lock = asyncio.Lock()

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        # ä½¿ç”¨ curl_cffi çš„ AsyncSession (æ”¯æŒæµè§ˆå™¨æŒ‡çº¹ä¼ªè£…)
        self.client = AsyncSession()

        # è¾“å‡ºé…ç½®ä¿¡æ¯
        safe_cookie = self.cookies.get('_safe', '')
        safe_preview = f"{safe_cookie[:8]}..." if safe_cookie else "æœªè®¾ç½®"
        
        if self.proxy:
            logger.info(f"[ASYNC] å¼‚æ­¥çˆ¬è™«å·²å¯åŠ¨ - æœ€å¤§å¹¶å‘: {self.max_connections}, ä»£ç†: {self.proxy}, ä¼ªè£…: {self.impersonate}, _safe: {safe_preview}")
        else:
            logger.info(f"[ASYNC] å¼‚æ­¥çˆ¬è™«å·²å¯åŠ¨ - æœ€å¤§å¹¶å‘: {self.max_connections}, æ— ä»£ç†, ä¼ªè£…: {self.impersonate}, _safe: {safe_preview}")
        return self

    async def __aexit__(self, *args):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        if self.client:
            try:
                # v1.4.3: [å…³é”®ä¿®å¤] å¿…é¡» await close()ï¼Œå¹¶åŠ å…¥æœ€åè¶…æ—¶ä¿æŠ¤
                # é˜²æ­¢ç”±äºæ®‹ç•™è¿æ¥å¯¼è‡´çš„ Session å…³é—­æŒ‚æ­»
                await asyncio.wait_for(self.client.close(), timeout=5.0)
            except:
                pass

        logger.info(
            f"[ASYNC] å¼‚æ­¥çˆ¬è™«å·²å…³é—­ - "
            f"æ€»è¯·æ±‚: {self.stats['total_requests']}, "
            f"æˆåŠŸ: {self.stats['success_count']}, "
            f"å¤±è´¥: {self.stats['failed_count']}"
        )

    async def _wait_if_paused_async(self, bridge) -> bool:
        logger.info("â¸ï¸ [ASYNC] ä»»åŠ¡å·²æš‚åœï¼Œç­‰å¾…æ¢å¤...")
        while True:
            await asyncio.sleep(0.5)
            action = bridge.check_control_signals()
            current_state = bridge.coordinator.get_current_state()

            if action.action == 'stop':
                self._should_stop_crawling = True
                self._parser._should_stop_crawling = True
                logger.info("â¹ï¸ [ASYNC] æš‚åœæœŸé—´æ”¶åˆ°åœæ­¢ä¿¡å·")
                return True
            elif action.action == 'resume':
                logger.info("â–¶ï¸ [ASYNC] ä»»åŠ¡å·²æ¢å¤")
                return False
            elif not current_state.is_paused:
                if current_state.current_state == 'idle':
                    self._should_stop_crawling = True
                    self._parser._should_stop_crawling = True
                    logger.info("â¹ï¸ [ASYNC] æ£€æµ‹åˆ°çŠ¶æ€å·²å˜ä¸ºidleï¼Œä»»åŠ¡å·²åœæ­¢")
                    return True
                logger.info("â–¶ï¸ [ASYNC] ä»»åŠ¡å·²æ¢å¤ï¼ˆçŠ¶æ€å˜æ›´ï¼‰")
                return False

    async def _maybe_handle_control_signal(self) -> bool:
        if self._should_stop_crawling:
            return True
        try:
            from crawler_control.cc_control_bridge import get_crawler_control_bridge
            bridge = get_crawler_control_bridge()
            pending = bridge.queue_manager.get_pending_signals()
            has_control = any(s.type in ('stop', 'pause', 'resume') and not s.processed for s in pending)
            current_state = bridge.coordinator.get_current_state()
            if not has_control and not current_state.is_paused:
                return False

            async with self._control_lock:
                if self._should_stop_crawling:
                    return True
                action = bridge.check_control_signals()

                if action.action == 'stop':
                    self._should_stop_crawling = True
                    self._parser._should_stop_crawling = True
                    logger.info("â›” [ASYNC] æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢åç»­è¯·æ±‚")
                    return True

                if action.action == 'pause' or current_state.is_paused:
                    should_stop = await self._wait_if_paused_async(bridge)
                    return should_stop
        except Exception as e:
            logger.debug(f"[ASYNC] æ£€æŸ¥æ§åˆ¶ä¿¡å·å¤±è´¥: {e}")
        return False

    async def fetch(self, url: str, max_retries: int = 3, **kwargs) -> Optional[Any]:
        """
        å¼‚æ­¥è·å–å•ä¸ªURL (å¸¦é‡è¯•)

        Args:
            url: ç›®æ ‡URL
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            **kwargs: ä¼ é€’ç»™ curl_cffi çš„é¢å¤–å‚æ•°

        Returns:
            Responseå¯¹è±¡,å¤±è´¥è¿”å›None
        """
        for retry in range(max_retries):
            # ğŸ’¤ éšæœºå»¶è¿Ÿ
            if await self._maybe_handle_control_signal():
                return None
            
            base_delay = random.uniform(self.delay_min, self.delay_max)
            delay = base_delay * (retry + 1) if retry > 0 else base_delay
            await asyncio.sleep(delay)

            if await self._maybe_handle_control_signal():
                return None

            # v1.4.3: [ç›‘æ§] è¿›å…¥ä¿¡å·é‡å‰
            logger.debug(f"[ASYNC] URLæ’é˜Ÿä¸­: {url[:60]}...")
            async with self.semaphore:  # é™æµ
                self.stats['total_requests'] += 1
                logger.debug(f"[ASYNC] ğŸ›« å¼€å§‹è¯·æ±‚: {url[:60]}")
                start_time = time.time()

                try:
                    # æ„å»ºè¯·æ±‚å‚æ•°
                    request_params = {
                        'headers': self.headers,
                        'cookies': self.cookies,
                        'timeout': self.timeout,
                        'allow_redirects': True,
                        'impersonate': self.impersonate,
                    }
                    if self.proxy:
                        request_params['proxies'] = {'http': self.proxy, 'https': self.proxy}
                    request_params.update(kwargs)

                    # v1.4.1: ç¡¬è¶…æ—¶ä¿æŠ¤
                    hard_timeout = self.timeout + 15
                    
                    try:
                        response = await asyncio.wait_for(
                            self.client.get(url, **request_params),
                            timeout=hard_timeout
                        )
                    except asyncio.TimeoutError:
                        raise Exception(f"ç¡¬è¶…æ—¶æ‹¦æˆª (>{hard_timeout}s)")

                    logger.debug(f"[ASYNC] ğŸ›¬ å“åº”åˆ°è¾¾ (Status: {response.status_code}): {url[:60]}")
                    
                    if response.status_code >= 400:
                        raise Exception(f"HTTP {response.status_code}")

                    # ğŸ” æ£€æŸ¥å¹´é¾„éªŒè¯
                    html_text = response.text
                    if "var safeid" in html_text:
                        logger.debug(f"[ASYNC] æ£€æµ‹åˆ°å¹´é¾„éªŒè¯: {url}")
                        from .parser import extract_safeid
                        safeid = extract_safeid(html_text.encode('utf-8'))
                        if safeid:
                            async with self._cookie_lock:
                                self.cookies['_safe'] = safeid
                                if hasattr(self, '_parser') and self._parser:
                                    self._parser.cookie['_safe'] = safeid
                            
                            request_params['cookies'] = self.cookies
                            # äºŒæ¬¡è¯·æ±‚ä¹ŸåŠ å…¥è¶…æ—¶ä¿æŠ¤
                            response = await asyncio.wait_for(
                                self.client.get(url, **request_params),
                                timeout=hard_timeout
                            )
                            if response.status_code >= 400:
                                raise Exception(f"HTTP {response.status_code} (Verified)")

                    elapsed = time.time() - start_time
                    self.stats['success_count'] += 1
                    # ...
                    logger.debug(f"[ASYNC] âœ“ {url} - {elapsed:.2f}s")
                    return response

                except Exception as e:
                    if retry < max_retries - 1:
                        logger.warning(f"[ASYNC] é‡è¯• {retry + 1}/{max_retries}: {url} - {e}")
                        continue
                    else:
                        self.stats['failed_count'] += 1
                        logger.error(f"[ASYNC] âœ— {url} æœ€ç»ˆå¤±è´¥: {e}")
                        return None

    async def fetch_batch(
        self,
        urls: List[str],
        max_retries: int = 3,
        **kwargs
    ) -> List[Optional[Any]]:
        """
        å¹¶å‘è·å–å¤šä¸ªURL (å¸¦é‡è¯•)

        Args:
            urls: URLåˆ—è¡¨
            max_retries: æ¯ä¸ªURLçš„æœ€å¤§é‡è¯•æ¬¡æ•°
            **kwargs: ä¼ é€’ç»™ curl_cffi çš„é¢å¤–å‚æ•°

        Returns:
            Responseå¯¹è±¡åˆ—è¡¨ï¼ˆå¤±è´¥çš„ä¸ºNoneï¼‰
        """
        logger.info(f"[ASYNC] å¼€å§‹æ‰¹é‡è·å– {len(urls)} ä¸ªURL")

        if await self._maybe_handle_control_signal():
            return [None] * len(urls)

        tasks = [self.fetch(url, max_retries=max_retries, **kwargs) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=False)

        success_count = sum(1 for r in results if r is not None)
        logger.info(
            f"[ASYNC] æ‰¹é‡è·å–å®Œæˆ - "
            f"æˆåŠŸ: {success_count}/{len(urls)}"
        )

        return results

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()

    def _record_error_type(self, error_type: str) -> bool:
        """
        è®°å½•é”™è¯¯ç±»å‹ï¼ˆæ—¶é—´çª—è®¡æ•°ï¼‰å¹¶æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢çˆ¬å–

        Args:
            error_type: é”™è¯¯ç±»å‹æ ‡è¯†

        Returns:
            True å¦‚æœåº”è¯¥åœæ­¢çˆ¬å–ï¼ŒFalse ç»§ç»­
        """
        if self._should_stop_crawling:
            return True

        # è·å–å½“å‰æ—¶é—´
        current_time = time.time()

        # åˆå§‹åŒ–è¯¥é”™è¯¯ç±»å‹çš„æ—¶é—´çª—é˜Ÿåˆ—
        if error_type not in self._error_window:
            self._error_window[error_type] = deque(maxlen=100)

        # è®°å½•é”™è¯¯æ—¶é—´
        self._error_window[error_type].append(current_time)

        # æ¸…ç†æ—¶é—´çª—å¤–çš„æ—§é”™è¯¯
        window_start = current_time - self._time_window_seconds
        while self._error_window[error_type] and self._error_window[error_type][0] < window_start:
            self._error_window[error_type].popleft()

        # è·å–æ—¶é—´çª—å†…çš„é”™è¯¯è®¡æ•°
        error_count = len(self._error_window[error_type])

        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é˜ˆå€¼
        if error_count >= self._error_threshold:
            self._should_stop_crawling = True
            logger.error(f"â›” [ASYNC] é”™è¯¯ç±»å‹ '{error_type}' åœ¨{self._time_window_seconds}ç§’å†…å·²å‡ºç° {error_count} æ¬¡ï¼Œè¶…è¿‡é˜ˆå€¼ {self._error_threshold}ï¼Œåœæ­¢çˆ¬å–")
            logger.error(f"âš ï¸ [ASYNC] å¯èƒ½é‡åˆ°åçˆ¬æˆ–æœåŠ¡å¼‚å¸¸ï¼Œé¿å…ç»§ç»­è¯·æ±‚")
            return True
        elif error_count % 5 == 0:
            logger.warning(f"âš ï¸ [ASYNC] é”™è¯¯ç±»å‹ '{error_type}' å·²å‡ºç° {error_count} æ¬¡ï¼ˆæ—¶é—´çª—å†…ï¼‰")

        return False

    def _parse_detail_html(self, url: str, html: str) -> Optional[Dict[str, Any]]:
        """
        è§£æè¯¦æƒ…é¡µHTMLï¼ˆå¤ç”¨åŸæœ‰çš„ç¨³å®šè§£æé€»è¾‘ï¼‰

        Args:
            url: è¯¦æƒ…é¡µURL
            html: HTMLå†…å®¹

        Returns:
            è§£æåçš„èµ„æºæ•°æ®å­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # å¤ç”¨åŸæœ‰çš„ç¨³å®šè§£æé€»è¾‘ï¼Œç¡®ä¿æ•°æ®æ ¼å¼ä¸€è‡´
            result = self._parser._parse_detail_html_stable(url, html)

            if result:
                logger.debug(f"[ASYNC] è§£ææˆåŠŸ: {url}")
            else:
                logger.debug(f"[ASYNC] è§£æè¿”å›ç©ºç»“æœ: {url}")

            return result

        except Exception as e:
            logger.error(f"[ASYNC] è§£æè¯¦æƒ…é¡µå¤±è´¥: {url}, é”™è¯¯: {e}")
            return None

    async def crawl_detail_page(self, url: str) -> Optional[Dict[str, Any]]:
        """
        å¼‚æ­¥çˆ¬å–å•ä¸ªè¯¦æƒ…é¡µ

        Args:
            url: è¯¦æƒ…é¡µURL

        Returns:
            è§£æåçš„èµ„æºæ•°æ®å­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        response = await self.fetch(url)
        if not response:
            return None

        return self._parse_detail_html(url, response.text)

    async def crawl_details_batch(self, urls: List[str]) -> List[Optional[Dict[str, Any]]]:
        """
        å¼‚æ­¥æ‰¹é‡çˆ¬å–å¤šä¸ªè¯¦æƒ…é¡µ

        Args:
            urls: è¯¦æƒ…é¡µURLåˆ—è¡¨

        Returns:
            è§£æåçš„èµ„æºæ•°æ®åˆ—è¡¨
        """
        logger.info(f"[ASYNC] å¼€å§‹æ‰¹é‡çˆ¬å– {len(urls)} ä¸ªè¯¦æƒ…é¡µ")

        if await self._maybe_handle_control_signal():
            return [None] * len(urls)

        # å¹¶å‘è·å–æ‰€æœ‰é¡µé¢
        responses = await self.fetch_batch(urls)

        # è§£ææ‰€æœ‰å“åº”
        results = []
        for i, (url, response) in enumerate(zip(urls, responses)):
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢è§£æï¼ˆä½¿ç”¨ SHT å®ä¾‹çš„åœæ­¢æ ‡å¿—ï¼‰
            if self._parser._should_stop_crawling and i > 0:
                logger.error(f"â›” [ASYNC] æ£€æµ‹åˆ°åœæ­¢æ ‡å¿—ï¼Œå‰©ä½™ {len(urls) - i} ä¸ªå“åº”æœªè§£æ")
                # å¡«å……å‰©ä½™çš„None
                results.extend([None] * (len(urls) - i))
                break

            if response:
                data = self._parse_detail_html(url, response.text)
                results.append(data)
            else:
                results.append(None)

        success_count = sum(1 for r in results if r is not None)
        logger.info(
            f"[ASYNC] è¯¦æƒ…é¡µæ‰¹é‡çˆ¬å–å®Œæˆ - "
            f"æˆåŠŸ: {success_count}/{len(urls)}"
        )

        return results

    def _parse_tid_list(self, html: str) -> List[int]:
        """è§£æTIDåˆ—è¡¨"""
        try:
            # ç»Ÿä¸€è½¬æ¢ä¸º bytes ç»™ pq
            html_bytes = html if isinstance(html, bytes) else html.encode('utf-8')
            doc = pq(html_bytes)
            # ä½¿ç”¨ç²¾ç¡®é€‰æ‹©å™¨ (åŒæ­¥ç‰ˆé€»è¾‘)
            items = doc("div.n5_htnrys.cl")[1:]  # è·³è¿‡ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆé€šå¸¸æ˜¯æ ‡é¢˜è¡Œï¼‰
            id_list = []
            for item in items:
                pq_item = pq(item)
                link = pq_item("div a").eq(0).attr('href')
                if link:
                    parsed_url = urlparse(link)
                    query_params = parse_qs(parsed_url.query)
                    tid = query_params.get('tid', [''])[0]
                    if tid and tid.isdigit():
                        id_list.append(int(tid))
            return id_list
        except Exception as e:
            logger.debug(f"[ASYNC] è§£æTIDåˆ—è¡¨å¤±è´¥: {e}")
            return []

    async def crawl_tid_list(self, url: str) -> List[int]:
        """å¼‚æ­¥çˆ¬å–å•ä¸ªTIDåˆ—è¡¨é¡µ"""
        response = await self.fetch(url)
        if not response:
            return []
        return self._parse_tid_list(response.text)

    async def crawl_tids_batch(self, urls: List[str]) -> List[List[int]]:
        """å¼‚æ­¥æ‰¹é‡çˆ¬å–å¤šä¸ªTIDåˆ—è¡¨é¡µ"""
        logger.info(f"[ASYNC] å¼€å§‹æ‰¹é‡è·å– {len(urls)} ä¸ªTIDåˆ—è¡¨é¡µ")
        
        if await self._maybe_handle_control_signal():
            return [[]] * len(urls)

        responses = await self.fetch_batch(urls)
        
        results = []
        for response in responses:
            if response:
                results.append(self._parse_tid_list(response.text))
            else:
                results.append([])
        
        total_found = sum(len(r) for r in results)
        logger.info(f"[ASYNC] æ‰¹é‡TIDè·å–å®Œæˆ - å…±ä» {len(urls)} é¡µä¸­å‘ç° {total_found} ä¸ªTID")
        return results
