#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡å¤„ç†å™¨æ¨¡å—
å°†å¤§é‡ URL åˆ†æ‰¹å¤„ç†ï¼Œä½¿ç”¨ FastCrawler è¿›è¡Œé«˜æ•ˆçˆ¬å–
"""

import time
import logging
import concurrent.futures
from typing import List, Dict, Any, Optional, Callable
from .fast_crawler import FastCrawler

logger = logging.getLogger(__name__)


class BatchProcessor:
    """
    æ‰¹é‡å¤„ç†å™¨ - åˆ†æ‰¹å¤„ç†å¤§é‡ URL
    
    å°†å¤§é‡ URL åˆ†æˆå°æ‰¹æ¬¡ï¼Œä½¿ç”¨ FastCrawler è¿›è¡Œå¹¶å‘çˆ¬å–ï¼Œ
    ç„¶åå¯¹æ¯ä¸ªç»“æœåº”ç”¨å¤„ç†å‡½æ•°
    """
    
    def __init__(
        self,
        batch_size: int = 10,
        max_workers: int = 5,
        delay_min: float = 0.3,
        delay_max: float = 0.8,
        batch_timeout: int = 300  # å•æ‰¹æ¬¡è¶…æ—¶æ—¶é—´(ç§’),é»˜è®¤5åˆ†é’Ÿ
    ):
        """
        åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨

        Args:
            batch_size: æ¯æ‰¹å¤„ç†çš„ URL æ•°é‡
            max_workers: FastCrawler çš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            delay_min: æœ€å°å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            delay_max: æœ€å¤§å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            batch_timeout: å•æ‰¹æ¬¡æœ€å¤§è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.batch_timeout = batch_timeout
        
        # åˆ›å»º FastCrawler å®ä¾‹
        self.crawler = FastCrawler(
            max_workers=max_workers,
            delay_min=delay_min,
            delay_max=delay_max
        )
    
    def process_urls_in_batches(
        self,
        urls: List[str],
        process_func: Callable[[str, str], Any],
        headers: Optional[Dict] = None,
        cookies: Optional[Dict] = None,
        proxies: Optional[Dict] = None
    ) -> List[Any]:
        """
        åˆ†æ‰¹å¤„ç† URL åˆ—è¡¨
        
        Args:
            urls: URL åˆ—è¡¨
            process_func: å¤„ç†å‡½æ•°ï¼Œæ¥æ”¶ (url, html) è¿”å›å¤„ç†ç»“æœ
            headers: è¯·æ±‚å¤´
            cookies: Cookie å­—å…¸
            proxies: ä»£ç†é…ç½®
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨ï¼Œä¸è¾“å…¥ URL é¡ºåºä¸€è‡´
        """
        logger.info(
            f"[BATCH_PROCESSOR] å¼€å§‹åˆ†æ‰¹å¤„ç† {len(urls)} ä¸ªURLï¼Œ"
            f"æ‰¹æ¬¡å¤§å°: {self.batch_size}"
        )
        
        all_results = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(urls), self.batch_size):
            batch_urls = urls[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (len(urls) + self.batch_size - 1) // self.batch_size

            logger.info(
                f"ğŸ•·ï¸ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}: "
                f"{len(batch_urls)} ä¸ªURL"
            )

            # æ‰¹é‡è·å– HTML å†…å®¹ï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
            batch_start_time = time.time()
            try:
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(
                        self.crawler.fetch_urls_batch,
                        batch_urls,
                        headers,
                        cookies,
                        proxies
                    )
                    try:
                        html_contents = future.result(timeout=self.batch_timeout)
                    except concurrent.futures.TimeoutError:
                        logger.error(f"â±ï¸ æ‰¹æ¬¡ {batch_num} å¤„ç†è¶…æ—¶")
                        html_contents = [None] * len(batch_urls)
            except Exception as e:
                logger.error(f"âš ï¸ æ‰¹æ¬¡ {batch_num} å¤„ç†å¼‚å¸¸: {e}")
                html_contents = [None] * len(batch_urls)
            
            # å¯¹æ¯ä¸ªç»“æœåº”ç”¨å¤„ç†å‡½æ•°
            batch_results = []
            for url, html in zip(batch_urls, html_contents):
                if html:
                    try:
                        result = process_func(url, html)
                        batch_results.append(result)
                    except Exception as e:
                        logger.warning(
                            f"! [BATCH_PROCESSOR] å¤„ç†URLå¤±è´¥: {url}, é”™è¯¯: {e}"
                        )
                        batch_results.append(None)
                else:
                    batch_results.append(None)
            
            all_results.extend(batch_results)
            
            # æ‰¹æ¬¡é—´çŸ­æš‚å»¶è¿Ÿ
            if batch_num < total_batches:
                time.sleep(0.1)
        
        # ç»Ÿè®¡æˆåŠŸæ•°é‡
        success_count = sum(1 for r in all_results if r is not None)
        logger.info(
            f"âœ“ [BATCH_PROCESSOR] åˆ†æ‰¹å¤„ç†å®Œæˆ: "
            f"æˆåŠŸ {success_count}/{len(urls)}"
        )
        
        return all_results
    
    def get_crawler_stats(self) -> Dict[str, Any]:
        """è·å–åº•å±‚ FastCrawler çš„ç»Ÿè®¡ä¿¡æ¯"""
        return self.crawler.get_stats()
    
    def close(self) -> None:
        """å…³é—­å¤„ç†å™¨ï¼Œæ¸…ç†èµ„æº"""
        self.crawler.close()
