#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SHT çˆ¬è™«æ ¸å¿ƒå¼•æ“ - è´Ÿè´£ç½‘ç«™é¡µé¢çš„è§£æã€æ•°æ®æå–åŠ FastCrawler é«˜æ€§èƒ½å¹¶å‘çˆ¬å–å®ç°
é›†æˆäº†å¤šç§é˜²å±è”½ç­–ç•¥å’Œç²¾ç»†çš„è§£æé€»è¾‘ï¼Œæ˜¯ç³»ç»Ÿæ•°æ®è·å–çš„åŸºç¡€å±‚
"""

import os
from urllib.parse import urlparse, parse_qs, urlencode, quote
from curl_cffi import requests
from pyquery import PyQuery as pq
import re
import random
from datetime import datetime, timedelta
import bencodepy
import hashlib
import binascii
from typing import List, Dict, Any, Optional

import logging
from configuration import Config
from models import Resource, Category, db
from utils import retry_on_lock
from health import validator, validate_batch_results

# ----------------- FastCrawler Integration -----------------
from concurrent.futures import ThreadPoolExecutor
from queue import Queue
from curl_cffi.requests import Session
import time
import threading

class FastCrawler:
    """é«˜æ€§èƒ½çˆ¬å–å™¨ - å…¼å®¹curl_cffi"""
    
    def __init__(self, max_workers: int = 10, max_connections: int = 20, 
                 delay_min: float = 0.3, delay_max: float = 0.8):
        self.max_workers = max_workers
        self.max_connections = max_connections
        self.delay_min = delay_min
        self.delay_max = delay_max
        
        # åˆ›å»ºä¼šè¯æ± 
        self.session_pool = Queue(maxsize=max_connections)
        self._init_session_pool()
        
        # çº¿ç¨‹æ± 
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_time': 0,
            'avg_response_time': 0
        }
    
    def _init_session_pool(self):
        """åˆå§‹åŒ–ä¼šè¯æ± """
        for _ in range(self.max_connections):
            session = Session()
            session.timeout = 30
            self.session_pool.put(session)
    
    def get_session(self) -> Session:
        return self.session_pool.get()
    
    def return_session(self, session: Session):
        self.session_pool.put(session)
    
    def fetch_url(self, url: str, headers: Dict = None, cookies: Dict = None, 
                  proxies: Dict = None) -> Optional[str]:
        """è·å–å•ä¸ªURLå†…å®¹"""
        # ğŸ’¤ éšæœºå»¶è¿Ÿ: æ¨¡æ‹ŸçœŸäººæ“ä½œï¼Œå¯¹é½å¼‚æ­¥æ¨¡å¼çš„é˜²å°é€»è¾‘
        if self.delay_max > 0:
            delay = random.uniform(self.delay_min, self.delay_max)
            time.sleep(delay)

        session = None
        start_time = time.time()
        
        try:
            session = self.get_session()
            
            request_kwargs = {
                'timeout': 30,
                'allow_redirects': True,
                'impersonate': 'chrome110'
            }
            
            if headers: request_kwargs['headers'] = headers
            if cookies: request_kwargs['cookies'] = cookies
            if proxies: request_kwargs['proxies'] = proxies
            
            self.stats['total_requests'] += 1
            
            response = session.get(url, **request_kwargs)
            response.raise_for_status()
            
            self.stats['successful_requests'] += 1
            
            response_time = time.time() - start_time
            self.stats['total_time'] += response_time
            self.stats['avg_response_time'] = self.stats['total_time'] / self.stats['successful_requests']
            
            return response.text
            
        except Exception as e:
            self.stats['failed_requests'] += 1
            logger.warning(f"! [API] è·å–URLå¤±è´¥: {url}, é”™è¯¯: {e}")
            return None
        finally:
            if session:
                self.return_session(session)
    
    def fetch_urls_batch(self, urls: List[str], headers: Dict = None, 
                        cookies: Dict = None, proxies: Dict = None) -> List[Optional[str]]:
        """æ‰¹é‡è·å–URLå†…å®¹ - ä¿æŒé¡ºåº"""
        logger.info(f"[CRAWLER] å¼€å§‹æ‰¹é‡è·å– {len(urls)}ä¸ªURL")
        
        futures_with_index = []
        for i, url in enumerate(urls):
            future = self.executor.submit(
                self.fetch_url, url, headers, cookies, proxies
            )
            futures_with_index.append((i, future))
        
        results = [None] * len(urls)
        completed = 0
        
        for index, future in futures_with_index:
            try:
                result = future.result(timeout=30)
                results[index] = result
                completed += 1
                if completed % 10 == 0 or completed == len(urls):
                    logger.info(f"[CRAWLER] æ‰¹é‡è·å–è¿›åº¦: {completed}/{len(urls)}")
            except Exception as e:
                logger.warning(f"! [CRAWLER] æ‰¹é‡è·å–ä»»åŠ¡å¤±è´¥ (ç´¢å¼•{index}): {e}")
                results[index] = None
        
        return results
    
    def get_stats(self) -> Dict[str, Any]:
        return self.stats.copy()
    
    def close(self):
        self.executor.shutdown(wait=True)
        while not self.session_pool.empty():
            session = self.session_pool.get()
            session.close()

class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self, batch_size: int = 10, max_workers: int = 5, 
                 delay_min: float = 0.3, delay_max: float = 0.8):
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.crawler = FastCrawler(
            max_workers=max_workers, 
            delay_min=delay_min, 
            delay_max=delay_max
        )
    
    def process_urls_in_batches(self, urls: List[str], process_func, 
                               headers: Dict = None, cookies: Dict = None, 
                               proxies: Dict = None) -> List[Any]:
        """åˆ†æ‰¹å¤„ç†URLåˆ—è¡¨"""
        logger.info(f"[CRAWLER] å¼€å§‹åˆ†æ‰¹å¤„ç† {len(urls)}ä¸ªURLï¼Œæ‰¹æ¬¡å¤§å°: {self.batch_size}")
        all_results = []
        
        for i in range(0, len(urls), self.batch_size):
            batch_urls = urls[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (len(urls) + self.batch_size - 1) // self.batch_size
            
            logger.info(f" ğŸ•·ï¸ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}: {len(batch_urls)}ä¸ªURL")
            
            html_contents = self.crawler.fetch_urls_batch(
                batch_urls, headers, cookies, proxies
            )
            
            batch_results = []
            for url, html in zip(batch_urls, html_contents):
                if html:
                    try:
                        result = process_func(url, html)
                        batch_results.append(result)
                    except Exception as e:
                        logger.warning(f"! [CRAWLER] å¤„ç†URLå¤±è´¥: {url}, é”™è¯¯: {e}")
                        batch_results.append(None)
                else:
                    batch_results.append(None)
            
            all_results.extend(batch_results)
            
            if batch_num < total_batches:
                time.sleep(0.1)
        
        success_count = sum(1 for r in all_results if r is not None)
        logger.info(f"âœ“ [CRAWLER] åˆ†æ‰¹å¤„ç†å®Œæˆ: æˆåŠŸ {success_count}/{len(urls)}")
        return all_results
    
    def get_crawler_stats(self) -> Dict[str, Any]:
        return self.crawler.get_stats()
    
    def close(self):
        self.crawler.close()

# -----------------------------------------------------------

# è·å–æ—¥å¿—è®°å½•å™¨ - ä¸å†é‡å¤é…ç½®basicConfig
logger = logging.getLogger(__name__)


def extract_and_convert_video_size(html_content):
    doc = pq(html_content)
    message_text = doc('.message').text()
    clean_text = re.sub(r'\s+', ' ', message_text).strip()
    pattern = r"(\d+\.?\d*)([GM])"
    match = re.search(pattern, clean_text)
    if not match:
        return None
    size_num_str, unit = match.groups()
    try:
        size_num = float(size_num_str)
    except ValueError:
        return None

    if unit.upper() == 'G':
        mb_size = size_num * 1024
    elif unit.upper() == 'M':
        mb_size = size_num
    else:
        return None
    return int(mb_size)


def extract_safeid(html_content):
    doc = pq(html_content)
    for script_elem in doc('script'):
        script_text = pq(script_elem).text().strip()
        if not script_text or 'safeid' not in script_text:
            continue
        match = re.search(r"safeid\s*=\s*['\"]([^'\"]+)['\"]", script_text)
        if match:
            return match.group(1)
    return None


def extract_exact_datetime(html_content):
    doc = pq(html_content)
    date_text = doc('dt.z.cl').eq(0).text().strip()
    if not date_text:
        return ""
    processed_text = date_text.replace('&nbsp;', ' ').strip()
    processed_text = re.sub(r'\s+', ' ', processed_text)
    today = datetime.now().date()
    if re.match(r'^\d+ å°æ—¶å‰$', processed_text):
        return today.strftime('%Y-%m-%d')
    elif processed_text.startswith('åŠå°æ—¶å‰'):
        return today.strftime('%Y-%m-%d')
    elif re.match(r'^\d+ åˆ†é’Ÿå‰$', processed_text):
        return today.strftime('%Y-%m-%d')
    elif re.match(r'^\d+ ç§’å‰$', processed_text):
        return today.strftime('%Y-%m-%d')
    elif processed_text.startswith('æ˜¨å¤© '):
        yesterday = today - timedelta(days=1)
        return yesterday.strftime('%Y-%m-%d')
    elif processed_text.startswith('å‰å¤© '):
        day_before_yesterday = today - timedelta(days=2)
        return day_before_yesterday.strftime('%Y-%m-%d')
    elif re.match(r'^\d+ å¤©å‰$', processed_text):
        days = int(re.search(r'(\d+) å¤©å‰', processed_text).group(1))
        target_date = today - timedelta(days=days)
        return target_date.strftime('%Y-%m-%d')
    elif re.match(r'^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}$', processed_text):
        pure_date_str = processed_text.split(' ')[0]
        return pure_date_str
    else:
        logger.warning(f"! [CRAWLER] æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_text}")
        return None


def extract_bracket_content(html_content):
    doc = pq(html_content)
    h2_text = doc('h2.n5_bbsnrbt').text()
    clean_text = h2_text.strip()
    pattern = r"\[(.*?)\]"
    match = re.search(pattern, clean_text)

    if match:
        return match.group(1)
    else:
        return None




class SHT:
    proxy: str = None
    proxies = {}
    headers = {}
    cookie = {}
    bypass = None
    flare_solver = None

    def __init__(self):
        # ä½¿ç”¨åŸç‰ˆç»è¿‡éªŒè¯çš„iPhone User-Agentï¼ŒæˆåŠŸç‡æ›´é«˜
        ua = "Mozilla/5.0 (iPhone; CPU iPhone OS 18_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.5 Mobile/15E148 Safari/604.1"
        self.headers = {
            'User-Agent': ua
        }
        # ç®€åŒ–cookieç®¡ç†ï¼Œåªç»´æŠ¤å¿…è¦çš„çŠ¶æ€
        self.cookie = {
            '_safe': ''
        }

        # æ·»åŠ å®ä¾‹çº§åˆ«çš„æ¿å—ä¿¡æ¯ç¼“å­˜ï¼Œé¿å…é‡å¤ç½‘ç»œè¯·æ±‚
        self._forums_cache = None
        self._forums_cache_time = 0
        self._cache_duration = 300  # 5åˆ†é’Ÿç¼“å­˜

        # ä»£ç†é…ç½®ä¼˜å…ˆçº§ï¼šç¯å¢ƒå˜é‡ > é…ç½®æ–‡ä»¶
        env_proxy = os.environ.get("PROXY")
        config_proxy = getattr(Config, 'PROXY', None) if hasattr(Config, 'PROXY') else None

        proxy_val = env_proxy or config_proxy
        self.proxies = {"http": proxy_val, "https": proxy_val} if proxy_val else {}

        # è¾“å‡ºä»£ç†é…ç½®ç”¨äºè°ƒè¯•
        if self.proxies:
            logger.info(f"[NET] ä½¿ç”¨ä»£ç†: {proxy_val}")
        else:
            logger.warning(f"! æœªé…ç½®ä»£ç†ï¼Œå¯èƒ½é‡åˆ°è®¿é—®é—®é¢˜")

        self.bypass = os.environ.get("BYPASS_URL") or getattr(Config, 'BYPASS_URL', None)
        self.flare_solver = os.environ.get("FLARE_SOLVERR_URL") or getattr(Config, 'FLARE_SOLVERR_URL', None)

        # Sessionç®¡ç† - ä¿æŒè¿æ¥å’ŒcookieæŒä¹…åŒ–
        self._session = None

        # åçˆ¬æ£€æµ‹å’Œè‡ªé€‚åº”å»¶è¿Ÿæœºåˆ¶
        self._consecutive_failures = 0  # è¿ç»­å¤±è´¥è®¡æ•°
        self._slow_mode = False  # æ…¢é€Ÿæ¨¡å¼æ ‡å¿—
        self._failure_threshold = 3  # è¿ç»­å¤±è´¥é˜ˆå€¼ï¼ˆ3æ¬¡åé™çº§ï¼‰
        self._slow_mode_delay = (1.0, 3.0)  # æ…¢é€Ÿæ¨¡å¼å»¶è¿ŸèŒƒå›´ï¼ˆç§’ï¼‰
        self._normal_mode_delay = (0.3, 0.8)  # æ­£å¸¸æ¨¡å¼å»¶è¿ŸèŒƒå›´ï¼ˆç§’ï¼‰

        # é”™è¯¯ç±»å‹è®¡æ•°å™¨ - é¿å…æ—¥å¿—åˆ·å±å’Œè¢«åçˆ¬
        self._error_type_counter = {}  # è®°å½•å„ç±»é”™è¯¯çš„å‡ºç°æ¬¡æ•°
        self._error_threshold = 15  # ç›¸åŒé”™è¯¯ç±»å‹çš„é˜ˆå€¼
        self._should_stop_crawling = False  # åœæ­¢çˆ¬å–æ ‡å¿—

    def _get_session(self):
        """è·å–æˆ–åˆ›å»ºSessionå®ä¾‹ - ä¿æŒè¿æ¥å’ŒcookieæŒä¹…åŒ–"""
        if self._session is None:
            from curl_cffi.requests import Session as CurlSession
            self._session = CurlSession()
            # è®¾ç½®åŸºæœ¬å‚æ•°
            self._session.timeout = 10
            logger.debug("åˆ›å»ºæ–°çš„Sessionå®ä¾‹")
        return self._session

    def _close_session(self):
        """å…³é—­Sessionå®ä¾‹"""
        if self._session:
            try:
                self._session.close()
                logger.debug("Sessionå·²å…³é—­")
            except:
                pass
            self._session = None

    def _record_failure(self):
        """è®°å½•å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦é™çº§åˆ°æ…¢é€Ÿæ¨¡å¼"""
        self._consecutive_failures += 1
        if not self._slow_mode and self._consecutive_failures >= self._failure_threshold:
            self._slow_mode = True
            logger.warning(f"! æ£€æµ‹åˆ°è¿ç»­{self._consecutive_failures}æ¬¡å¤±è´¥ï¼Œé™çº§åˆ°æ…¢é€Ÿæ¨¡å¼ï¼ˆå»¶è¿Ÿ{self._slow_mode_delay[0]}-{self._slow_mode_delay[1]}ç§’ï¼‰")

    def _record_error_type(self, error_type: str) -> bool:
        """
        è®°å½•é”™è¯¯ç±»å‹å¹¶æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢çˆ¬å–

        Args:
            error_type: é”™è¯¯ç±»å‹æ ‡è¯†ï¼ˆå¦‚ "parse_error", "encoding_error"ç­‰ï¼‰

        Returns:
            True å¦‚æœåº”è¯¥åœæ­¢çˆ¬å–ï¼ŒFalse ç»§ç»­
        """
        if self._should_stop_crawling:
            return True

        # è®°å½•é”™è¯¯æ¬¡æ•°
        self._error_type_counter[error_type] = self._error_type_counter.get(error_type, 0) + 1
        count = self._error_type_counter[error_type]

        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é˜ˆå€¼
        if count >= self._error_threshold:
            self._should_stop_crawling = True
            logger.error(f"â›” [CRAWLER] é”™è¯¯ç±»å‹ '{error_type}' å·²å‡ºç° {count} æ¬¡ï¼Œè¶…è¿‡é˜ˆå€¼ {self._error_threshold}ï¼Œåœæ­¢çˆ¬å–")
            logger.error(f"âš ï¸ [CRAWLER] å¯èƒ½é‡åˆ°åçˆ¬æˆ–æœåŠ¡å¼‚å¸¸ï¼Œé¿å…ç»§ç»­è¯·æ±‚")
            return True
        elif count % 5 == 0:  # æ¯5æ¬¡æ‰“å°ä¸€æ¬¡è­¦å‘Š
            logger.warning(f"âš ï¸ [CRAWLER] é”™è¯¯ç±»å‹ '{error_type}' å·²å‡ºç° {count} æ¬¡")

        return False

    def _record_success(self):
        """è®°å½•æˆåŠŸï¼Œé‡ç½®å¤±è´¥è®¡æ•°"""
        if self._consecutive_failures > 0:
            logger.debug(f"âœ“ è¯·æ±‚æˆåŠŸï¼Œé‡ç½®å¤±è´¥è®¡æ•°ï¼ˆä¹‹å‰è¿ç»­å¤±è´¥{self._consecutive_failures}æ¬¡ï¼‰")
            self._consecutive_failures = 0
        # å¦‚æœè¿ç»­æˆåŠŸ3æ¬¡ï¼Œå¯ä»¥å°è¯•æ¢å¤æ­£å¸¸æ¨¡å¼
        if self._slow_mode:
            # ç®€å•ç­–ç•¥ï¼šæˆåŠŸåç«‹å³æ¢å¤æ­£å¸¸æ¨¡å¼
            self._slow_mode = False
            logger.info(f"âœ“ è¯·æ±‚æˆåŠŸï¼Œæ¢å¤æ­£å¸¸é€Ÿåº¦æ¨¡å¼")

    def _adaptive_delay(self):
        """è‡ªé€‚åº”å»¶è¿Ÿ - æ ¹æ®æ¨¡å¼é€‰æ‹©å»¶è¿Ÿæ—¶é—´"""
        import random
        if self._slow_mode:
            delay = random.uniform(*self._slow_mode_delay)
            logger.debug(f"æ…¢é€Ÿæ¨¡å¼å»¶è¿Ÿ {delay:.2f} ç§’...")
        else:
            delay = random.uniform(*self._normal_mode_delay)
            logger.debug(f"æ­£å¸¸å»¶è¿Ÿ {delay:.2f} ç§’...")
        time.sleep(delay)

    def get_original(self, url):
        """è·å–åŸå§‹é¡µé¢å†…å®¹ - åŸºäºåŸå§‹sht.pyçš„ç¨³å®šå®ç°ï¼Œæé«˜æˆåŠŸç‡"""
        try:
            # ä½¿ç”¨Sessionä¿æŒè¿æ¥å’ŒcookieæŒä¹…åŒ–
            session = self._get_session()

            # ä½¿ç”¨å›ºå®šå‚æ•°ç»„åˆï¼Œä¸åŸç‰ˆä¿æŒä¸€è‡´
            res = session.get(url,
                             proxies=self.proxies,
                             cookies=self.cookie,
                             headers=self.headers,
                             allow_redirects=True,
                             timeout=10,
                             impersonate="chrome110")

            html = res.text.encode('utf-8')
            doc = pq(html)
            page_title = doc('head>title').text()

            # æ£€æŸ¥CloudFlare - ä½¿ç”¨åŸç‰ˆçš„ç²¾å‡†æ£€æµ‹
            if 'Just a moment' in page_title:
                logger.info(f"[API] æ£€æµ‹åˆ°CloudFlareï¼Œå°è¯•ç»•è¿‡")
                html = self.bypass_cf(url)
                if not html:
                    logger.error(f"âœ— [API] CloudFlareç»•è¿‡å¤±è´¥")
                    self._record_failure()  # è®°å½•å¤±è´¥
                    return None

            # æ£€æŸ¥18ç¦éªŒè¯ - ä½¿ç”¨åŸç‰ˆçš„æ£€æµ‹æ–¹å¼
            doc = pq(html)
            if "var safeid" in doc.text():
                logger.info(f"[API] æ£€æµ‹åˆ°å¹´é¾„éªŒè¯ï¼Œå°è¯•è·å–safeid")
                html = self.bypass_r18(html, url)
                if not html:
                    logger.error(f"âœ— [API] å¹´é¾„éªŒè¯ç»•è¿‡å¤±è´¥")
                    self._record_failure()  # è®°å½•å¤±è´¥
                    return None

            # éªŒè¯é¡µé¢æœ‰æ•ˆæ€§ - ä½¿ç”¨åŸç‰ˆçš„æˆåŠŸæ ‡å¿—
            doc = pq(html)
            page_title = doc('head>title').text()
            # æ¥å—å¤šç§æœ‰æ•ˆé¡µé¢æ ‡é¢˜ï¼š98å ‚è®ºå›é¡µé¢ã€é¦–é¡µã€é—¨æˆ·ç­‰
            valid_titles = ["98å ‚", "é—¨æˆ·", "forum", "Discuz"]
            if any(keyword in page_title for keyword in valid_titles):
                logger.debug(f"å¹´é¾„éªŒè¯ç»•è¿‡æˆåŠŸ")
                self._record_success()  # è®°å½•æˆåŠŸ
                return html
            else:
                logger.warning(f"å¹´é¾„éªŒè¯åé¡µé¢æ ‡é¢˜å¼‚å¸¸: {page_title}")
                self._record_failure()  # è®°å½•å¤±è´¥
                return None

        except Exception as e:
            logger.error(f"âœ— [API] è·å–é¡µé¢å¤±è´¥: {url}, é”™è¯¯: {e}")
            self._record_failure()  # è®°å½•å¤±è´¥
            return None

    def bypass_cf(self, url):
        """ç»•è¿‡CloudFlare - ç®€åŒ–ç‰ˆæœ¬ï¼Œå‚è€ƒsht.py"""
        if not self.flare_solver:
            logger.warning(f"! [CONFIG] æœªé…ç½®FLARE_SOLVERR_URLï¼Œæ— æ³•ç»•è¿‡Cloudflare")
            return None
            
        payload = {
            "cmd": "request.get",
            "url": url,
            "maxTimeout": 60000,
            "proxy": {"url": self.proxies['http']} if self.proxies and 'http' in self.proxies else None,
            "cookies": [{"name": k, "value": v} for k, v in self.cookie.items()]
        }
        try:
            res = requests.post(self.flare_solver, headers={"Content-Type": "application/json"}, json=payload)
            result = res.json()
            if result['solution']['status'] != 200:
                return None
            html = result['solution']['response'].encode('utf-8')
            doc = pq(html)
            if "var safeid" in doc.text():
                safeid = extract_safeid(html)
                self.cookie['_safe'] = safeid
                return self.bypass_cf(url)  # é€’å½’å°è¯•
            return html
        except Exception as e:
            logger.error(f"âœ— [API] ç»•è¿‡Cloudflareå¤±è´¥: {e}")
            return None

    def bypass_r18(self, html, url):
        """ç»•è¿‡å¹´é¾„éªŒè¯ - åŸºäºåŸå§‹sht.pyçš„å®ç°ï¼Œç®€åŒ–cookieç®¡ç†"""
        safeid = extract_safeid(html)
        if safeid:
            # ç«‹å³æ›´æ–°cookieï¼Œä¿æŒçŠ¶æ€åŒæ­¥
            self.cookie['_safe'] = safeid
            logger.debug(f"è·å–åˆ°safeid: {safeid[:8]}...")

            # ä½¿ç”¨Sessionä¿æŒè¿æ¥
            session = self._get_session()

            # ä½¿ç”¨ç›¸åŒçš„è¯·æ±‚å‚æ•°é‡æ–°è®¿é—®
            res = session.get(url,
                             proxies=self.proxies,
                             cookies=self.cookie,
                             headers=self.headers,
                             allow_redirects=True,
                             timeout=10,
                             impersonate="chrome110")
            html = res.text.encode('utf-8')
            doc = pq(html)
            page_title = doc('head>title').text()

            # ä½¿ç”¨åŸç‰ˆçš„æˆåŠŸéªŒè¯æ–¹å¼
            if "98å ‚" in page_title:
                logger.debug("å¹´é¾„éªŒè¯ç»•è¿‡æˆåŠŸ")
                return html
            else:
                logger.warning(f"å¹´é¾„éªŒè¯åé¡µé¢æ ‡é¢˜å¼‚å¸¸: {page_title}")
        else:
            logger.warning("æœªèƒ½æå–åˆ°safeid")
        return None

    def _retry_with_delay(self, func, func_name, max_attempts=3, delay_seconds=5):
        """å¸¦é‡è¯•æœºåˆ¶çš„å‡½æ•°è°ƒç”¨åŒ…è£…å™¨

        Args:
            func: è¦æ‰§è¡Œçš„å‡½æ•°
            func_name: å‡½æ•°åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            max_attempts: æœ€å¤§å°è¯•æ¬¡æ•°ï¼ˆé»˜è®¤3æ¬¡ï¼‰
            delay_seconds: å¤±è´¥åçš„å»¶è¿Ÿæ—¶é—´ï¼ˆé»˜è®¤5ç§’ï¼‰

        Returns:
            å‡½æ•°æ‰§è¡Œç»“æœï¼Œå¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥åˆ™è¿”å›Noneæˆ–{}
        """
        import time

        for attempt in range(1, max_attempts + 1):
            try:
                logger.info(f" [{func_name}] ç¬¬ {attempt}/{max_attempts} æ¬¡å°è¯•")
                result = func()

                # å¦‚æœæˆåŠŸè·å–åˆ°æ•°æ®ï¼Œç«‹å³è¿”å›
                if result:
                    if attempt > 1:
                        logger.info(f"âœ“ [{func_name}] ç¬¬ {attempt} æ¬¡å°è¯•æˆåŠŸ")
                    return result
                else:
                    logger.warning(f"! [{func_name}] ç¬¬ {attempt} æ¬¡å°è¯•æœªè·å–åˆ°æ•°æ®")

            except Exception as e:
                logger.error(f"âœ— [{func_name}] ç¬¬ {attempt} æ¬¡å°è¯•å‡ºé”™: {e}")

            # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
            if attempt < max_attempts:
                logger.info(f"â³ [{func_name}] {delay_seconds}ç§’åè¿›è¡Œç¬¬ {attempt + 1} æ¬¡å°è¯•...")
                time.sleep(delay_seconds)

        logger.error(f"âœ— [{func_name}] æ‰€æœ‰ {max_attempts} æ¬¡å°è¯•å‡å¤±è´¥")
        return {} if isinstance(result, dict) else None

    def get_all_forums_info(self):
        """è·å–æ‰€æœ‰æ¿å—ä¿¡æ¯ - åŒé‡ç­–ç•¥ï¼šä¼˜å…ˆæ‰‹æœºç‰ˆåˆ—è¡¨é¡µï¼Œå¤±è´¥åˆ™å°è¯•æ¡Œé¢ç‰ˆé¦–é¡µ"""
        import time

        # æ£€æŸ¥å®ä¾‹ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        current_time = time.time()
        if self._forums_cache and (current_time - self._forums_cache_time) < self._cache_duration:
            logger.info(f"âœ“ ä½¿ç”¨å®ä¾‹ç¼“å­˜çš„æ¿å—ä¿¡æ¯ï¼ˆ{int(self._cache_duration - (current_time - self._forums_cache_time))}ç§’åè¿‡æœŸï¼‰")
            return self._forums_cache

        logger.info("[CRAWLER] å¼€å§‹è·å–æ‰€æœ‰æ¿å—ä¿¡æ¯ï¼ˆå‡†ç¡®æ•°æ®ï¼ŒåŒé‡ç­–ç•¥ï¼Œå¸¦é‡è¯•æœºåˆ¶ï¼‰")

        # ã€ç­–ç•¥Aã€‘ä¼˜å…ˆå°è¯•æ‰‹æœºç‰ˆæ¿å—åˆ—è¡¨é¡µï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼šæœ€å¤š3æ¬¡ï¼Œé—´éš”5ç§’ï¼‰
        forums_info = self._retry_with_delay(
            func=self._get_forums_from_mobile_list,
            func_name="ç­–ç•¥A-æ‰‹æœºç‰ˆåˆ—è¡¨é¡µ",
            max_attempts=3,
            delay_seconds=5
        )

        # ã€ç­–ç•¥Bã€‘å¦‚æœæ‰‹æœºç‰ˆå¤±è´¥ï¼Œå°è¯•æ¡Œé¢ç‰ˆé¦–é¡µä½œä¸ºå¤‡ä»½ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        if not forums_info:
            logger.warning(f"! æ‰‹æœºç‰ˆæ¿å—åˆ—è¡¨é¡µè·å–å¤±è´¥ï¼Œå°è¯•æ¡Œé¢ç‰ˆé¦–é¡µä½œä¸ºå¤‡ä»½")
            forums_info = self._retry_with_delay(
                func=self._get_forums_from_desktop_home,
                func_name="ç­–ç•¥B-æ¡Œé¢ç‰ˆé¦–é¡µ",
                max_attempts=3,
                delay_seconds=5
            )

        # å¦‚æœä¸¤ç§æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨é¢„å®šä¹‰åˆ—è¡¨
        if not forums_info:
            logger.warning(f"! ä¸¤ç§æ–¹æ³•å‡å¤±è´¥ï¼Œä½¿ç”¨é¢„å®šä¹‰æ¿å—åˆ—è¡¨")
            forums_info = self._get_default_forums()

        # æ›´æ–°ç¼“å­˜
        self._forums_cache = forums_info
        self._forums_cache_time = current_time

        return forums_info

    def _get_forums_from_mobile_list(self):
        """ç­–ç•¥Aï¼šä»æ‰‹æœºç‰ˆæ¿å—åˆ—è¡¨é¡µæå–å‡†ç¡®æ•°æ®"""
        try:
            # åªè·å–é¢„è®¾çš„11ä¸ªæ¿å—
            from constants import VALID_FIDS
            
            logger.info(f"[CRAWLER] [ç­–ç•¥A] å°è¯•ä»æ‰‹æœºç‰ˆæ¿å—åˆ—è¡¨é¡µè·å–ï¼ˆä»…é™{len(VALID_FIDS)}ä¸ªé¢„è®¾æ¿å—ï¼‰")

            url = "https://sehuatang.org/forum.php?forumlist=1&mobile=2"
            html = self.get_original(url)

            if not html:
                logger.warning(f"! [ç­–ç•¥A] æ— æ³•è·å–æ¿å—åˆ—è¡¨é¡µ")
                return {}

            doc = pq(html)
            forum_items = doc('div.sub_forum ul li')

            if len(forum_items) == 0:
                logger.warning(f"! [ç­–ç•¥A] æœªæ‰¾åˆ°æ¿å—åˆ—è¡¨é¡¹")
                return {}

            logger.debug(f"[CRAWLER] [ç­–ç•¥A] åœ¨æ¿å—åˆ—è¡¨é¡µæ‰¾åˆ° {len(forum_items)} ä¸ªæ¿å—ï¼Œç­›é€‰é¢„è®¾æ¿å—")
            forums_info = {}

            for item in forum_items:
                try:
                    item_pq = pq(item)

                    # æå–æ¿å—é“¾æ¥å’Œfid
                    link_elem = item_pq('a.btdb').eq(0)
                    if not link_elem:
                        continue

                    href = link_elem.attr('href')
                    if not href:
                        continue

                    fid_match = re.search(r'fid=(\d+)', href)
                    if not fid_match:
                        continue

                    fid = fid_match.group(1)

                    # åªå¤„ç†é¢„è®¾çš„æ¿å—
                    if fid not in VALID_FIDS:
                        continue

                    # æå–æ¿å—åç§°ï¼ˆå»é™¤<span class="num">ï¼‰
                    name_text = link_elem.text().strip()
                    num_span = link_elem.find('span.num')
                    if num_span:
                        num_text = num_span.text().strip()
                        name = name_text.replace(num_text, '').strip()
                    else:
                        name = name_text

                    if not name:
                        continue

                    # æå–ä¸»é¢˜æ•°ï¼š<i>ä¸»é¢˜:<span title="41167">4ä¸‡</span> å¸–æ•°:...</i>
                    total_topics = None
                    stats_elem = item_pq('i').eq(0)
                    if stats_elem:
                        stats_text = stats_elem.text()
                        if 'ä¸»é¢˜' in stats_text:
                            # ä¼˜å…ˆä»span titleæå–
                            topic_spans = stats_elem.find('span[title]')
                            for span in topic_spans:
                                span_pq = pq(span)
                                span_html = stats_elem.html()
                                span_outer = span_pq.outerHtml()

                                span_index = span_html.find(span_outer) if span_outer else -1
                                if span_index > 0:
                                    before_text = span_html[:span_index]
                                    if 'ä¸»é¢˜' in before_text and before_text.rfind('ä¸»é¢˜') > before_text.rfind('å¸–æ•°'):
                                        title_value = span_pq.attr('title')
                                        if title_value and title_value.isdigit():
                                            total_topics = int(title_value)
                                            logger.debug(f"âœ… [ç­–ç•¥A] [{name}] ä»titleå±æ€§æå–: {total_topics}")
                                            break

                            # å¤‡ç”¨ï¼šä»æ–‡æœ¬æå–
                            if total_topics is None:
                                topic_match = re.search(r'ä¸»é¢˜[ï¼š:]\s*(\d+)', stats_text)
                                if topic_match:
                                    match_pos = topic_match.start()
                                    if 'ä¸»é¢˜' in stats_text[:match_pos + 10]:
                                        total_topics = int(topic_match.group(1))
                                        logger.debug(f"âœ… [ç­–ç•¥A] [{name}] ä»æ–‡æœ¬æå–: {total_topics}")

                    forums_info[fid] = {
                        'fid': fid,
                        'name': name,
                        'total_topics': total_topics,
                        'total_pages': None,
                    }

                    if total_topics is not None:
                        logger.info(f"[CRAWLER] [ç­–ç•¥A] {name} (fid={fid}) - {total_topics}ä¸»é¢˜")

                except Exception as e:
                    logger.debug(f"! [ç­–ç•¥A] è§£ææ¿å—é¡¹å¤±è´¥: {e}")
                    continue

            if forums_info:
                logger.info(f"âœ“ [ç­–ç•¥A] æˆåŠŸè·å– {len(forums_info)} ä¸ªæ¿å—ä¿¡æ¯")
            return forums_info

        except Exception as e:
            logger.error(f"âœ— [ç­–ç•¥A] å¤±è´¥: {e}")
            return {}

    def _get_forums_from_desktop_home(self):
        """ç­–ç•¥Bï¼šä»æ¡Œé¢ç‰ˆé¦–é¡µæå–å‡†ç¡®æ•°æ®ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        try:
            # åªè·å–é¢„è®¾çš„11ä¸ªæ¿å—
            from constants import VALID_FIDS
            
            logger.info(f"[CRAWLER] [ç­–ç•¥B] å°è¯•ä»æ¡Œé¢ç‰ˆé¦–é¡µè·å–ï¼ˆä»…é™{len(VALID_FIDS)}ä¸ªé¢„è®¾æ¿å—ï¼‰")

            # ä¸´æ—¶åˆ‡æ¢åˆ°æ¡Œé¢ç‰ˆUA
            original_ua = self.headers.get('User-Agent')
            desktop_ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            self.headers['User-Agent'] = desktop_ua

            try:
                url = "https://sehuatang.org/forum.php"
                html = self.get_original(url)
            finally:
                self.headers['User-Agent'] = original_ua

            if not html:
                logger.warning(f"! [ç­–ç•¥B] æ— æ³•è·å–é¦–é¡µ")
                return {}

            doc = pq(html)
            forum_cards = doc('td.fl_g')

            if len(forum_cards) == 0:
                logger.warning(f"! [ç­–ç•¥B] æœªæ‰¾åˆ°æ¿å—å¡ç‰‡")
                return {}

            logger.debug(f"ğŸ“‹ [ç­–ç•¥B] åœ¨é¦–é¡µæ‰¾åˆ° {len(forum_cards)} ä¸ªæ¿å—å¡ç‰‡ï¼Œç­›é€‰é¢„è®¾æ¿å—")
            forums_info = {}

            for card in forum_cards:
                try:
                    card_pq = pq(card)

                    # æå–æ¿å—é“¾æ¥å’Œåç§°
                    link_elem = card_pq('dt a').eq(0)
                    if not link_elem:
                        continue

                    name = link_elem.text().strip()
                    href = link_elem.attr('href')

                    if not href or not name:
                        continue

                    # ä»hrefæå–fid: forum-2-1.html -> fid=2
                    fid_match = re.search(r'forum-(\d+)-', href)
                    if not fid_match:
                        continue

                    fid = fid_match.group(1)

                    # åªå¤„ç†é¢„è®¾çš„æ¿å—
                    if fid not in VALID_FIDS:
                        continue

                    # æå–ä¸»é¢˜æ•°ï¼š<dd><em>ä¸»é¢˜: <span title="68250">6ä¸‡</span></em>...
                    total_topics = None
                    topic_dd = card_pq('dd').eq(0)
                    if topic_dd:
                        topic_text = topic_dd.text()
                        if 'ä¸»é¢˜' in topic_text:
                            # ä¼˜å…ˆä»span titleæå–
                            topic_spans = topic_dd.find('span[title]')
                            for span in topic_spans:
                                span_pq = pq(span)
                                parent_text = span_pq.parent().text()

                                if 'ä¸»é¢˜' in parent_text and 'å¸–æ•°' not in parent_text:
                                    title_value = span_pq.attr('title')
                                    if title_value and title_value.isdigit():
                                        total_topics = int(title_value)
                                        logger.debug(f"âœ… [ç­–ç•¥B] [{name}] ä»titleå±æ€§æå–: {total_topics}")
                                        break

                            # å¤‡ç”¨ï¼šä»æ–‡æœ¬æå–
                            if total_topics is None:
                                topic_match = re.search(r'ä¸»é¢˜[ï¼š:]\s*(\d+)', topic_text)
                                if topic_match:
                                    total_topics = int(topic_match.group(1))
                                    logger.debug(f"âœ… [ç­–ç•¥B] [{name}] ä»æ–‡æœ¬æå–: {total_topics}")

                    forums_info[fid] = {
                        'fid': fid,
                        'name': name,
                        'total_topics': total_topics,
                        'total_pages': None,
                    }

                    if total_topics is not None:
                        logger.info(f"ğŸ“‹ [ç­–ç•¥B] {name} (fid={fid}) - {total_topics}ä¸»é¢˜")

                except Exception as e:
                    logger.debug(f"âš ï¸ [ç­–ç•¥B] è§£ææ¿å—å¡ç‰‡å¤±è´¥: {e}")
                    continue

            if forums_info:
                logger.info(f"âœ“ [ç­–ç•¥B] æˆåŠŸè·å– {len(forums_info)} ä¸ªæ¿å—ä¿¡æ¯")
            return forums_info

        except Exception as e:
            logger.error(f"âœ— [ç­–ç•¥B] å¤±è´¥: {e}")
            return {}

    def _get_default_forums(self):
        """è·å–é¢„å®šä¹‰çš„æ¿å—åˆ—è¡¨"""
        default_forums = {
            '2': 'å›½äº§åŸåˆ›',
            '36': 'äºšæ´²æ— ç åŸåˆ›', 
            '37': 'äºšæ´²æœ‰ç åŸåˆ›',
            '103': 'é«˜æ¸…ä¸­æ–‡å­—å¹•',
            '107': 'ä¸‰çº§å†™çœŸ',
            '160': 'VRè§†é¢‘åŒº',
            '104': 'ç´ äººæœ‰ç ç³»åˆ—',
            '38': 'æ¬§ç¾æ— ç ',
            '151': '4KåŸç‰ˆ',
            '152': 'éŸ©å›½ä¸»æ’­',
            '39': 'åŠ¨æ¼«åŸåˆ›'
        }
        
        forums_info = {}
        for fid, name in default_forums.items():
            forums_info[fid] = {
                'fid': fid,
                'name': name,
                'total_topics': None,  # ä¸ä½¿ç”¨ä¼°ç®—å€¼ï¼Œè¿”å›Noneè¡¨ç¤ºéœ€è¦åŒæ­¥
                'total_pages': None    # ä¸ä½¿ç”¨ä¼°ç®—å€¼ï¼Œè¿”å›Noneè¡¨ç¤ºéœ€è¦åŒæ­¥
            }

        return forums_info
    def _test_topic_parsing(self, test_html):
        """æµ‹è¯•ä¸»é¢˜æ•°è§£æé€»è¾‘"""
        try:
            # æ¨¡æ‹Ÿç”¨æˆ·æä¾›çš„HTMLç»“æ„
            stats_text = "ä¸»é¢˜:2427 å¸–æ•°:7ä¸‡"
            stats_html = test_html
            
            # æ–¹æ³•1: ç›´æ¥ä»æ–‡æœ¬ä¸­æå– "ä¸»é¢˜:æ•°å­—" (æœ€å¯é )
            topic_match = re.search(r'ä¸»é¢˜[ï¼š:]\s*(\d+)', stats_text)
            if topic_match:
                return f"æˆåŠŸæå–ä¸»é¢˜æ•°: {topic_match.group(1)}"
            
            # æ–¹æ³•2: ä»HTMLä¸­æå–ï¼Œä½†è¦é¿å…span titleä¸­çš„æ•°å­—
            clean_html = re.sub(r'<span[^>]*title="[^"]*"[^>]*>.*?</span>', '', stats_html)
            topic_match_clean = re.search(r'ä¸»é¢˜[ï¼š:]\s*(\d+)', clean_html)
            if topic_match_clean:
                return f"ä»æ¸…ç†HTMLæå–ä¸»é¢˜æ•°: {topic_match_clean.group(1)}"
            
            return "è§£æå¤±è´¥"
        except Exception as e:
            return f"è§£æå¼‚å¸¸: {e}"

    def get_forum_info(self, fid, all_forums_cache=None):
        """è·å–å•ä¸ªæ¿å—ä¿¡æ¯ï¼šæ€»é¡µæ•°ã€ä¸»é¢˜æ•°é‡ç­‰

        Args:
            fid: æ¿å—ID
            all_forums_cache: å¯é€‰çš„æ¿å—ä¿¡æ¯ç¼“å­˜ï¼Œé¿å…é‡å¤è·å–
        """
        logger.debug(f" è·å–æ¿å—ä¿¡æ¯: fid={fid}")

        try:
            # ä¼˜å…ˆä»é¦–é¡µè·å–å‡†ç¡®çš„ä¸»é¢˜æ•°ï¼Œè€Œä¸æ˜¯ä»å•ä¸ªæ¿å—é¡µé¢æ¨æ–­
            # å¦‚æœæ²¡æœ‰ä¼ å…¥ç¼“å­˜ï¼Œåˆ™è°ƒç”¨get_all_forums_info()è·å–é¦–é¡µæ•°æ®
            if not all_forums_cache:
                logger.debug("ğŸ“‹ æœªä¼ å…¥ç¼“å­˜ï¼Œä»é¦–é¡µè·å–æ‰€æœ‰æ¿å—ä¿¡æ¯")
                all_forums_cache = self.get_all_forums_info()

            # ä½¿ç”¨é¦–é¡µè·å–çš„å‡†ç¡®æ•°æ®
            if all_forums_cache and fid in all_forums_cache:
                forum_info = {
                    'fid': fid,
                    'name': all_forums_cache[fid]['name'],
                    'total_topics': all_forums_cache[fid].get('total_topics'),  # ä½¿ç”¨é¦–é¡µçš„å‡†ç¡®æ•°æ®
                    'total_pages': all_forums_cache[fid].get('total_pages')     # å¯èƒ½ä¸ºNone
                }
                logger.debug(f"âœ… ä»é¦–é¡µæ•°æ®è·å–åˆ°å‡†ç¡®ä¿¡æ¯: total_topics={forum_info['total_topics']}")
            else:
                # å°è¯•ä»æŒä¹…åŒ–ç¼“å­˜è·å–
                cat = Category.query.filter_by(fid=str(fid)).first()
                cached_forum = cat.to_dict() if cat else None

                if cached_forum:
                    forum_info = {
                        'fid': fid,
                        'name': cached_forum['name'],
                        'total_topics': cached_forum.get('total_topics'),
                        'total_pages': cached_forum.get('total_pages')
                    }
                    logger.debug(f"âœ… ä»æŒä¹…åŒ–ç¼“å­˜è·å–åˆ°ä¿¡æ¯: {forum_info}")
                else:
                    # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    forum_info = {
                        'fid': fid,
                        'name': f'æ¿å—{fid}',
                        'total_topics': None,
                        'total_pages': None
                    }
                    logger.warning(f"! ç¼“å­˜ä¸­æœªæ‰¾åˆ°fid={fid}ï¼Œä½¿ç”¨é»˜è®¤ä¿¡æ¯")

            # åªåœ¨éœ€è¦æ—¶è·å–é¡µæ•°ä¿¡æ¯ï¼ˆé¦–é¡µæ²¡æœ‰é¡µæ•°ï¼Œéœ€è¦å•ç‹¬è·å–ï¼‰
            # å¦‚æœä¸»é¢˜æ•°ä¸ºNoneï¼Œè¯´æ˜é¦–é¡µä¹Ÿæ²¡è·å–åˆ°ï¼Œéœ€è¦è®¿é—®æ¿å—é¡µé¢
            if forum_info['total_pages'] is None:
                url = f"https://sehuatang.org/forum.php?mod=forumdisplay&fid={fid}&mobile=2"
                logger.debug(f" è·å–é¡µæ•°ä¿¡æ¯: {url}")

                html = self.get_original(url)
                if html:
                    doc = pq(html)

                    pages_found = False

                    # æ–¹æ³•1: ä½¿ç”¨ç”¨æˆ·æä¾›çš„å…·ä½“é€‰æ‹©å™¨è·å–æ€»é¡µæ•°
                    page_span = doc('#fd_page_top > div > label > span')
                    if page_span and page_span.length > 0:
                        page_text = page_span.text()
                        logger.debug(f"ğŸ“„ é¡µæ•°æ–‡æœ¬: {page_text}")

                        # ä» "/ 2272 é¡µ" æ ¼å¼ä¸­æå–é¡µæ•°
                        page_match = re.search(r'/\s*(\d+)\s*é¡µ', page_text)
                        if page_match:
                            forum_info['total_pages'] = int(page_match.group(1))
                            logger.debug(f"âœ… ä»é¡µæ•°spanæå–æ€»é¡µæ•°: {forum_info['total_pages']}")
                            pages_found = True
                        else:
                            # å¤‡ç”¨æ–¹æ³•ï¼šä»titleå±æ€§æå–
                            title_attr = page_span.attr('title')
                            if title_attr:
                                title_match = re.search(r'å…±\s*(\d+)\s*é¡µ', title_attr)
                                if title_match:
                                    forum_info['total_pages'] = int(title_match.group(1))
                                    logger.debug(f"âœ… ä»titleå±æ€§æå–æ€»é¡µæ•°: {forum_info['total_pages']}")
                                    pages_found = True

                    # æ–¹æ³•2: å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨é€šç”¨åˆ†é¡µé€‰æ‹©å™¨
                    if not pages_found:
                        pg_elements = doc('div.pg, .pg, [class*="pg"], .pages, .pagination')
                        for pg_element in pg_elements:
                            pg_pq = pq(pg_element)
                            page_text = pg_pq.text()
                            logger.debug(f"ğŸ“„ å¤‡ç”¨åˆ†é¡µæ–‡æœ¬: {page_text}")

                            # æŸ¥æ‰¾ "/ XX é¡µ" çš„æ–‡æœ¬
                            page_match = re.search(r'/\s*(\d+)\s*é¡µ', page_text)
                            if page_match:
                                forum_info['total_pages'] = int(page_match.group(1))
                                logger.debug(f"âœ… å¤‡ç”¨æ–¹æ³•è§£ææ€»é¡µæ•°: {forum_info['total_pages']}")
                                pages_found = True
                                break

                            # æŸ¥æ‰¾æœ€åä¸€é¡µçš„é“¾æ¥
                            last_links = pg_pq.find('a.last, a[class*="last"], a:contains("æœ«é¡µ"), a:contains("æœ€å")')
                            for last_link in last_links:
                                last_href = pq(last_link).attr('href')
                                if last_href:
                                    page_match = re.search(r'page=(\d+)', last_href)
                                    if not page_match:
                                        page_match = re.search(r'-(\d+)\.html', last_href)
                                    if page_match:
                                        forum_info['total_pages'] = int(page_match.group(1))
                                        logger.debug(f"âœ… ä»é“¾æ¥è§£ææ€»é¡µæ•°: {forum_info['total_pages']}")
                                        pages_found = True
                                        break

                            if pages_found:
                                break

                    # æ–¹æ³•3: æŸ¥æ‰¾æ‰€æœ‰åŒ…å«é¡µæ•°çš„é“¾æ¥
                    if not pages_found:
                        all_links = doc('a[href*="page="], a[href*="forum.php"]')
                        max_page = 0
                        for link in all_links:
                            href = pq(link).attr('href')
                            if href:
                                page_match = re.search(r'page=(\d+)', href)
                                if page_match:
                                    page_num = int(page_match.group(1))
                                    if page_num > max_page:
                                        max_page = page_num

                        if max_page > 1:
                            forum_info['total_pages'] = max_page
                            logger.debug(f"âœ… ä»é“¾æ¥ä¸­æ‰¾åˆ°æœ€å¤§é¡µæ•°: {forum_info['total_pages']}")
                            pages_found = True

                    if not pages_found:
                        logger.warning(f"! æœªæ‰¾åˆ°é¡µæ•°ä¿¡æ¯ï¼Œä¿æŒä¸ºNone")
                else:
                    logger.warning(f"! æ— æ³•è·å–æ¿å—é¡µé¢ï¼Œé¡µæ•°ä¿æŒä¸ºNone")

            # æ„å»ºæ˜¾ç¤ºæ–‡æœ¬
            if forum_info['total_topics'] is None:
                topics_display = "æœªçŸ¥"
            else:
                topics_display = str(forum_info['total_topics'])

            if forum_info['total_pages'] is None:
                pages_display = "æœªçŸ¥"
            else:
                pages_display = str(forum_info['total_pages'])

            logger.info(f"ğŸ“‹ æ¿å—ä¿¡æ¯ [{forum_info['name']}]: æ€»è®¡{topics_display}ä¸»é¢˜, å…±{pages_display}é¡µ")

            return forum_info
            
        except Exception as e:
            logger.error(f"âœ— è·å–æ¿å—ä¿¡æ¯å¤±è´¥: fid={fid}, é”™è¯¯: {e}")
            logger.debug(f" è¯¦ç»†é”™è¯¯ä¿¡æ¯", exc_info=True)
            return None

    def crawler_tid_list(self, url):
        """çˆ¬å–é¡µé¢ä¸­çš„tidåˆ—è¡¨ - åŸºäºåŸå§‹sht.pyçš„ç¨³å®šå®ç°ï¼Œå¢åŠ é‡è¯•æœºåˆ¶"""
        # æ·»åŠ é‡è¯•æœºåˆ¶ï¼Œæé«˜æˆåŠŸç‡
        for retry in range(3):
            try:
                html = self.get_original(url)
                if html:
                    doc = pq(html)
                    # ä½¿ç”¨åŸç‰ˆçš„ç²¾ç¡®é€‰æ‹©å™¨
                    items = doc("div.n5_htnrys.cl")[1:]  # è·³è¿‡ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆé€šå¸¸æ˜¯æ ‡é¢˜è¡Œï¼‰
                    id_list = []
                    for item in items:
                        pq_item = pq(item)
                        link = pq_item("div a").eq(0).attr('href')  # æå–hrefå±æ€§
                        if link:
                            parsed_url = urlparse(link)
                            query_params = parse_qs(parsed_url.query)  # è§£æä¸ºå­—å…¸ï¼ˆå€¼ä¸ºåˆ—è¡¨ï¼‰
                            tid = query_params.get('tid', [''])[0]
                            if tid and tid.isdigit():
                                id_list.append(int(tid))
                    
                    if id_list:
                        logger.debug(f"æˆåŠŸæå–åˆ° {len(id_list)} ä¸ªtid")
                        return id_list
                    else:
                        logger.warning(f"é¡µé¢æ— æœ‰æ•ˆtidï¼Œé‡è¯• {retry + 1}/3")
                else:
                    logger.warning(f"è·å–é¡µé¢å¤±è´¥ï¼Œé‡è¯• {retry + 1}/3")
                    
            except Exception as e:
                logger.warning(f"çˆ¬å–{url}å¤±è´¥ï¼Œé‡è¯• {retry + 1}/3: {e}")
                
            # é‡è¯•å‰çŸ­æš‚å»¶è¿Ÿ
            if retry < 2:
                import time
                time.sleep(1)
        
        logger.error(f"è¿ç»­3æ¬¡çˆ¬å–å¤±è´¥: {url}")
        return []
    
    def _fix_mobile_session_and_retry(self, original_url):
        """ä¿®å¤ä¼šè¯é—®é¢˜ - æ”¹ç”¨æ¡Œé¢ç‰ˆè®¿é—®ç­–ç•¥"""
        logger.info("ğŸ”§ å¼€å§‹ä¿®å¤ä¼šè¯é—®é¢˜...")
        
        try:
            import time
            import random
            
            # 1. å…ˆè®¿é—®æ¡Œé¢ç‰ˆè®ºå›é¦–é¡µ
            desktop_forum_url = "https://sehuatang.org/forum.php"
            logger.info(f"ğŸ–¥ï¸ è®¿é—®æ¡Œé¢ç‰ˆè®ºå›é¦–é¡µ: {desktop_forum_url}")
            
            forum_html = self.get_original(desktop_forum_url)
            if not forum_html:
                logger.error(f"âœ— æ¡Œé¢ç‰ˆè®ºå›é¦–é¡µè®¿é—®å¤±è´¥")
                return None
            
            logger.info(f"âœ“ æ¡Œé¢ç‰ˆè®ºå›é¦–é¡µè®¿é—®æˆåŠŸï¼Œé•¿åº¦: {len(forum_html)}")
            
            # 2. ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œæ¨¡æ‹Ÿç”¨æˆ·æµè§ˆ
            delay = random.uniform(3, 6)
            logger.debug(f"ğŸ˜´ ç­‰å¾… {delay:.1f} ç§’ï¼Œæ¨¡æ‹Ÿç”¨æˆ·æµè§ˆ...")
            time.sleep(delay)
            
            # 3. å°†åŸå§‹URLè½¬æ¢ä¸ºæ¡Œé¢ç‰ˆURLï¼ˆç§»é™¤mobile=2å‚æ•°ï¼‰
            desktop_url = original_url.replace('&mobile=2', '').replace('mobile=2&', '').replace('mobile=2', '')
            
            # å¦‚æœURLä¸­æœ‰backforums=1ï¼Œä¹Ÿç§»é™¤å®ƒï¼Œå› ä¸ºè¿™å¯èƒ½æ˜¯æ‰‹æœºç‰ˆç‰¹æœ‰çš„
            desktop_url = desktop_url.replace('&backforums=1', '').replace('backforums=1&', '').replace('backforums=1', '')
            
            logger.info(f" ä½¿ç”¨æ¡Œé¢ç‰ˆURLé‡æ–°è®¿é—®: {desktop_url}")
            
            retry_html = self.get_original(desktop_url)
            if not retry_html:
                logger.error(f"âœ— æ¡Œé¢ç‰ˆURLè®¿é—®å¤±è´¥")
                return None
            
            # 4. æ£€æŸ¥æ˜¯å¦ä¿®å¤æˆåŠŸ
            if len(retry_html) > 20000:  # æ­£å¸¸é¡µé¢åº”è¯¥æ¯”è¾ƒå¤§
                logger.info(f"âœ“ ä¼šè¯ä¿®å¤æˆåŠŸï¼Œè·å–åˆ°æ­£å¸¸æ¡Œé¢ç‰ˆé¡µé¢")
                return retry_html
            elif "æ‰‹æœºç‰ˆ" in retry_html and "ç°åœ¨å°±ç™»å½•" in retry_html:
                logger.warning(f"! ä»ç„¶æ˜¯æ‰‹æœºç‰ˆå¼•å¯¼é¡µé¢ï¼Œä¿®å¤å¤±è´¥")
                return None
            else:
                logger.info(f"âœ“ ä¼šè¯å¯èƒ½å·²ä¿®å¤ï¼Œè¿”å›æ–°å†…å®¹")
                return retry_html
                
        except Exception as e:
            logger.error(f"âœ— ä¼šè¯ä¿®å¤è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
            return None
    
    def _extract_tids_with_regex(self, html: str, url: str) -> List[int]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–tid - å¤‡ç”¨æ–¹æ³•"""
        logger.debug("ğŸ”§ ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¤‡ç”¨æ–¹æ³•æå–tid")
        
        try:
            import re
            
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«tidçš„é“¾æ¥
            tid_pattern = r'mod=viewthread&tid=(\d+)'
            matches = re.findall(tid_pattern, html)
            
            id_list = []
            for match in matches:
                try:
                    tid_int = int(match)
                    if tid_int not in id_list:  # å»é‡
                        id_list.append(tid_int)
                except ValueError:
                    continue
            
            logger.info(f"ğŸ“‹ æ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•æå–åˆ° {len(id_list)} ä¸ªtid: {url}")
            return id_list
            
        except Exception as e:
            logger.error(f"âœ— æ­£åˆ™è¡¨è¾¾å¼æå–å¤±è´¥: {e}")
            return []

    def crawler_detail(self, url):
        """çˆ¬å–å•ä¸ªè¯¦æƒ…é¡µ - åŸºäºåŸå§‹sht.pyçš„å®ç°ï¼Œä¼˜åŒ–æ•°æ®æå–ç¨³å®šæ€§"""
        try:
            html = self.get_original(url)
            if html:
                doc = pq(html)
                
                # ä½¿ç”¨åŸç‰ˆçš„ç²¾ç¡®ç£åŠ›é“¾æ¥æå–æ–¹å¼
                all_text = doc('div.blockcode').text()
                magnet_pattern = r'magnet:\?xt=urn:btih:[0-9a-fA-F]+'
                match = re.search(magnet_pattern, all_text)
                magnet = None
                
                if match:
                    magnet = match.group()
                    logger.debug(f"æ‰¾åˆ°ç£åŠ›é“¾æ¥: {url}")
                
                # å¤‡ç”¨torrentå¤„ç† - ä¿æŒåŸç‰ˆé€»è¾‘
                if not magnet:
                    torrent = doc("a:contains('.torrent')").eq(0)
                    if torrent:
                        torrent_url = torrent.attr('href')
                        logger.debug(f"å°è¯•è§£ætorrentæ–‡ä»¶: {url}")
                        magnet = self.parse_torrent_get_magnet(url, f"https://sehuatang.org/{torrent_url}")
                
                if magnet:
                    # æå–å…¶ä»–ä¿¡æ¯ - ä½¿ç”¨åŸç‰ˆçš„é€‰æ‹©å™¨
                    date = extract_exact_datetime(html)
                    size = extract_and_convert_video_size(html)
                    sub_type = extract_bracket_content(html)
                    
                    # æ ‡é¢˜å¤„ç† - ä¿æŒåŸç‰ˆé€»è¾‘
                    title = doc('h2.n5_bbsnrbt').text()
                    pattern = r"^\[.*?\]"
                    title = re.sub(pattern, "", title).strip()
                    
                    # é¢„è§ˆå›¾ç‰‡æå–
                    img_elements = doc('div.message img')
                    img_src_list = []
                    for img in img_elements.items():
                        src = img.attr('src')
                        if src:
                            img_src_list.append(src.strip())
                    
                    result = {
                        "title": title,
                        "sub_type": sub_type,
                        "publish_date": date,
                        "magnet": magnet,
                        "preview_images": ",".join(img_src_list),
                        "size": size
                    }
                    
                    logger.debug(f"è§£ææˆåŠŸ: {url}, æ ‡é¢˜: {title[:50]}...")
                    return result
                else:
                    logger.warning(f"æœªæ‰¾åˆ°ç£åŠ›é“¾æ¥: {url}")
                    return {}
            else:
                logger.warning(f"è·å–é¡µé¢å¤±è´¥: {url}")
                return {}
                
        except Exception as e:
            logger.error(f"è§£æè¯¦æƒ…é¡µå¤±è´¥: {url}, é”™è¯¯: {e}")
            return {}

    def parse_torrent_get_magnet(self, refer, torrent_source, is_local=False):
        """è§£ætorrentæ–‡ä»¶è·å–ç£åŠ›é“¾æ¥ - åŸºäºåŸå§‹sht.pyçš„å®ç°ï¼Œå¢å¼ºé”™è¯¯å¤„ç†"""
        try:
            import bencodepy
            import hashlib
            import binascii
            from urllib.parse import urlencode
            
            torrent_bin = None
            if is_local:
                with open(torrent_source, "rb") as f:
                    torrent_bin = f.read()
                if len(torrent_bin) == 0:
                    logger.error("é”™è¯¯ï¼šæœ¬åœ° torrent æ–‡ä»¶ä¸ºç©º")
                    return None
            else:
                # ä½¿ç”¨ä¸get_originalç›¸åŒçš„è¯·æ±‚å‚æ•°ï¼Œä¿æŒä¸€è‡´æ€§
                header = self.headers.copy()
                header['Referer'] = refer
                resp = requests.get(
                    torrent_source,
                    proxies=self.proxies,
                    cookies=self.cookie,
                    headers=header,
                    allow_redirects=True,
                    timeout=10,
                    impersonate="chrome110"
                )
                resp.raise_for_status()
                torrent_bin = resp.content
                if len(torrent_bin) < 100:
                    logger.warning(f"è­¦å‘Šï¼šä¸‹è½½å†…å®¹è¿‡å°ï¼ˆ{len(torrent_bin)} å­—èŠ‚ï¼‰ï¼Œéåˆæ³• torrent æ–‡ä»¶")
                    return None

            # ä½¿ç”¨bencodepyè§£ætorrentæ–‡ä»¶
            torrent_dict = bencodepy.decode(torrent_bin)
            info_dict = None
            if b"info" in torrent_dict:
                info_dict = torrent_dict[b"info"]
            elif "info" in torrent_dict:
                info_dict = torrent_dict["info"]
            else:
                logger.error("é”™è¯¯ï¼šç§å­ç¼ºå°‘ info æ ¸å¿ƒå­—æ®µï¼ˆéåˆæ³• torrent æ–‡ä»¶ï¼‰")
                return None

            # è®¡ç®—info hash
            info_bin = bencodepy.encode(info_dict)
            info_hash = hashlib.sha1(info_bin).digest()
            info_hash_hex = binascii.hexlify(info_hash).decode("utf-8")

            # æå–torrentåç§°
            torrent_name = "Unknown_Torrent"
            if b"name" in info_dict:
                torrent_name = info_dict[b"name"]
            elif "name" in info_dict:
                torrent_name = info_dict["name"]

            # å¤„ç†ç¼–ç 
            if isinstance(torrent_name, bytes):
                try:
                    torrent_name = torrent_name.decode("utf-8")
                except UnicodeDecodeError:
                    torrent_name = torrent_name.decode("utf-8", errors="ignore")
            elif isinstance(torrent_name, str):
                pass
            else:
                torrent_name = str(torrent_name)
            
            # æ„å»ºç£åŠ›é“¾æ¥
            encoded_name = urlencode({"dn": torrent_name})[3:]
            magnet_link = f"magnet:?xt=urn:btih:{info_hash_hex}&dn={encoded_name}"
            
            logger.debug(f"æˆåŠŸè§£ætorrentæ–‡ä»¶: {torrent_name[:50]}...")
            return magnet_link
            
        except Exception as e:
            logger.error(f"è§£ætorrentæ–‡ä»¶å¤±è´¥ï¼š{e}")
            return None
    
    def crawler_details_batch(self, urls: List[str], use_batch_mode: bool = False) -> List[Dict]:
        """æ‰¹é‡çˆ¬å–è¯¦æƒ…é¡µ - å¯é€‰æ‹©æ‰¹é‡æˆ–å•ä¸ªå¤„ç†æ¨¡å¼"""
        logger.info(f" å¼€å§‹{'æ‰¹é‡' if use_batch_mode else 'å•ä¸ª'}çˆ¬å– {len(urls)} ä¸ªè¯¦æƒ…é¡µ")

        # æ¯æ¬¡æ‰¹é‡çˆ¬å–å¼€å§‹æ—¶é‡ç½®çŠ¶æ€
        self._consecutive_failures = 0
        self._slow_mode = False
        self._error_type_counter = {}  # v1.3.0: é‡ç½®é”™è¯¯ç±»å‹è®¡æ•°å™¨
        self._should_stop_crawling = False  # v1.3.0: é‡ç½®åœæ­¢æ ‡å¿—
        logger.info(f"[CRAWLER] åˆå§‹åŒ–çˆ¬å–çŠ¶æ€ï¼šæ­£å¸¸æ¨¡å¼ï¼Œå»¶è¿Ÿ{self._normal_mode_delay[0]}-{self._normal_mode_delay[1]}ç§’")

        try:
            if use_batch_mode:
                # æ‰¹é‡å¤„ç†æ¨¡å¼
                try:
                    # ä½¿ç”¨å†…éƒ¨å®šä¹‰çš„ BatchProcessor
                    # from fast_crawler import batch_processor (å·²ç§»é™¤)

                    # è·å–é…ç½®çš„çº¿ç¨‹æ•°å’Œå»¶è¿Ÿ
                    from configuration import config_manager
                    max_workers = config_manager.get('CRAWLER_THREAD_COUNT', 10)
                    delay_min = config_manager.get('CRAWLER_SYNC_DELAY_MIN', 0.3)
                    delay_max = config_manager.get('CRAWLER_SYNC_DELAY_MAX', 0.8)
                    
                    # åˆ›å»ºæ–°çš„å¤„ç†å™¨å®ä¾‹ - ä¿®æ­£å‚æ•°ä¼ é€’é”™è¯¯ï¼Œæ˜ç¡®æŒ‡å®š batch_size å’Œ max_workers
                    # åŒæ—¶ä¼ å…¥é…ç½®çš„éšæœºå»¶è¿Ÿï¼Œç¡®ä¿çº¿ç¨‹æ¨¡å¼ä¹Ÿå…·å¤‡é˜²å°èƒ½åŠ›
                    local_batch_processor = BatchProcessor(
                        batch_size=max_workers, 
                        max_workers=max_workers,
                        delay_min=delay_min,
                        delay_max=delay_max
                    )

                    def process_detail_html(url: str, html: str) -> Dict:
                        """å¤„ç†è¯¦æƒ…é¡µHTML - ä½¿ç”¨åŸç‰ˆçš„ç¨³å®šè§£æé€»è¾‘"""
                        return self._parse_detail_html_stable(url, html)

                    # æ‰¹é‡å¤„ç†ï¼Œä½¿ç”¨ä¸åŸç‰ˆç›¸åŒçš„è¯·æ±‚å‚æ•°
                    results = local_batch_processor.process_urls_in_batches(
                        urls,
                        process_detail_html,
                        headers=self.headers,
                        cookies=self.cookie,
                        proxies=self.proxies
                    )

                    # å…³é—­å¤„ç†å™¨
                    local_batch_processor.close()

                    # ç¡®ä¿ç»“æœæ•°é‡ä¸è¾“å…¥URLæ•°é‡ä¸€è‡´ï¼Œå¤±è´¥çš„ä½ç½®ç”¨Noneå¡«å……

                    # ç¡®ä¿ç»“æœæ•°é‡ä¸è¾“å…¥URLæ•°é‡ä¸€è‡´ï¼Œå¤±è´¥çš„ä½ç½®ç”¨Noneå¡«å……
                    if len(results) != len(urls):
                        logger.warning(f"ç»“æœæ•°é‡ä¸åŒ¹é…: è¾“å…¥{len(urls)}ä¸ªURLï¼Œè¿”å›{len(results)}ä¸ªç»“æœ")
                        # è¡¥é½åˆ°ç›¸åŒé•¿åº¦
                        while len(results) < len(urls):
                            results.append(None)

                    # ç»Ÿè®¡æœ‰æ•ˆç»“æœ
                    valid_count = sum(1 for r in results if r and r.get('magnet'))

                    logger.info(f"âœ“ æ‰¹é‡çˆ¬å–å®Œæˆ: æˆåŠŸ {valid_count}/{len(urls)}")

                    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
                    try:
                        stats = local_batch_processor.get_crawler_stats()
                        logger.info(f"[STATS] çˆ¬å–ç»Ÿè®¡: å¹³å‡å“åº”æ—¶é—´ {stats['avg_response_time']:.2f}s, "
                                   f"æˆåŠŸç‡ {stats['successful_requests']}/{stats['total_requests']}")
                    except:
                        pass

                    return results  # è¿”å›å®Œæ•´ç»“æœåˆ—è¡¨ï¼ŒåŒ…å«None

                except Exception as e:
                    logger.error(f"æ‰¹é‡çˆ¬å–å¤±è´¥: {e}")
                    logger.info("é™çº§åˆ°å•ä¸ªå¤„ç†æ¨¡å¼")
                    use_batch_mode = False  # é™çº§åˆ°å•ä¸ªæ¨¡å¼

            if not use_batch_mode:
                # å•ä¸ªå¤„ç†æ¨¡å¼ - æ›´ç¨³å®šå¯é 
                logger.info("ä½¿ç”¨å•ä¸ªå¤„ç†æ¨¡å¼ï¼ˆæ›´ç¨³å®šï¼‰")
                results = []
                success_count = 0

                for i, url in enumerate(urls):
                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢çˆ¬å–
                    if self._should_stop_crawling:
                        logger.error(f"â›” [CRAWLER] æ£€æµ‹åˆ°åœæ­¢æ ‡å¿—ï¼Œå‰©ä½™ {len(urls) - i} ä¸ªURLæœªçˆ¬å–")
                        # å¡«å……å‰©ä½™çš„None
                        results.extend([None] * (len(urls) - i))
                        break

                    try:
                        result = self.crawler_detail(url)
                        if result and result.get('magnet'):
                            results.append(result)
                            success_count += 1
                        else:
                            results.append(None)

                        # è¿›åº¦æ—¥å¿—
                        if (i + 1) % 10 == 0 or (i + 1) == len(urls):
                            logger.info(f"å•ä¸ªå¤„ç†è¿›åº¦: {i + 1}/{len(urls)}, æˆåŠŸ: {success_count}")
                            # æ˜¾ç¤ºå½“å‰æ¨¡å¼
                            mode_info = "æ…¢é€Ÿæ¨¡å¼" if self._slow_mode else "æ­£å¸¸æ¨¡å¼"
                            logger.info(f"[CRAWLER] å½“å‰çˆ¬å–æ¨¡å¼: {mode_info}ï¼Œè¿ç»­å¤±è´¥æ¬¡æ•°: {self._consecutive_failures}")

                        # ä½¿ç”¨è‡ªé€‚åº”å»¶è¿Ÿï¼Œæ ¹æ®å¤±è´¥æƒ…å†µåŠ¨æ€è°ƒæ•´
                        if i < len(urls) - 1:  # æœ€åä¸€ä¸ªä¸éœ€è¦å»¶è¿Ÿ
                            self._adaptive_delay()

                    except Exception as detail_e:
                        logger.warning(f"å•ä¸ªå¤„ç†å¤±è´¥: {url}, é”™è¯¯: {detail_e}")
                        results.append(None)

                logger.info(f"âœ“ å•ä¸ªå¤„ç†å®Œæˆ: æˆåŠŸ {success_count}/{len(urls)}")
                return results
        finally:
            # çˆ¬å–ç»“æŸæ—¶æ¸…ç†Session
            self._close_session()
            logger.debug("çˆ¬å–å®Œæˆï¼Œå·²æ¸…ç†Session")
    
    def _parse_detail_html_stable(self, url: str, html) -> Dict:
        """è§£æè¯¦æƒ…é¡µHTMLå†…å®¹ - åŸºäºåŸç‰ˆsht.pyçš„ç¨³å®šè§£æé€»è¾‘"""
        try:
            import re  # ç¡®ä¿reæ¨¡å—åœ¨æ–¹æ³•å¼€å§‹æ—¶å°±å¯¼å…¥

            # å¤„ç†bytesè¾“å…¥ - è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            if isinstance(html, bytes):
                html = html.decode('utf-8', errors='ignore')

            # åŸºæœ¬æ£€æŸ¥
            if not html or len(html) < 100:
                logger.warning(f"é¡µé¢å†…å®¹è¿‡çŸ­æˆ–ä¸ºç©º: {url}, é•¿åº¦: {len(html) if html else 0}")
                return {}

            # ç§»é™¤ XML/HTML encoding å£°æ˜ï¼ˆpyquery ä¸æ”¯æŒå¸¦å£°æ˜çš„ Unicode å­—ç¬¦ä¸²ï¼‰
            # è¿™ä¿®å¤äº† "Unicode strings with encoding declaration are not supported" é”™è¯¯
            html = re.sub(r'<\?xml[^>]+\?>', '', html)

            # æ£€æŸ¥å¸¸è§é”™è¯¯é¡µé¢ - ä½¿ç”¨åŸç‰ˆçš„æ£€æµ‹æ–¹å¼
            if "æŠ±æ­‰ï¼ŒæŒ‡å®šçš„ä¸»é¢˜ä¸å­˜åœ¨æˆ–å·²è¢«åˆ é™¤æˆ–æ­£åœ¨è¢«å®¡æ ¸" in html:
                logger.warning(f"ä¸»é¢˜ä¸å­˜åœ¨æˆ–å·²åˆ é™¤: {url}")
                return {}

            if "æ‚¨æ— æƒè¿›è¡Œå½“å‰æ“ä½œ" in html:
                logger.warning(f"æ— æƒè®¿é—®é¡µé¢: {url}")
                return {}

            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¹´é¾„éªŒè¯
            if "var safeid" in html:
                logger.warning(f"é¡µé¢éœ€è¦å¹´é¾„éªŒè¯: {url}")
                return {}

            # æ£€æŸ¥é¡µé¢æ ‡é¢˜ç¡®è®¤æœ‰æ•ˆæ€§
            doc = pq(html)
            page_title = doc('head>title').text()
            if "98å ‚" not in page_title:
                logger.warning(f"é¡µé¢æ ‡é¢˜å¼‚å¸¸: {url}, æ ‡é¢˜: {page_title}")
                return {}
            
            # æ£€æŸ¥æ ‡é¢˜
            title = doc('h2.n5_bbsnrbt').text()
            if not title:
                logger.warning(f"æœªæ‰¾åˆ°æ ‡é¢˜: {url}")
                return {}
            
            # ä½¿ç”¨åŸç‰ˆçš„ç²¾ç¡®ç£åŠ›é“¾æ¥æå–æ–¹å¼
            all_text = doc('div.blockcode').text()
            magnet_pattern = r'magnet:\?xt=urn:btih:[0-9a-fA-F]+'
            match = re.search(magnet_pattern, all_text)
            magnet = None
            
            if match:
                magnet = match.group()
                logger.debug(f"æ‰¾åˆ°ç£åŠ›é“¾æ¥: {url}")
            else:
                # å°è¯•æŸ¥æ‰¾torrenté“¾æ¥ - ä¿æŒåŸç‰ˆé€»è¾‘
                torrent = doc("a:contains('.torrent')").eq(0)
                if torrent:
                    torrent_url = torrent.attr('href')
                    logger.debug(f"æ‰¾åˆ°torrenté“¾æ¥ï¼Œå°è¯•è§£æ: {url}")
                    magnet = self.parse_torrent_get_magnet(url, f"https://sehuatang.org/{torrent_url}")
                else:
                    logger.warning(f"æœªæ‰¾åˆ°ç£åŠ›é“¾æ¥æˆ–torrenté“¾æ¥: {url}")
                    logger.debug(f"blockcodeå†…å®¹é•¿åº¦: {len(all_text)}, é¢„è§ˆ: {all_text[:100]}...")
            
            if magnet:
                # æå–å…¶ä»–ä¿¡æ¯ - ä½¿ç”¨åŸç‰ˆçš„å‡½æ•°
                date = extract_exact_datetime(html)
                size = extract_and_convert_video_size(html)
                sub_type = extract_bracket_content(html)
                
                # æ¸…ç†æ ‡é¢˜ - ä¿æŒåŸç‰ˆé€»è¾‘
                pattern = r"^\[.*?\]"
                title = re.sub(pattern, "", title).strip()
                
                # æå–é¢„è§ˆå›¾ç‰‡
                img_elements = doc('div.message img')
                img_src_list = []
                for img in img_elements.items():
                    src = img.attr('src')
                    if src:
                        img_src_list.append(src.strip())
                
                result = {
                    "title": title,
                    "sub_type": sub_type,
                    "publish_date": date,
                    "magnet": magnet,
                    "preview_images": ",".join(img_src_list),
                    "size": size
                }
                
                logger.debug(f"è§£ææˆåŠŸ: {url}, æ ‡é¢˜: {title[:50]}...")
                return result
            else:
                logger.warning(f"é¡µé¢è§£æå¤±è´¥ï¼Œæ— ç£åŠ›é“¾æ¥: {url}")
                return {}
                
        except Exception as e:
            error_msg = str(e)
            # è¯†åˆ«é”™è¯¯ç±»å‹
            if "Unicode strings with encoding declaration" in error_msg:
                error_type = "encoding_declaration_error"
            elif "ValueError" in str(type(e)):
                error_type = "value_error"
            elif "PyQueryError" in str(type(e)) or "etree" in error_msg:
                error_type = "parse_error"
            else:
                error_type = "unknown_error"

            # è®°å½•é”™è¯¯ç±»å‹å¹¶æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
            should_stop = self._record_error_type(error_type)
            if should_stop:
                logger.error(f"è§£æè¯¦æƒ…é¡µå¤±è´¥ ({error_type}): {url}, é”™è¯¯: {e}")
                logger.error(f"â›” å·²åœæ­¢çˆ¬å–ï¼Œé¿å…ç»§ç»­è¯·æ±‚")
            else:
                logger.error(f"è§£æè¯¦æƒ…é¡µå¤±è´¥ ({error_type}): {url}, é”™è¯¯: {e}")
                import traceback
                logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

        return {}

    def save_to_db(self, data, section, tid, detail_url):
        """å°†çˆ¬å–çš„æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå¢åŠ æ•°æ®åº“é”é‡è¯•æœºåˆ¶"""
        # è°ƒç”¨å®é™…çš„ä¿å­˜æ–¹æ³•
        return self._save_to_db(data, tid, section, detail_url)

    @retry_on_lock(max_retries=3, initial_delay=0.5)
    def _save_to_db(self, data: Dict, tid: int, section: str = None, detail_url: str = None) -> bool:
        """ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“ (åŒ…å«è‡ªåŠ¨é‡è¯•æœºåˆ¶)"""
        # å»¶è¿Ÿå¯¼å…¥é˜²æ­¢å¾ªç¯å¼•ç”¨
        from models import db, Resource
        
        try:
            # å…¼å®¹æ€§å¤„ç†ï¼šåˆ›å»ºåº”ç”¨ä¸Šä¸‹æ–‡
            # æ³¨æ„ï¼šç”±äºæˆ‘ä»¬åœ¨ç±»åˆå§‹åŒ–æ—¶å·²ç»æœ‰äº†appï¼Œè¿™é‡Œå¯ä»¥ç›´æ¥ä½¿ç”¨
            # ä½†ä¸ºäº†ä¿é™©èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨ get_flask_app_context
            
            # æ£€æŸ¥éªŒè¯é…ç½®
            # éªŒè¯æ•°æ®
            try:
                # æ‰§è¡Œå•æ¡è®°å½•éªŒè¯
                validation_result = validator._validate_single(tid, detail_url, data)

                if not validation_result['valid']:
                    logger.warning(f"ä¿å­˜å‰éªŒè¯å¤±è´¥: tid={tid}, åŸå› : {', '.join(validation_result['reasons'])}")
                    # é»˜è®¤ç­–ç•¥ï¼šä»…è­¦å‘Šï¼Œç»§ç»­ä¿å­˜
            except Exception as e:
                # éªŒè¯è¿‡ç¨‹ä¸åº”é˜»å¡ä¿å­˜
                logger.warning(f"ä¿å­˜å‰éªŒè¯å¼‚å¸¸: {e}ï¼Œç»§ç»­ä¿å­˜")

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆä¿®å¤ bugï¼šæ­£ç¡®çš„ exists æŸ¥è¯¢ï¼‰
            existing_resource = Resource.query.filter_by(tid=tid).first()
            if existing_resource:
                logger.debug(f"èµ„æºå·²å­˜åœ¨ï¼Œè·³è¿‡: tid={tid}")
                return False
            
            # æ•°æ®æ¸…ç†å’Œæ ‡å‡†åŒ–
            title = data.get('title', '').strip()[:500]  # é™åˆ¶é•¿åº¦å¹¶å»é™¤ç©ºç™½
            sub_type = data.get('sub_type', '').strip()[:200] if data.get('sub_type') else None
            publish_date = self._normalize_date(data.get('publish_date', ''))
            
            # ğŸ” TIDä¸æ ‡é¢˜çš„æœ€ç»ˆä¸€è‡´æ€§æ£€æŸ¥
            if detail_url and f"tid={tid}" not in detail_url:
                logger.warning(f"! TIDä¸URLä¸åŒ¹é…: tid={tid}, url={detail_url}")
            
            # åˆ›å»ºæ–°èµ„æºè®°å½•
            resource = Resource(
                title=title,
                sub_type=sub_type,
                publish_date=publish_date,
                magnet=data.get('magnet'),
                preview_images=data.get('preview_images'),
                size=data.get('size'),
                tid=tid,
                section=section[:100] if section else None,
                detail_url=detail_url[:500] if detail_url else None
            )
            
            # ä½¿ç”¨addæ·»åŠ 
            db.session.add(resource)
            db.session.commit()

            # æ¸…é™¤ç›¸å…³ç¼“å­˜
            try:
                from cache_manager import cache_manager, CacheKeys
                cache_manager.delete(CacheKeys.STATS)  # æ¸…é™¤ç»Ÿè®¡ç¼“å­˜
                cache_manager.delete(CacheKeys.CATEGORIES)  # æ¸…é™¤åˆ†ç±»ç¼“å­˜
                logger.debug(f"å·²æ¸…é™¤ç›¸å…³ç¼“å­˜: tid={tid}")
            except Exception as e:
                logger.warning(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")

            logger.info(f"âœ“ æˆåŠŸä¿å­˜èµ„æº: tid={tid}, title={title[:50]}...")
            return True
            
        except ImportError:
            # å¿½ç•¥å¯¼å…¥é”™è¯¯
            pass
        except Exception as e:
            # å…³é”®ï¼šå¦‚æœæ˜¯æ•°æ®åº“é”å®šé”™è¯¯ï¼Œé‡æ–°æŠ›å‡ºè®©è£…é¥°å™¨å¤„ç†
            if "database is locked" in str(e).lower():
                raise e
                
            # å…¶ä»–é”™è¯¯è®°å½•å¹¶è¿”å›å¤±è´¥
            logger.error(f"ä¿å­˜èµ„æºåˆ°æ•°æ®åº“å¤±è´¥: tid={tid}, é”™è¯¯: {e}")
            try:
                db.session.rollback()
            except:
                pass
            return False
    
    def _normalize_date(self, date_str):
        """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼"""
        if not date_str:
            return None
        
        # å¦‚æœå·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼Œç›´æ¥è¿”å›
        if len(date_str) == 10 and date_str.count('-') == 2:
            return date_str
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šæ—¥æœŸæ ¼å¼è½¬æ¢é€»è¾‘
        # ä¾‹å¦‚ï¼šå°† "2024å¹´1æœˆ1æ—¥" è½¬æ¢ä¸º "2024-01-01"
        
        return date_str[:20]  # é™åˆ¶é•¿åº¦
