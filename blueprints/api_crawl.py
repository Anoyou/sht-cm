#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ¬è™«API Blueprint - å¤„ç†çˆ¬è™«æ§åˆ¶ã€è®ºå›ä¿¡æ¯ã€å¥åº·æ£€æŸ¥å’ŒSHT2BMç›¸å…³çš„API
"""

import os
import json
import logging
import threading
import time
from datetime import datetime, timezone, timedelta
from flask import Blueprint, jsonify, request, current_app
import concurrent.futures

from models import db, Category, Resource
from configuration import Config
from cache_manager import cache_manager, CacheKeys
from utils.api_response import (
    success_response,
    error_response,
    ErrorCode,
    missing_parameter_response,
    invalid_request_response,
    invalid_parameter_response,
    not_found_response,
    operation_failed_response,
    operation_in_progress_response,
    log_api_call,
    log_error_with_traceback
)

api_crawl_bp = Blueprint('api_crawl', __name__, url_prefix='/api')

# çˆ¬è™«å¯åŠ¨äº’æ–¥é”ï¼Œé˜²æ­¢å¹¶å‘å¯åŠ¨
_crawl_start_lock = threading.Lock()

logger = logging.getLogger(__name__)


# ==================== çˆ¬è™«æ§åˆ¶API ====================

@api_crawl_bp.route('/crawl/status')
def api_crawl_status():
    """è·å–çˆ¬è™«çŠ¶æ€ - å¤šè¿›ç¨‹å®‰å…¨ç‰ˆæœ¬ï¼Œç›´æ¥è¯»å–çŠ¶æ€æ–‡ä»¶"""
    # ä¿®å¤ï¼šç§»é™¤ç¼“å­˜ï¼Œç›´æ¥ä»çŠ¶æ€åè°ƒå™¨è·å–æœ€æ–°çŠ¶æ€
    # è¿™æ ·å¯ä»¥ç¡®ä¿å¤šä¸ª worker è¿›ç¨‹çœ‹åˆ°ä¸€è‡´çš„çŠ¶æ€
    try:
        from crawler_control.cc_control_bridge import get_crawler_control_bridge
        bridge = get_crawler_control_bridge()

        # å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶ä»æ–‡ä»¶é‡æ–°åŠ è½½çŠ¶æ€ï¼Œç¡®ä¿å¤šè¿›ç¨‹ä¸€è‡´æ€§
        coordinator_state = bridge.get_current_state(force_reload=True)

        # æ£€æŸ¥ä¿¡å·é˜Ÿåˆ—ä¸­æ˜¯å¦æœ‰å¾…å¤„ç†çš„ä¿¡å·
        # å¦‚æœæœ‰ï¼Œè¿”å›å¯¹åº”çš„"è¿‡æ¸¡çŠ¶æ€"ï¼Œé¿å…å‰ç«¯çŠ¶æ€è·³åŠ¨
        pending_signals = bridge.queue_manager.get_pending_signals()

        # åˆ¤æ–­æ˜¯å¦æœ‰å¾…å¤„ç†çš„æš‚åœ/åœæ­¢/æ¢å¤ä¿¡å·
        has_pending_pause = any(s.type == 'pause' and not s.processed for s in pending_signals)
        has_pending_stop = any(s.type == 'stop' and not s.processed for s in pending_signals)
        has_pending_resume = any(s.type == 'resume' and not s.processed for s in pending_signals)

        # éªŒè¯ä¿¡å·ä¸å½“å‰çŠ¶æ€çš„ä¸€è‡´æ€§ï¼Œé¿å…æ˜¾ç¤ºè¿‡æ—¶çš„pendingä¿¡å·
        # å¦‚æœå½“å‰çŠ¶æ€å·²ç»æ˜¯ç›®æ ‡çŠ¶æ€ï¼Œå¿½ç•¥å¯¹åº”çš„pendingä¿¡å·
        if has_pending_pause and coordinator_state['is_paused']:
            # å½“å‰å·²ç»æš‚åœäº†ï¼Œä¸å†æ˜¾ç¤º"æ­£åœ¨æš‚åœ"
            has_pending_pause = False
        if has_pending_resume and coordinator_state['is_crawling'] and not coordinator_state['is_paused']:
            # å½“å‰å·²ç»æ¢å¤è¿è¡Œäº†ï¼Œä¸å†æ˜¾ç¤º"æ­£åœ¨æ¢å¤"
            has_pending_resume = False
        if has_pending_stop and not coordinator_state['is_crawling'] and not coordinator_state['is_paused']:
            # å½“å‰å·²ç»åœæ­¢äº†ï¼Œä¸å†æ˜¾ç¤º"æ­£åœ¨åœæ­¢"
            has_pending_stop = False

        # ç¡®å®šæœ€ç»ˆçŠ¶æ€ï¼ˆä¼˜å…ˆçº§ï¼šstop > pause > resume > å½“å‰çŠ¶æ€ï¼‰
        if has_pending_stop:
            # æœ‰å¾…å¤„ç†çš„åœæ­¢ä¿¡å· - æ— è®ºå½“å‰çŠ¶æ€å¦‚ä½•ï¼Œéƒ½åº”æ˜¾ç¤ºæ­£åœ¨åœæ­¢
            # åœæ­¢æ—¶ä¸åº”æ˜¾ç¤ºä¸ºæš‚åœçŠ¶æ€ï¼Œé¿å…æ··æ·†
            is_crawling = coordinator_state['is_crawling'] if coordinator_state['current_state'] == 'running' else False
            is_paused = False  # åœæ­¢è¿‡ç¨‹ä¸­ä¸åº”æ ‡è®°ä¸ºæš‚åœ
            message = 'æ­£åœ¨åœæ­¢...'
        elif has_pending_pause:
            # æœ‰å¾…å¤„ç†çš„æš‚åœä¿¡å·
            # æš‚åœæ„å‘³ç€ä¸å†çˆ¬å–ï¼Œæ‰€ä»¥ is_crawling åº”ä¸º False
            is_crawling = False  # æš‚åœæ—¶ä¸åº”æ˜¾ç¤ºä¸ºçˆ¬å–ä¸­
            is_paused = True     # æå‰æ ‡è®°ä¸ºæš‚åœï¼Œé¿å…è·³åŠ¨
            message = 'æ­£åœ¨æš‚åœ...'
        elif has_pending_resume:
            # æœ‰å¾…å¤„ç†çš„æ¢å¤ä¿¡å·
            is_crawling = True  # å³å°†æ¢å¤è¿è¡Œ
            is_paused = False   # æå‰æ ‡è®°ä¸ºéæš‚åœ
            message = 'æ­£åœ¨æ¢å¤...'
        else:
            # æ²¡æœ‰å¾…å¤„ç†ä¿¡å·ï¼Œä½¿ç”¨å½“å‰çŠ¶æ€
            is_crawling = coordinator_state['is_crawling']
            is_paused = coordinator_state['is_paused']
            if is_crawling:
                message = 'æ­£åœ¨çˆ¬å–'
            elif is_paused:
                message = 'å·²æš‚åœ'
            else:
                message = 'ç©ºé—²'

        # ä½¿ç”¨çŠ¶æ€åè°ƒå™¨çš„çŠ¶æ€ä½œä¸ºæƒå¨æ¥æº
        crawl_status = {
            'is_crawling': is_crawling,
            'is_paused': is_paused,
            'message': message
        }
        crawl_progress = coordinator_state.get('progress', {})

        # v1.5.4: åªåœ¨çŠ¶æ€å˜åŒ–æ—¶è¾“å‡ºæ—¥å¿—ï¼Œä¸”ä»…åœ¨çˆ¬è™«æ´»è·ƒæ—¶è®°å½•
        last_state_key = "last_crawl_status"
        last_state = cache_manager.shared_get(last_state_key)
        current_state_tuple = (crawl_status['is_crawling'], crawl_status['is_paused'])
        
        # åªåœ¨ä»¥ä¸‹æƒ…å†µè®°å½•æ—¥å¿—ï¼š
        # 1. çŠ¶æ€ç¡®å®å‘ç”Ÿäº†å˜åŒ–
        # 2. çˆ¬è™«å¤„äºæ´»è·ƒçŠ¶æ€ï¼ˆæ­£åœ¨è¿è¡Œæˆ–æš‚åœä¸­ï¼‰
        if not last_state or list(last_state) != list(current_state_tuple):
            if crawl_status['is_crawling'] or crawl_status['is_paused']:
                logger.debug(f"[STATUS] çŠ¶æ€å˜åŒ–: is_crawling={crawl_status['is_crawling']}, is_paused={crawl_status['is_paused']}")
            cache_manager.shared_set(last_state_key, current_state_tuple, ttl=60)

    except Exception as e:
        logger.warning(f"[STATUS] æ— æ³•ä»çŠ¶æ€åè°ƒå™¨è·å–çŠ¶æ€ï¼Œé™çº§åˆ° app.config: {e}")
        # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨ app.config
        crawl_status = current_app.config.get('CRAWL_STATUS', {})
        crawl_progress = current_app.config.get('CRAWL_PROGRESS', {})
    

    SUMMARY_FILE = current_app.config.get('SUMMARY_FILE', '')

    # åªä½¿ç”¨çŠ¶æ€åè°ƒå™¨ä½œä¸ºå”¯ä¸€æƒå¨æº
    # çŠ¶æ€æ–‡ä»¶è¯»å–å¾ˆå¿«ï¼ˆå‡ æ¯«ç§’ï¼‰ï¼Œä¸ä¼šå½±å“æ€§èƒ½

    # æ„å»ºçŠ¶æ€æ•°æ®
    status_data = {
        **crawl_status,
        'progress': crawl_progress if crawl_status.get('is_crawling') else None,
        'timestamp': datetime.now(timezone.utc).isoformat()
    }

    # å¦‚æœæœ‰æœ€è¿‘çš„çˆ¬å–ç»“æœï¼ŒåŒ…å«åœ¨çŠ¶æ€ä¸­
    if not crawl_status.get('is_crawling'):
        try:
            if os.path.exists(SUMMARY_FILE):
                with open(SUMMARY_FILE, 'r', encoding='utf-8') as f:
                    summary_data = json.load(f)
                    # åªè¿”å›æœ€è¿‘çš„ç»“æœæ‘˜è¦
                    if summary_data:
                        results = summary_data.get('results', {})
                        duration = summary_data.get('duration', {})
                        status_data['last_result'] = {
                            'total_saved': results.get('total_saved', 0),
                            'total_skipped': results.get('total_skipped', 0),
                            'total_time': duration.get('formatted', '0ç§’'),
                            'duration_seconds': duration.get('total_seconds', 0),
                            'sections': summary_data.get('per_section_results', {}),
                            'timestamp': summary_data.get('timestamp', ''),
                            'success_rate': results.get('success_rate', 0),
                            'page_statistics': summary_data.get('page_statistics', {}),
                            'section_page_breakdown': summary_data.get('section_page_breakdown', {}),
                            'metadata': summary_data.get('engine_set', {})  # æ·»åŠ å¼•æ“é…ç½®ä¿¡æ¯
                        }
        except Exception as e:
            logger.error(f"âœ— [LOG] è¯»å–æ‘˜è¦æ–‡ä»¶å¤±è´¥: {e}")
            pass

    # ä¿®å¤ï¼šç§»é™¤ç¼“å­˜ï¼Œç¡®ä¿å¤šè¿›ç¨‹ç¯å¢ƒä¸‹çŠ¶æ€å®æ—¶ä¸€è‡´
    # æ¯æ¬¡éƒ½è¿”å›æœ€æ–°çŠ¶æ€ï¼Œé¿å… worker è¿›ç¨‹é—´çŠ¶æ€ä¸åŒæ­¥

    return jsonify(status_data)


@api_crawl_bp.route('/crawl/start', methods=['POST'])
def api_crawl_start():
    """æ‰‹åŠ¨è§¦å‘çˆ¬è™«ä»»åŠ¡ï¼ˆå¸¦å¹¶å‘å¯åŠ¨äº’æ–¥é”ï¼‰"""
    start_time = time.time()

    crawl_status = current_app.config.get('CRAWL_STATUS', {})
    LOG_DIR = current_app.config.get('LOG_DIR', '')
    OPTIONS_FILE = current_app.config.get('OPTIONS_FILE', '')

    logger.info(f"[CRAWLER] æ”¶åˆ°çˆ¬è™«å¯åŠ¨è¯·æ±‚")

    # ä½¿ç”¨äº’æ–¥é”é˜²æ­¢å¹¶å‘å¯åŠ¨
    if not _crawl_start_lock.acquire(blocking=False):
        logger.warning(f"[CRAWLER] æ£€æµ‹åˆ°å¹¶å‘å¯åŠ¨è¯·æ±‚ï¼Œæ‹’ç»å¯åŠ¨")

        duration_ms = (time.time() - start_time) * 1000

        log_api_call(
            logger,
            method='POST',
            endpoint='/api/crawl/start',
            params={},
            status='error',
            response_code=429,
            duration_ms=duration_ms,
            error='çˆ¬è™«æ­£åœ¨å¯åŠ¨ä¸­ï¼Œè¯·ç¨åå†è¯•'
        )

        return operation_in_progress_response('çˆ¬è™«æ­£åœ¨å¯åŠ¨ä¸­ï¼Œè¯·ç¨åå†è¯•')

    try:
        # ä½¿ç”¨åŸå­æ“ä½œæ£€æŸ¥å¹¶è®¾ç½®çŠ¶æ€ï¼Œé˜²æ­¢ç«æ€æ¡ä»¶
        if crawl_status.get('is_crawling'):
            logger.warning(f"[CRAWLER] çˆ¬è™«ä»»åŠ¡å·²åœ¨è¿›è¡Œä¸­ï¼Œæ‹’ç»å¯åŠ¨")
            _crawl_start_lock.release()

            duration_ms = (time.time() - start_time) * 1000

            log_api_call(
                logger,
                method='POST',
                endpoint='/api/crawl/start',
                params={},
                status='error',
                response_code=400,
                duration_ms=duration_ms,
                error='çˆ¬è™«ä»»åŠ¡æ­£åœ¨è¿›è¡Œä¸­'
            )

            return error_response(
                code=ErrorCode.BUSINESS_ERROR,
                message='çˆ¬è™«ä»»åŠ¡æ­£åœ¨è¿›è¡Œä¸­'
            )

        # ç«‹å³è®¾ç½®çŠ¶æ€ä¸º Trueï¼Œé˜²æ­¢é‡å¤ç‚¹å‡»
        crawl_status['is_crawling'] = True
        crawl_status['message'] = 'æ­£åœ¨åˆå§‹åŒ–...'

        # åŒæ­¥åˆ°å…±äº«çŠ¶æ€
        cache_manager.shared_set(CacheKeys.CRAWL_STATUS, crawl_status)
        cache_manager.shared_set(CacheKeys.CRAWL_PROGRESS, current_app.config.get('CRAWL_PROGRESS', {}))

    except Exception as e:
        logger.error(f"[CRAWLER] å¯åŠ¨å‰è®¾ç½®çŠ¶æ€å¤±è´¥: {e}")
        _crawl_start_lock.release()

        duration_ms = (time.time() - start_time) * 1000

        log_error_with_traceback(
            logger,
            e,
            context={'endpoint': '/api/crawl/start'},
            message='å¯åŠ¨å‰è®¾ç½®çŠ¶æ€å¤±è´¥'
        )

        log_api_call(
            logger,
            method='POST',
            endpoint='/api/crawl/start',
            params={},
            status='error',
            response_code=500,
            duration_ms=duration_ms,
            error=str(e)
        )

        return error_response(
            code=ErrorCode.INTERNAL_ERROR,
            message=f'å¯åŠ¨å¤±è´¥: {str(e)}'
        )
    finally:
        # ç«‹å³é‡Šæ”¾é”ï¼Œå…è®¸åç»­è¯·æ±‚
        _crawl_start_lock.release()

    payload = request.get_json(silent=True) or {}
    section_fids = payload.get('fids') or None
    date_mode = payload.get('date_mode') or 'all'
    date_value = payload.get('date_value') or None
    dateline = payload.get('dateline') or None
    max_pages = int(payload.get('max_pages') or 3)
    page_mode = payload.get('page_mode') or 'smart'
    page_range = payload.get('page_range') or None  # æ–°å¢ï¼šé¡µç èŒƒå›´ [start, end]

    logger.info(f"[CRAWLER] çˆ¬è™«å‚æ•°: fids={section_fids}, date_mode={date_mode}, max_pages={max_pages}")

    # åœ¨å¯åŠ¨çº¿ç¨‹å‰è·å– app å¼•ç”¨å’Œé…ç½®è·¯å¾„
    app = current_app._get_current_object()

    def run_task():
        """çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå‡½æ•° - åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œ"""
        try:
            with app.app_context():
                # è·å–å…¨å±€å˜é‡
                crawl_status = app.config.get('CRAWL_STATUS', {})
                crawl_control = app.config.get('CRAWL_CONTROL', {})
                crawl_progress = app.config.get('CRAWL_PROGRESS', {})
                LOG_DIR = app.config.get('LOG_DIR', '')
                OPTIONS_FILE = app.config.get('OPTIONS_FILE', '')

                logger.info(f"[CRAWLER] çº¿ç¨‹å†…éƒ¨å¼€å§‹æ‰§è¡Œ")

                crawl_control['paused'] = False
                crawl_control['stop'] = False
                crawl_progress.update({
                    'sections_total': 0,
                    'sections_done': 0,
                    'current_section': '',
                    'current_section_pages': 0,
                    'current_section_processed': 0,
                    'current_page': 0,
                    'max_pages': max_pages,
                    'processed_pages': 0,
                    'total_saved': 0,
                    'total_skipped': 0,
                    'current_section_saved': 0,
                    'current_section_skipped': 0,
                    'estimated_total_pages': 0
                })
                crawl_status['is_crawling'] = True

                # æ„å»ºæ›´å‹å¥½çš„çŠ¶æ€æ¶ˆæ¯
                def format_crawl_message(fids, date_mode, _date_value, dateline, max_pages):
                    # FIDåˆ°åˆ†ç±»åç§°çš„æ˜ å°„
                    from constants import SECTION_MAP

                    # æ ¼å¼åŒ–åˆ†ç±»åç§°
                    if fids and len(fids) > 0:
                        category_names = [SECTION_MAP.get(fid, f"æ¿å—{fid}") for fid in fids]
                        if len(category_names) == 1:
                            category_str = category_names[0]
                        elif len(category_names) <= 3:
                            category_str = "ã€".join(category_names)
                        else:
                            category_str = f"{category_names[0]}ç­‰{len(category_names)}ä¸ªæ¿å—"
                    else:
                        category_str = "å…¨éƒ¨æ¿å—"

                    # æ ¼å¼åŒ–æ—¶é—´èŒƒå›´
                    time_ranges = {
                        'all': 'å…¨éƒ¨æ—¶é—´',
                        'day': 'ä¸€å¤©å†…', '1day': 'ä¸€å¤©å†…',
                        '2day': 'ä¸¤å¤©å†…',
                        '3day': 'ä¸‰å¤©å†…',
                        'week': 'ä¸€å‘¨å†…', '1week': 'ä¸€å‘¨å†…',
                        'month': 'ä¸€ä¸ªæœˆå†…', '1month': 'ä¸€ä¸ªæœˆå†…',
                        '3month': 'ä¸‰ä¸ªæœˆå†…',
                        '6month': 'åŠå¹´å†…',
                        'year': 'ä¸€å¹´å†…', '1year': 'ä¸€å¹´å†…'
                    }

                    if date_mode and date_mode != 'all':
                        time_str = time_ranges.get(date_mode, f'{date_mode}å†…')
                    elif dateline:
                        time_str = f'æŒ‡å®šæ—¶é—´èŒƒå›´'
                    else:
                        time_str = 'å…¨éƒ¨æ—¶é—´'

                    return f'æ­£åœ¨çˆ¬å– {category_str} - {time_str} - {max_pages}é¡µ'

                crawl_status['message'] = format_crawl_message(section_fids, date_mode, date_value, dateline, max_pages)
                logger.info(f"[CRAWLER] å‡†å¤‡å¯åŠ¨çˆ¬è™«: {crawl_status['message']}")

                try:
                    # å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–
                    logger.info(f"[CRAWLER] å¯¼å…¥ crawler_scheduler æ¨¡å—")
                    from scheduler.core import run_crawling_with_options

                    logger.info(f"[CRAWLER] ä¿å­˜çˆ¬è™«é…ç½®åˆ°: {OPTIONS_FILE}")
                    try:
                        os.makedirs(LOG_DIR, exist_ok=True)
                        with open(OPTIONS_FILE, 'w', encoding='utf-8') as f:
                            json.dump({
                                'fids': section_fids,
                                'date_mode': date_mode,
                                'date_value': date_value,
                                'dateline': dateline,
                                'max_pages': max_pages,
                                'page_mode': page_mode,
                                'page_range': page_range # ä¿å­˜é¡µç èŒƒå›´
                            }, f, ensure_ascii=False)
                        logger.info(f"[CRAWLER] é…ç½®ä¿å­˜æˆåŠŸ")
                    except Exception as e:
                        logger.warning(f"[CRAWLER] é…ç½®ä¿å­˜å¤±è´¥: {e}")

                    logger.info(f"[CRAWLER] å¼€å§‹æ‰§è¡Œçˆ¬è™«ä»»åŠ¡")
                    run_crawling_with_options(section_fids=section_fids, date_mode=date_mode, date_value=date_value, dateline=dateline, max_pages=max_pages, page_mode=page_mode, page_range=page_range, task_type='manual')
                    crawl_status['message'] = 'çˆ¬å–å®Œæˆ'
                    crawl_status['last_crawl_time'] = datetime.now().isoformat()
                    logger.info(f"[CRAWLER] çˆ¬è™«ä»»åŠ¡å®Œæˆ")
                except Exception as e:
                    import traceback
                    logger.error(f"âœ— [CRAWLER] çˆ¬å–å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
                    crawl_status['message'] = f'çˆ¬å–å‡ºé”™: {str(e)}'
                finally:
                    crawl_status['is_crawling'] = False
                    crawl_status['is_paused'] = False  # ç¡®ä¿æ¸…é™¤æš‚åœæ ‡å¿—
                    logger.info(f"[CRAWLER] çˆ¬è™«ä»»åŠ¡å®Œæˆ")

                    # é‡ç½®çŠ¶æ€åè°ƒå™¨åˆ°ç©ºé—²çŠ¶æ€
                    try:
                        from crawler_control.cc_control_bridge import get_crawler_control_bridge
                        bridge = get_crawler_control_bridge()
                        bridge.reset_to_idle()
                        logger.info(f"[CRAWLER] çŠ¶æ€åè°ƒå™¨å·²é‡ç½®åˆ°ç©ºé—²çŠ¶æ€")
                    except Exception as reset_err:
                        logger.warning(f"[CRAWLER] é‡ç½®çŠ¶æ€åè°ƒå™¨å¤±è´¥: {reset_err}")

                    # æ¸…é™¤å…±äº«çŠ¶æ€ä¸­çš„æš‚åœæ ‡å¿—
                    try:
                        crawl_control = app.config.get('CRAWL_CONTROL', {})
                        crawl_control['paused'] = False
                        crawl_control['stop'] = False
                        cache_manager.shared_set(CacheKeys.CRAWL_CONTROL, crawl_control)
                        cache_manager.shared_set(CacheKeys.CRAWL_STATUS, crawl_status)
                    except Exception as cache_err:
                        logger.warning(f"[CRAWLER] æ¸…é™¤å…±äº«çŠ¶æ€å¤±è´¥: {cache_err}")

        except Exception as e:
            import traceback
            logger.error(f"âœ— [CRAWLER] çº¿ç¨‹æ‰§è¡Œå¤±è´¥: {traceback.format_exc()}")

            # å‘é€å¼‚å¸¸ç»ˆæ­¢é€šçŸ¥
            try:
                from scheduler.notifier import _send_telegram_message, render_message_template
                error_msg, parse_mode = render_message_template('crawler_thread_error', {
                    'error_type': 'çº¿ç¨‹æ‰§è¡Œå¤±è´¥',
                    'error_message': str(e)[:200],
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'suggestion': 'æ£€æŸ¥æ—¥å¿—è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯'
                })
                if not error_msg:
                    error_msg = f"""âŒ *çˆ¬è™«ä»»åŠ¡å¼‚å¸¸ç»ˆæ­¢*
â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ é”™è¯¯ç±»å‹ï¼šçº¿ç¨‹æ‰§è¡Œå¤±è´¥
ğŸ“ é”™è¯¯ä¿¡æ¯ï¼š{str(e)[:200]}
â° ç»ˆæ­¢æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥æ—¥å¿—è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯"""
                    parse_mode = 'Markdown'
                _send_telegram_message(error_msg, parse_mode=parse_mode)
            except Exception as notify_err:
                logger.error(f"å‘é€å¼‚å¸¸é€šçŸ¥å¤±è´¥: {notify_err}")

            # æ¸…ç†çŠ¶æ€
            try:
                crawl_status = app.config.get('CRAWL_STATUS', {})
                crawl_status['is_crawling'] = False
                crawl_status['message'] = f'ä»»åŠ¡å¼‚å¸¸ç»ˆæ­¢: {str(e)[:100]}'

                from crawler_control.cc_control_bridge import get_crawler_control_bridge
                bridge = get_crawler_control_bridge()
                bridge.coordinator.reset_state()
            except Exception as cleanup_err:
                logger.error(f"æ¸…ç†çŠ¶æ€å¤±è´¥: {cleanup_err}")

    thread = threading.Thread(target=run_task)
    thread.daemon = False  # édaemonçº¿ç¨‹ï¼Œé˜²æ­¢é™é»˜ç»ˆæ­¢
    thread.start()

    logger.info(f"[CRAWLER] çˆ¬è™«çº¿ç¨‹å·²å¯åŠ¨ï¼Œçº¿ç¨‹ID: {thread.name}")

    duration_ms = (time.time() - start_time) * 1000

    log_api_call(
        logger,
        method='POST',
        endpoint='/api/crawl/start',
        params={'fids': section_fids, 'date_mode': date_mode, 'max_pages': max_pages},
        status='success',
        response_code=200,
        duration_ms=duration_ms
    )

    return success_response(
        message='çˆ¬è™«ä»»åŠ¡å·²å¯åŠ¨'
    )


@api_crawl_bp.route('/crawl/stop', methods=['POST'])
def api_crawl_stop():
    """åœæ­¢çˆ¬è™«ä»»åŠ¡"""
    start_time = time.time()
    from crawler_control.cc_control_bridge import get_crawler_control_bridge

    crawl_status = current_app.config.get('CRAWL_STATUS', {})

    if not crawl_status.get('is_crawling'):
        duration_ms = (time.time() - start_time) * 1000

        log_api_call(
            logger,
            method='POST',
            endpoint='/api/crawl/stop',
            params={},
            status='error',
            response_code=400,
            duration_ms=duration_ms,
            error='å½“å‰æ²¡æœ‰æ­£åœ¨è¿è¡Œçš„çˆ¬è™«ä»»åŠ¡'
        )

        return error_response(
            code=ErrorCode.BUSINESS_ERROR,
            message='å½“å‰æ²¡æœ‰æ­£åœ¨è¿è¡Œçš„çˆ¬è™«ä»»åŠ¡'
        )

    try:
        # ä½¿ç”¨æ–°çš„ä¿¡å·é˜Ÿåˆ—ç³»ç»Ÿ
        bridge = get_crawler_control_bridge()
        signal_id = bridge.send_stop_signal()

        # æ›´æ–°çŠ¶æ€æ˜¾ç¤º
        crawl_status['message'] = 'æ­£åœ¨åœæ­¢...'

        # åŒæ­¥åˆ°å…±äº«çŠ¶æ€ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        crawl_control = current_app.config.get('CRAWL_CONTROL', {})
        crawl_control['stop'] = True
        crawl_control['paused'] = False
        cache_manager.shared_set(CacheKeys.CRAWL_CONTROL, crawl_control)
        cache_manager.shared_set(CacheKeys.CRAWL_STATUS, crawl_status)

        logger.info(f"[CRAWLER] æ”¶åˆ°åœæ­¢è¯·æ±‚ï¼Œä¿¡å·ID: {signal_id}")

        duration_ms = (time.time() - start_time) * 1000

        log_api_call(
            logger,
            method='POST',
            endpoint='/api/crawl/stop',
            params={},
            status='success',
            response_code=200,
            duration_ms=duration_ms
        )

        return success_response(
            data={'signal_id': signal_id},
            message='åœæ­¢æŒ‡ä»¤å·²å‘é€'
        )
    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000

        log_error_with_traceback(
            logger,
            e,
            context={'endpoint': '/api/crawl/stop'},
            message='å‘é€åœæ­¢ä¿¡å·å¤±è´¥'
        )

        log_api_call(
            logger,
            method='POST',
            endpoint='/api/crawl/stop',
            params={},
            status='error',
            response_code=500,
            duration_ms=duration_ms,
            error=str(e)
        )

        return error_response(
            code=ErrorCode.INTERNAL_ERROR,
            message='å‘é€åœæ­¢ä¿¡å·å¤±è´¥',
            details=str(e)
        )


@api_crawl_bp.route('/crawl/pause', methods=['POST'])
def api_crawl_pause():
    """æš‚åœçˆ¬è™«ä»»åŠ¡"""
    start_time = time.time()

    try:
        from crawler_control.cc_control_bridge import get_crawler_control_bridge

        crawl_status = current_app.config.get('CRAWL_STATUS', {})

        if not crawl_status.get('is_crawling'):
            duration_ms = (time.time() - start_time) * 1000

            log_api_call(
                logger,
                method='POST',
                endpoint='/api/crawl/pause',
                params={},
                status='error',
                response_code=400,
                duration_ms=duration_ms,
                error='å½“å‰æ²¡æœ‰æ­£åœ¨è¿è¡Œçš„çˆ¬è™«ä»»åŠ¡'
            )

            return error_response(
                code=ErrorCode.BUSINESS_ERROR,
                message='å½“å‰æ²¡æœ‰æ­£åœ¨è¿è¡Œçš„çˆ¬è™«ä»»åŠ¡'
            )

        # ä½¿ç”¨æ–°çš„ä¿¡å·é˜Ÿåˆ—ç³»ç»Ÿ
        bridge = get_crawler_control_bridge()

        # æ£€æŸ¥å½“å‰çŠ¶æ€ï¼Œé¿å…é‡å¤æš‚åœ
        try:
            current_state = bridge.get_current_state()
            if current_state.get('is_paused'):
                logger.info(f"[CRAWLER] çˆ¬è™«å·²ç»å¤„äºæš‚åœçŠ¶æ€ï¼Œå¿½ç•¥é‡å¤è¯·æ±‚")

                duration_ms = (time.time() - start_time) * 1000

                log_api_call(
                    logger,
                    method='POST',
                    endpoint='/api/crawl/pause',
                    params={'note': 'å·²æš‚åœ'},
                    status='success',
                    response_code=200,
                    duration_ms=duration_ms
                )

                return success_response(
                    data={'already_paused': True},
                    message='çˆ¬è™«å·²å¤„äºæš‚åœçŠ¶æ€'
                )
        except Exception as state_err:
            logger.warning(f"[CRAWLER] è·å–å½“å‰çŠ¶æ€å¤±è´¥: {state_err}ï¼Œç»§ç»­å‘é€æš‚åœä¿¡å·")

        signal_id = bridge.send_pause_signal()

        # æ›´æ–°çŠ¶æ€æ˜¾ç¤º
        crawl_status['message'] = 'æ­£åœ¨æš‚åœ...'

        # åŒæ­¥åˆ°å…±äº«çŠ¶æ€ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        crawl_control = current_app.config.get('CRAWL_CONTROL', {})
        crawl_control['paused'] = True
        cache_manager.shared_set(CacheKeys.CRAWL_CONTROL, crawl_control)
        cache_manager.shared_set(CacheKeys.CRAWL_STATUS, crawl_status)

        duration_ms = (time.time() - start_time) * 1000

        log_api_call(
            logger,
            method='POST',
            endpoint='/api/crawl/pause',
            params={},
            status='success',
            response_code=200,
            duration_ms=duration_ms
        )

        return success_response(
            data={'signal_id': signal_id},
            message='æš‚åœæŒ‡ä»¤å·²å‘é€'
        )

    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000

        log_error_with_traceback(
            logger,
            e,
            context={'endpoint': '/api/crawl/pause'},
            message='å‘é€æš‚åœä¿¡å·å¤±è´¥'
        )

        log_api_call(
            logger,
            method='POST',
            endpoint='/api/crawl/pause',
            params={},
            status='error',
            response_code=500,
            duration_ms=duration_ms,
            error=str(e)
        )

        return error_response(
            code=ErrorCode.INTERNAL_ERROR,
            message='å‘é€æš‚åœä¿¡å·å¤±è´¥',
            details=str(e)
        )


@api_crawl_bp.route('/crawl/resume', methods=['POST'])
def api_crawl_resume():
    """æ¢å¤çˆ¬è™«ä»»åŠ¡"""
    start_time = time.time()
    from crawler_control.cc_control_bridge import get_crawler_control_bridge

    crawl_status = current_app.config.get('CRAWL_STATUS', {})

    if not crawl_status.get('is_crawling'):
        duration_ms = (time.time() - start_time) * 1000

        log_api_call(
            logger,
            method='POST',
            endpoint='/api/crawl/resume',
            params={},
            status='error',
            response_code=400,
            duration_ms=duration_ms,
            error='å½“å‰æ²¡æœ‰æ­£åœ¨è¿è¡Œçš„çˆ¬è™«ä»»åŠ¡'
        )

        return error_response(
            code=ErrorCode.BUSINESS_ERROR,
            message='å½“å‰æ²¡æœ‰æ­£åœ¨è¿è¡Œçš„çˆ¬è™«ä»»åŠ¡'
        )

    try:
        # ä½¿ç”¨æ–°çš„ä¿¡å·é˜Ÿåˆ—ç³»ç»Ÿ
        bridge = get_crawler_control_bridge()
        signal_id = bridge.send_resume_signal()

        # æ›´æ–°çŠ¶æ€æ˜¾ç¤º
        crawl_status['message'] = 'æ­£åœ¨æ¢å¤...'

        # åŒæ­¥åˆ°å…±äº«çŠ¶æ€ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        crawl_control = current_app.config.get('CRAWL_CONTROL', {})
        crawl_control['paused'] = False
        cache_manager.shared_set(CacheKeys.CRAWL_CONTROL, crawl_control)
        cache_manager.shared_set(CacheKeys.CRAWL_STATUS, crawl_status)

        logger.info(f"[CRAWLER] æ”¶åˆ°æ¢å¤è¯·æ±‚ï¼Œä¿¡å·ID: {signal_id}")

        duration_ms = (time.time() - start_time) * 1000

        log_api_call(
            logger,
            method='POST',
            endpoint='/api/crawl/resume',
            params={},
            status='success',
            response_code=200,
            duration_ms=duration_ms
        )

        return success_response(
            data={'signal_id': signal_id},
            message='æ¢å¤æŒ‡ä»¤å·²å‘é€'
        )
    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000

        log_error_with_traceback(
            logger,
            e,
            context={'endpoint': '/api/crawl/resume'},
            message='å‘é€æ¢å¤ä¿¡å·å¤±è´¥'
        )

        log_api_call(
            logger,
            method='POST',
            endpoint='/api/crawl/resume',
            params={},
            status='error',
            response_code=500,
            duration_ms=duration_ms,
            error=str(e)
        )

        return error_response(
            code=ErrorCode.INTERNAL_ERROR,
            message='å‘é€æ¢å¤ä¿¡å·å¤±è´¥',
            details=str(e)
        )


@api_crawl_bp.route('/crawl/force-clear', methods=['POST'])
def api_force_clear_crawl():
    """
    å¼ºåˆ¶æ¸…é™¤å½“å‰ä»»åŠ¡

    ç›´æ¥é‡ç½®æ‰€æœ‰çŠ¶æ€ï¼Œä¸ç»è¿‡ä¿¡å·é˜Ÿåˆ—
    ç”¨äºç´§æ€¥æƒ…å†µä¸‹çš„çŠ¶æ€æ¢å¤
    """
    start_time = time.time()

    try:
        from crawler_control.cc_control_bridge import get_crawler_control_bridge

        bridge = get_crawler_control_bridge()

        # 1. æ¸…é™¤æ‰€æœ‰å¾…å¤„ç†ä¿¡å·
        bridge.queue_manager.clear_signals()

        # 2. é‡ç½®çŠ¶æ€åè°ƒå™¨åˆ°åˆå§‹çŠ¶æ€
        bridge.coordinator.reset_state()

        # 3. æ¸…é™¤ app.config ä¸­çš„çŠ¶æ€ï¼ˆå…³é”®ï¼ï¼‰
        crawl_status = current_app.config.get('CRAWL_STATUS', {})
        crawl_status['is_crawling'] = False
        crawl_status['is_paused'] = False
        crawl_status['message'] = 'ç©ºé—²'

        crawl_control = current_app.config.get('CRAWL_CONTROL', {})
        crawl_control['paused'] = False
        crawl_control['stop'] = False

        crawl_progress = current_app.config.get('CRAWL_PROGRESS', {})
        crawl_progress.clear()

        # 4. æ¸…é™¤ç¼“å­˜ä¸­çš„å…±äº«çŠ¶æ€
        cache_manager.shared_set(CacheKeys.IS_CRAWLING, False)
        cache_manager.shared_set(CacheKeys.IS_PAUSED, False)
        cache_manager.shared_set(CacheKeys.CRAWL_PROGRESS, {})

        # 5. æ¸…é™¤çˆ¬è™«æ§åˆ¶çŠ¶æ€ï¼ˆæ—§ç³»ç»Ÿå…¼å®¹ï¼‰
        cache_manager.shared_set(CacheKeys.CRAWL_CONTROL, {
            'running': False,
            'paused': False,
            'stopping': False
        })

        # 6. æ¸…é™¤çˆ¬è™«çŠ¶æ€ç¼“å­˜ï¼ˆæ—§ç³»ç»Ÿå…¼å®¹ï¼‰
        cache_manager.shared_set(CacheKeys.CRAWL_STATUS, {
            'is_crawling': False,
            'message': 'ç©ºé—²'
        })

        # 7. æ¸…é™¤ç»Ÿä¸€çˆ¬è™«çŠ¶æ€ï¼ˆæ–°çŠ¶æ€ç®¡ç†å™¨ï¼‰
        cache_manager.shared_set(CacheKeys.CRAWL_UNIFIED_STATE, {
            'is_crawling': False,
            'is_paused': False,
            'should_stop': False,
            'message': 'ç©ºé—²',
            'progress_percent': 0,
            'total_saved': 0,
            'total_skipped': 0,
            'total_failed': 0
        })

        logger.info("ğŸ”„ [CRAWLER] å¼ºåˆ¶æ¸…é™¤ä»»åŠ¡å®Œæˆ")

        duration_ms = (time.time() - start_time) * 1000

        log_api_call(
            logger,
            method='POST',
            endpoint='/api/crawl/force-clear',
            params={},
            status='success',
            response_code=200,
            duration_ms=duration_ms
        )

        return success_response(
            message='å·²å¼ºåˆ¶æ¸…é™¤å½“å‰ä»»åŠ¡'
        )

    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000

        log_error_with_traceback(
            logger,
            e,
            context={'endpoint': '/api/crawl/force-clear'},
            message='å¼ºåˆ¶æ¸…é™¤ä»»åŠ¡å¤±è´¥'
        )

        log_api_call(
            logger,
            method='POST',
            endpoint='/api/crawl/force-clear',
            params={},
            status='error',
            response_code=500,
            duration_ms=duration_ms,
            error=str(e)
        )

        return error_response(
            code=ErrorCode.INTERNAL_ERROR,
            message='å¼ºåˆ¶æ¸…é™¤ä»»åŠ¡å¤±è´¥',
            details=str(e)
        )


@api_crawl_bp.route('/crawl/summary')
def api_crawl_summary():
    """è·å–çˆ¬è™«æ‘˜è¦"""
    SUMMARY_FILE = current_app.config.get('SUMMARY_FILE', '')
    try:
        if os.path.exists(SUMMARY_FILE):
            with open(SUMMARY_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return jsonify(data)
    except Exception:
        pass
    return jsonify({})


@api_crawl_bp.route('/crawl/options', methods=['GET', 'POST'])
def api_crawl_options():
    """è·å–/ä¿å­˜çˆ¬è™«é€‰é¡¹"""
    start_time = time.time()
    LOG_DIR = current_app.config.get('LOG_DIR', '')
    OPTIONS_FILE = current_app.config.get('OPTIONS_FILE', '')

    if request.method == 'POST':
        data = request.get_json(silent=True) or {}
        try:
            os.makedirs(LOG_DIR, exist_ok=True)
            with open(OPTIONS_FILE, 'w', encoding='utf-8') as f:
                json.dump({
                    'fids': data.get('fids'),
                    'date_mode': data.get('date_mode'),
                    'date_value': data.get('date_value'),
                    'dateline': data.get('dateline'),
                    'max_pages': int(data.get('max_pages') or 3)
                }, f, ensure_ascii=False)

            duration_ms = (time.time() - start_time) * 1000

            log_api_call(
                logger,
                method='POST',
                endpoint='/api/crawl/options',
                params={'data_keys': list(data.keys()) if data else []},
                status='success',
                response_code=200,
                duration_ms=duration_ms
            )

            return success_response(message='ä¿å­˜æˆåŠŸ')
        except Exception as e:
            duration_ms = (time.time() - start_time) * 1000

            log_error_with_traceback(
                logger,
                e,
                context={'endpoint': '/api/crawl/options', 'method': 'POST'},
                message='ä¿å­˜é€‰é¡¹å¤±è´¥'
            )

            log_api_call(
                logger,
                method='POST',
                endpoint='/api/crawl/options',
                params={},
                status='error',
                response_code=500,
                duration_ms=duration_ms,
                error=str(e)
            )

            return error_response(
                code=ErrorCode.INTERNAL_ERROR,
                message='ä¿å­˜é€‰é¡¹å¤±è´¥',
                details=str(e)
            )
    else:
        try:
            if os.path.exists(OPTIONS_FILE):
                with open(OPTIONS_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                    duration_ms = (time.time() - start_time) * 1000

                    log_api_call(
                        logger,
                        method='GET',
                        endpoint='/api/crawl/options',
                        params={},
                        status='success',
                        response_code=200,
                        duration_ms=duration_ms
                    )

                    return success_response(
                        data=data,
                        message='è·å–é€‰é¡¹æˆåŠŸ'
                    )

            duration_ms = (time.time() - start_time) * 1000

            log_api_call(
                logger,
                method='GET',
                endpoint='/api/crawl/options',
                params={'note': 'no options file'},
                status='success',
                response_code=200,
                duration_ms=duration_ms
            )

            return success_response(
                data={},
                message='æš‚æ— é€‰é¡¹é…ç½®'
            )
        except Exception as e:
            duration_ms = (time.time() - start_time) * 1000

            log_error_with_traceback(
                logger,
                e,
                context={'endpoint': '/api/crawl/options', 'method': 'GET'},
                message='è·å–é€‰é¡¹å¤±è´¥'
            )

            log_api_call(
                logger,
                method='GET',
                endpoint='/api/crawl/options',
                params={},
                status='error',
                response_code=500,
                duration_ms=duration_ms,
                error=str(e)
            )

            return error_response(
                code=ErrorCode.INTERNAL_ERROR,
                message='è·å–é€‰é¡¹å¤±è´¥',
                details=str(e)
            )


@api_crawl_bp.route('/crawl/config', methods=['GET'])
def api_crawl_config_get():
    """è·å–çˆ¬è™«é…ç½®"""
    try:
        from configuration import config_manager
        config = config_manager.get_crawl_summary()

        return jsonify({
            'status': 'success',
            'data': config
        })
    except Exception as e:
        logger.error(f"è·å–çˆ¬è™«é…ç½®å¤±è´¥: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500


@api_crawl_bp.route('/crawl/config', methods=['POST'])
def api_crawl_config_set():
    """ä¿å­˜çˆ¬è™«é…ç½®"""
    try:
        from configuration import config_manager

        data = request.get_json()
        if not data:
            return jsonify({
                'status': 'error',
                'message': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'
            }), 400

        # ä½¿ç”¨æ–°çš„ä¸€æ¬¡æ€§è®¾ç½®æ–¹æ³•
        success = config_manager.update(data, section='crawler')

        if success:
            return jsonify({
                'status': 'success',
                'message': 'é…ç½®ä¿å­˜æˆåŠŸ'
            })
        else:
            return jsonify({
                'status': 'error',
                'message': 'ä¿å­˜é…ç½®å¤±è´¥'
            }), 500

    except Exception as e:
        logger.error(f"âœ— [CONFIG] ä¿å­˜çˆ¬è™«é…ç½®å¤±è´¥: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500


@api_crawl_bp.route('/crawl/selected-forums', methods=['GET'])
def api_get_selected_forums():
    """è·å–é€‰ä¸­çš„è®ºå›åˆ—è¡¨"""
    start_time = time.time()

    try:
        from configuration import config_manager
        selected_forums = config_manager.get_crawl_config('selected_forums') or []

        duration_ms = (time.time() - start_time) * 1000

        log_api_call(
            logger,
            method='GET',
            endpoint='/api/crawl/selected-forums',
            params={},
            status='success',
            response_code=200,
            duration_ms=duration_ms
        )

        return success_response(
            data={'selected_forums': selected_forums},
            message='è·å–é€‰ä¸­è®ºå›æˆåŠŸ'
        )
    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000

        log_error_with_traceback(
            logger,
            e,
            context={'endpoint': '/api/crawl/selected-forums', 'method': 'GET'},
            message='è·å–é€‰ä¸­è®ºå›å¤±è´¥'
        )

        log_api_call(
            logger,
            method='GET',
            endpoint='/api/crawl/selected-forums',
            params={},
            status='error',
            response_code=500,
            duration_ms=duration_ms,
            error=str(e)
        )

        return error_response(
            code=ErrorCode.INTERNAL_ERROR,
            message='è·å–é€‰ä¸­è®ºå›å¤±è´¥',
            details=str(e)
        )


@api_crawl_bp.route('/crawl/selected-forums', methods=['POST'])
def api_set_selected_forums():
    """ä¿å­˜é€‰ä¸­çš„è®ºå›åˆ—è¡¨"""
    start_time = time.time()

    try:
        from configuration import config_manager

        data = request.get_json()
        if not data or 'selected_forums' not in data:
            duration_ms = (time.time() - start_time) * 1000

            log_api_call(
                logger,
                method='POST',
                endpoint='/api/crawl/selected-forums',
                params={},
                status='error',
                response_code=400,
                duration_ms=duration_ms,
                error='æ— æ•ˆçš„è¯·æ±‚æ•°æ®'
            )

            return invalid_request_response('æ— æ•ˆçš„è¯·æ±‚æ•°æ®')

        selected_forums = data['selected_forums']
        if not isinstance(selected_forums, list):
            duration_ms = (time.time() - start_time) * 1000

            log_api_call(
                logger,
                method='POST',
                endpoint='/api/crawl/selected-forums',
                params={'selected_forums': type(selected_forums).__name__},
                status='error',
                response_code=400,
                duration_ms=duration_ms,
                error='é€‰ä¸­è®ºå›å¿…é¡»æ˜¯æ•°ç»„'
            )

            return invalid_parameter_response('selected_forums', details='é€‰ä¸­è®ºå›å¿…é¡»æ˜¯æ•°ç»„')

        success = config_manager.set_crawl_config('selected_forums', selected_forums)

        if success:
            duration_ms = (time.time() - start_time) * 1000

            log_api_call(
                logger,
                method='POST',
                endpoint='/api/crawl/selected-forums',
                params={'count': len(selected_forums)},
                status='success',
                response_code=200,
                duration_ms=duration_ms
            )

            return success_response(
                message='é€‰ä¸­è®ºå›å·²ä¿å­˜'
            )
        else:
            duration_ms = (time.time() - start_time) * 1000

            log_api_call(
                logger,
                method='POST',
                endpoint='/api/crawl/selected-forums',
                params={},
                status='error',
                response_code=500,
                duration_ms=duration_ms,
                error='ä¿å­˜å¤±è´¥'
            )

            return operation_failed_response('ä¿å­˜å¤±è´¥')

    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000

        log_error_with_traceback(
            logger,
            e,
            context={'endpoint': '/api/crawl/selected-forums', 'method': 'POST'},
            message='ä¿å­˜é€‰ä¸­è®ºå›å¤±è´¥'
        )

        log_api_call(
            logger,
            method='POST',
            endpoint='/api/crawl/selected-forums',
            params={},
            status='error',
            response_code=500,
            duration_ms=duration_ms,
            error=str(e)
        )

        return error_response(
            code=ErrorCode.INTERNAL_ERROR,
            message='ä¿å­˜é€‰ä¸­è®ºå›å¤±è´¥',
            details=str(e)
        )


# ==================== è®ºå›ä¿¡æ¯API ====================

@api_crawl_bp.route('/forum/info/<fid>')
def api_forum_info(fid):
    """è·å–æ¿å—ä¿¡æ¯API - ä½¿ç”¨æŒä¹…åŒ–å­˜å‚¨"""
    try:
        # å°è¯•ä»æ•°æ®åº“è·å–
        cat = Category.query.filter_by(fid=str(fid)).first()
        if cat:
            return jsonify({
                'status': 'success',
                'data': cat.to_dict(),
                'cached': True
            })

        # å¦‚æœæ•°æ®åº“æ²¡æœ‰ï¼Œå°è¯•è·å–æ–°æ•°æ®å¹¶ä¿å­˜
        logger.debug(f"[DB] æ•°æ®åº“æœªå‘½ä¸­ï¼Œå°è¯•è·å–æ¿å—ä¿¡æ¯: fid={fid}")
        from crawler import SHT  # ä»æ–°çš„ crawler æ¨¡å—å¯¼å…¥
        sht = SHT()

        # ä¼ é€’å·²æœ‰åˆ†ç±»ä¿¡æ¯ä½œä¸ºç¼“å­˜å‚è€ƒ
        all_categories = {c.fid: c.to_dict() for c in Category.query.all()}
        forum_info = sht.get_forum_info(fid, all_categories if all_categories else None)

        if forum_info:
            cat = Category(
                fid=str(fid),
                name=forum_info.get('name', 'Unknown'),
                parent_id=str(forum_info.get('parent_id', '')) if forum_info.get('parent_id') else None,
                description=forum_info.get('description')
            )
            db.session.add(cat)
            db.session.commit()

            return jsonify({
                'status': 'success',
                'data': cat.to_dict(),
                'cached': False
            })
        else:
            return jsonify({
                'status': 'error',
                'message': 'æ— æ³•è·å–æ¿å—ä¿¡æ¯'
            }), 404

    except Exception as e:
        logger.error(f"âœ— [CRAWLER] è·å–æ¿å—ä¿¡æ¯å¤±è´¥: fid={fid}, é”™è¯¯: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500


@api_crawl_bp.route('/forum/info/batch')
def api_forum_info_batch():
    """æ‰¹é‡è·å–æ‰€æœ‰æ¿å—ä¿¡æ¯API - ä½¿ç”¨æŒä¹…åŒ–å­˜å‚¨"""
    # æ·»åŠ æ˜ç¡®çš„å…¥å£æ—¥å¿—
    logger.info(f"[API] ========================================")
    logger.info(f"[API] æ”¶åˆ° /api/forum/info/batch è¯·æ±‚")
    logger.info(f"[API] ========================================")

    # è·å–å‚æ•°
    fast_mode = request.args.get('fast', 'false').lower() == 'true'
    force_refresh = request.args.get('force', 'false').lower() == 'true'

    logger.info(f"[API] è¯·æ±‚å‚æ•°: fast_mode={fast_mode}, force_refresh={force_refresh}")

    try:
        # æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶åˆ·æ–°
        if not force_refresh:
            # æ£€æŸ¥æ•°æ®åº“å†…å®¹
            categories = Category.get_all_categories()

            if categories:
                logger.debug(f"[DB] ä»æ•°æ®åº“è·å–è®ºå›ä¿¡æ¯: {len(categories)}ä¸ªæ¿å—")
                return jsonify({
                    'status': 'success',
                    'data': {c.fid: c.to_dict() for c in categories},
                    'cached': True,
                    'mode': 'fast' if fast_mode else 'detailed',
                    'cache_info': Category.get_cache_info()
                })

        # ç¼“å­˜æœªå‘½ä¸­ã€è¿‡æœŸæˆ–å¼ºåˆ¶åˆ·æ–°ï¼Œè·å–æ–°æ•°æ®
        if force_refresh:
            logger.info(f"[CRAWLER] å¼ºåˆ¶åˆ·æ–°è®ºå›ä¿¡æ¯")
        else:
            logger.info(f"[CACHE] æŒä¹…åŒ–ç¼“å­˜æ— æ•ˆï¼Œé‡æ–°è·å–è®ºå›ä¿¡æ¯")

        from crawler import SHT  # ä»æ–°çš„ crawler æ¨¡å—å¯¼å…¥

        if fast_mode:
            # å¿«é€Ÿæ¨¡å¼ï¼šåªè·å–åŸºæœ¬ä¿¡æ¯
            sht = SHT()
            all_forums_info = sht.get_all_forums_info()

            if all_forums_info:
                # ä¿å­˜åˆ°æ•°æ®åº“
                Category.update_forum_info(all_forums_info)

                return jsonify({
                    'status': 'success',
                    'data': all_forums_info,
                    'cached': False,
                    'mode': 'fast',
                    'cache_info': Category.get_cache_info()
                })
            else:
                return jsonify({
                    'status': 'error',
                    'message': 'æ— æ³•è·å–æ¿å—åŸºæœ¬ä¿¡æ¯'
                }), 404

        else:
            # è¯¦ç»†æ¨¡å¼ï¼šè·å–åŒ…æ‹¬é¡µæ•°åœ¨å†…çš„è¯¦ç»†ä¿¡æ¯
            sht = SHT()
            all_forums_info = sht.get_all_forums_info()

            if not all_forums_info:
                return jsonify({
                    'status': 'error',
                    'message': 'æ— æ³•è·å–æ¿å—åŸºæœ¬ä¿¡æ¯'
                }), 404

            # å¹¶å‘è·å–è¯¦ç»†ä¿¡æ¯
            detailed_forums_info = {}

            def get_forum_detail(fid):
                """è·å–å•ä¸ªæ¿å—è¯¦ç»†ä¿¡æ¯çš„çº¿ç¨‹å‡½æ•°"""
                try:
                    # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„SHTå®ä¾‹
                    thread_sht = SHT()
                    # ä¼ å…¥å…¨å±€ç¼“å­˜ï¼Œé¿å…é‡å¤ç½‘ç»œè¯·æ±‚
                    forum_detail = thread_sht.get_forum_info(fid, all_forums_info)
                    return fid, forum_detail
                except Exception as e:
                    logger.warning(f"! [CRAWLER] çº¿ç¨‹è·å–æ¿å— {fid} è¯¦ç»†ä¿¡æ¯å¼‚å¸¸: {e}")
                    return fid, None

            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘è·å–ï¼ˆå¢åŠ å¹¶å‘æ•°ï¼‰
            with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_fid = {
                    executor.submit(get_forum_detail, fid): fid
                    for fid in all_forums_info.keys()
                }

                # æ”¶é›†ç»“æœ
                for future in concurrent.futures.as_completed(future_to_fid, timeout=45):
                    try:
                        fid, forum_detail = future.result()
                        if forum_detail:
                            detailed_forums_info[fid] = forum_detail
                        else:
                            # å¦‚æœè·å–è¯¦ç»†ä¿¡æ¯å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬ä¿¡æ¯
                            detailed_forums_info[fid] = all_forums_info[fid]
                            logger.warning(f"! [CRAWLER] è·å–æ¿å— {fid} è¯¦ç»†ä¿¡æ¯å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬ä¿¡æ¯")
                    except Exception as e:
                        fid = future_to_fid[future]
                        logger.warning(f"è·å–æ¿å— {fid} è¯¦ç»†ä¿¡æ¯å¼‚å¸¸: {e}")
                        # ä½¿ç”¨åŸºæœ¬ä¿¡æ¯ä½œä¸ºå¤‡ç”¨
                        detailed_forums_info[fid] = all_forums_info[fid]

            if detailed_forums_info:
                # ä¿å­˜åˆ°æ•°æ®åº“
                Category.update_forum_info(detailed_forums_info)

                logger.info(f"å¹¶å‘è·å–å®Œæˆï¼Œå…± {len(detailed_forums_info)} ä¸ªæ¿å—")

                return jsonify({
                    'status': 'success',
                    'data': detailed_forums_info,
                    'cached': False,
                    'mode': 'detailed',
                    'cache_info': Category.get_cache_info()
                })
            else:
                return jsonify({
                    'status': 'error',
                    'message': 'æ— æ³•è·å–æ¿å—è¯¦ç»†ä¿¡æ¯'
                }), 404

    except Exception as e:
        logger.error(f"æ‰¹é‡è·å–æ¿å—ä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({
            'status': 'error',
            'message': f'è·å–æ¿å—ä¿¡æ¯å¤±è´¥: {str(e)}'
        }), 500


@api_crawl_bp.route('/forum/cache/info')
def api_forum_cache_info():
    """è·å–è®ºå›æ¿å—æ•°æ®åº“å­˜å‚¨ä¿¡æ¯"""
    try:
        cache_info = Category.get_cache_info()
        return jsonify({
            'status': 'success',
            'cache_info': cache_info
        })
    except Exception as e:
        logger.error(f"è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500


@api_crawl_bp.route('/forum/cache/clear', methods=['POST'])
def api_forum_cache_clear():
    """æ¸…é™¤è®ºå›æ¿å—æ•°æ® (é‡ç½® last_updated æ¥å¼ºåˆ¶åˆ·æ–°)"""
    try:
        # é€šè¿‡å°† last_updated è®¾ç½®ä¸ºè¿‡å»çš„æ—¶é—´æ¥ä½¿ç¼“å­˜å¤±æ•ˆ
        db.session.query(Category).update({Category.last_updated: datetime.now(timezone.utc) - timedelta(days=365)})
        db.session.commit()
        return jsonify({
            'status': 'success',
            'message': 'æ¿å—ç¼“å­˜å·²é‡ç½®'
        })
    except Exception as e:
        logger.error(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500


# ==================== å¥åº·æ£€æŸ¥API ====================

@api_crawl_bp.route('/health')
def api_health():
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥API - ç»Ÿä¸€è½¬å‘è‡³ health æ¢çº½"""
    from health import monitor

    # æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶åˆ·æ–°
    force_refresh = request.args.get('force', 'false').lower() == 'true'

    try:
        # å°è¯•ä»ç¼“å­˜è·å–ï¼ˆé™¤éå¼ºåˆ¶åˆ·æ–°ï¼‰
        cache_key = f"{CacheKeys.HEALTH}:check"
        if not force_refresh:
            cached_health = cache_manager.get(cache_key)
            if cached_health:
                return jsonify(cached_health)

        # è·å–å¥åº·æ‘˜è¦
        health_summary = monitor.get_summary()

        # è¡¥å……é…ç½®ä¸ä»£ç†çŠ¶æ€
        config_status = Config.get_config_summary()
        proxy_status = {'enabled': False, 'working': False, 'details': {}}

        if config_status.get('proxy_enabled', False):
            proxy_status['enabled'] = True
            proxy_url = Config.PROXY
            try:
                import requests
                from utils.retry_utils import retry_request, RETRY_CONFIG

                config = RETRY_CONFIG['proxy_check']
                resp = retry_request(
                    requests.get,
                    url='http://httpbin.org/ip',
                    proxies={'http': proxy_url, 'https': proxy_url},
                    raise_on_fail=False,
                    **config
                )
                proxy_status['working'] = resp is not None and resp.status_code == 200
                if proxy_status['working']:
                    proxy_status['details']['proxy_ip'] = resp.json().get('origin', 'Unknown')
            except:
                proxy_status['working'] = False

        # è·å–ç»Ÿè®¡æ¦‚è§ˆ
        db_metrics = health_summary.get('metrics', {}).get('db', {})

        # å®‰å…¨é€€ä¿ï¼šå¦‚æœæŒ‡æ ‡ä¸ºç©ºï¼Œå°è¯•å³æ—¶é‡‡é›†
        if not db_metrics or db_metrics.get('status') != 'ok':
            try:
                db_metrics = monitor._get_db_info()
            except:
                pass

        # å…¼å®¹æ€§é€‚é…
        db_ok = db_metrics.get('resources', 0) > 0 or db_metrics.get('categories', 0) > 0

        # ç³»ç»Ÿå„ä¸ªç»„ä»¶çš„çŠ¶æ€æ±‡æ€»
        database_status = {
            'status': 'ok' if db_ok else 'warning',
            'categories': db_metrics.get('categories', 0),
            'resources': db_metrics.get('resources', 0),
            'details': db_metrics
        }

        # SHT2BM çŠ¶æ€æ£€æµ‹
        sht2bm_enabled = os.environ.get('ENABLE_SHT2BM', 'true').lower() == 'true'
        sht2bm_running = False

        # æ£€æµ‹ SHT2BM API æ˜¯å¦å·²æ³¨å†Œï¼ˆç›´æ¥æ£€æŸ¥ Blueprintï¼Œé¿å… HTTP è¯·æ±‚å¯¼è‡´æ­»é”ï¼‰
        if sht2bm_enabled:
            try:
                # æ£€æŸ¥ Blueprint æ˜¯å¦å·²æ³¨å†Œ
                from flask import current_app as _app
                sht2bm_running = 'sht2bm' in _app.blueprints
                if not sht2bm_running:
                    # é™çº§ï¼šå°è¯•å¯¼å…¥æ¨¡å—æ£€æµ‹
                    try:
                        from sht2bm_adapter import sht2bm_bp
                        sht2bm_running = True
                    except:
                        pass
            except:
                sht2bm_running = False

        sht2bm_status = {
            'enabled': sht2bm_enabled,
            'running': sht2bm_running,
            'status': 'ok' if sht2bm_running else ('disabled' if not sht2bm_enabled else 'error'),
            'integrated': True,  # æ ‡è®°ä¸ºå†…ç½®é›†æˆ
            'endpoint': '/api/bt' if sht2bm_running else None
        }

        # åˆå¹¶ç»“æœ
        result = {
            'status': health_summary['status'],
            'score': health_summary['score'],
            'issues': health_summary['issues'],
            'metrics': health_summary['metrics'],
            'database': database_status,
            'sht2bm': sht2bm_status,
            'cache': health_summary['metrics'].get('cache', {}),
            'config': config_status,
            'config_status': config_status,
            'proxy_status': proxy_status,
            'version': Config.VERSION,
            'timestamp': datetime.now(timezone.utc).isoformat()
        }

        # å­˜å…¥ç¼“å­˜ (5åˆ†é’Ÿ)
        cache_manager.set(cache_key, result, ttl=300)
        return jsonify(result)

    except Exception as e:
        log_error_with_traceback(
            logger,
            e,
            context={'endpoint': '/api/health'},
            message='å¥åº·æ£€æŸ¥å¤±è´¥'
        )

        return error_response(
            code=ErrorCode.INTERNAL_ERROR,
            message='å¥åº·æ£€æŸ¥å¤±è´¥',
            details=str(e)
        )


# ==================== SHT2BM API ====================

@api_crawl_bp.route('/sht2bm/start', methods=['POST'])
def api_sht2bm_start():
    """å¯åŠ¨ SHT2BM æœåŠ¡"""
    try:
        # æ£€æŸ¥æœåŠ¡æ˜¯å¦å·²ç»è¿è¡Œ
        sht2bm_port = os.environ.get('BM_PORT', '5144')
        try:
            import requests
            from utils.retry_utils import retry_request, RETRY_CONFIG

            config = RETRY_CONFIG['health_check']
            response = retry_request(
                requests.get,
                url=f'http://localhost:{sht2bm_port}/api/health',
                raise_on_fail=False,
                **config
            )
            if response and response.status_code == 200:
                return jsonify({
                    'status': 'success',
                    'message': 'SHT2BM æœåŠ¡å·²åœ¨è¿è¡Œ',
                    'port': sht2bm_port
                })
        except:
            pass  # æœåŠ¡æœªè¿è¡Œï¼Œç»§ç»­å¯åŠ¨

        # å¯åŠ¨æœåŠ¡
        def start_service():
            try:
                # è®¾ç½®ç¯å¢ƒå˜é‡ - ä½¿ç”¨é…ç½®ç®¡ç†å™¨è·å–æ•°æ®åº“è·¯å¾„
                from configuration import Config
                db_path = os.environ.get('DB_PATH', Config.get_path('db_path'))
                os.environ['SHT_DB_PATH'] = db_path
                os.environ['PORT'] = sht2bm_port

                # SHT2BM å·²æ”¹ä¸º Blueprint æ–¹å¼é›†æˆï¼Œä¸éœ€è¦å•ç‹¬å¯åŠ¨
                logger.info("SHT2BM æœåŠ¡å·²é€šè¿‡ Blueprint é›†æˆï¼Œæ— éœ€å•ç‹¬å¯åŠ¨")

            except Exception as e:
                logger.error(f"SHT2BM æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")

        # åœ¨åå°çº¿ç¨‹ä¸­å¯åŠ¨
        thread = threading.Thread(target=start_service, daemon=True, name='SHT2BM-Manual-Start')
        thread.start()

        # ç­‰å¾…ä¸€ä¸‹è®©æœåŠ¡å¯åŠ¨
        time.sleep(2)

        # éªŒè¯æœåŠ¡æ˜¯å¦å¯åŠ¨æˆåŠŸ
        try:
            import requests
            from utils.retry_utils import retry_request, RETRY_CONFIG

            config = RETRY_CONFIG['health_check']
            response = retry_request(
                requests.get,
                url=f'http://localhost:{sht2bm_port}/api/health',
                raise_on_fail=False,
                **config
            )
            if response and response.status_code == 200:
                return jsonify({
                    'status': 'success',
                    'message': 'SHT2BM æœåŠ¡å¯åŠ¨æˆåŠŸ',
                    'port': sht2bm_port
                })
            else:
                return jsonify({
                    'status': 'error',
                    'message': f'SHT2BM æœåŠ¡å¯åŠ¨å¤±è´¥ï¼Œå“åº”ç : {response.status_code if response else "N/A"}'
                }), 500
        except Exception as e:
            return jsonify({
                'status': 'error',
                'message': f'SHT2BM æœåŠ¡å¯åŠ¨å¤±è´¥: {str(e)}'
            }), 500

    except Exception as e:
        logger.error(f"å¯åŠ¨ SHT2BM æœåŠ¡å¤±è´¥: {e}")
        return jsonify({
            'status': 'error',
            'message': f'å¯åŠ¨å¤±è´¥: {str(e)}'
        }), 500


@api_crawl_bp.route('/sht2bm/stop', methods=['POST'])
def api_sht2bm_stop():
    """åœæ­¢ SHT2BM æœåŠ¡"""
    try:
        import psutil

        sht2bm_port = os.environ.get('BM_PORT', '5144')

        # æŸ¥æ‰¾ SHT2BM è¿›ç¨‹
        killed_processes = []
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                cmdline = ' '.join(proc.info['cmdline'] or [])
                if 'sht2bm.py' in cmdline or f'PORT={sht2bm_port}' in cmdline:
                    proc.terminate()  # ä¼˜é›…ç»ˆæ­¢
                    killed_processes.append(proc.info['pid'])
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue

        if killed_processes:
            return jsonify({
                'status': 'success',
                'message': f'SHT2BM æœåŠ¡å·²åœæ­¢ï¼Œç»ˆæ­¢è¿›ç¨‹: {killed_processes}',
                'port': sht2bm_port
            })
        else:
            return jsonify({
                'status': 'warning',
                'message': 'SHT2BM æœåŠ¡æœªè¿è¡Œæˆ–æ— æ³•æ‰¾åˆ°ç›¸å…³è¿›ç¨‹',
                'port': sht2bm_port
            })

    except Exception as e:
        logger.error(f"åœæ­¢ SHT2BM æœåŠ¡å¤±è´¥: {e}")
        return jsonify({
            'status': 'error',
            'message': f'åœæ­¢å¤±è´¥: {str(e)}'
        }), 500


@api_crawl_bp.route('/sht2bm/sync', methods=['POST'])
def api_sht2bm_sync():
    """æ‰‹åŠ¨åŒæ­¥æ•°æ®åˆ°SHT2BM"""
    try:
        sht2bm_port = os.environ.get('BM_PORT', '5144')

        # æ£€æŸ¥SHT2BMæœåŠ¡æ˜¯å¦è¿è¡Œ
        try:
            import requests
            from utils.retry_utils import retry_request, RETRY_CONFIG

            config = RETRY_CONFIG['health_check']
            response = retry_request(
                requests.get,
                url=f'http://localhost:{sht2bm_port}/api/health',
                raise_on_fail=False,
                **config
            )
            if not response or response.status_code != 200:
                return jsonify({
                    'status': 'error',
                    'message': 'SHT2BMæœåŠ¡æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨æœåŠ¡'
                }), 400
        except:
            return jsonify({
                'status': 'error',
                'message': 'SHT2BMæœåŠ¡è¿æ¥å¤±è´¥'
            }), 400

        # è§¦å‘æ•°æ®åŒæ­¥
        try:
            import requests
            from utils.retry_utils import retry_request, RETRY_CONFIG

            config = RETRY_CONFIG['sht2bm_api']
            sync_response = retry_request(
                requests.post,
                url=f'http://localhost:{sht2bm_port}/api/sync',
                raise_on_fail=False,
                **config
            )
            if sync_response and sync_response.status_code == 200:
                return jsonify({
                    'status': 'success',
                    'message': 'SHT2BMæ•°æ®åŒæ­¥å®Œæˆ',
                    'data': sync_response.json()
                })
            else:
                return jsonify({
                    'status': 'error',
                    'message': f'åŒæ­¥å¤±è´¥: HTTP {sync_response.status_code if sync_response else "N/A"}'
                }), 500
        except Exception as e:
            return jsonify({
                'status': 'error',
                'message': f'åŒæ­¥å¤±è´¥: {str(e)}'
            }), 500

    except Exception as e:
        logger.error(f"SHT2BMåŒæ­¥å¤±è´¥: {e}")
        return jsonify({
            'status': 'error',
            'message': f'åŒæ­¥å¤±è´¥: {str(e)}'
        }), 500
